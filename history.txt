History file: model_versions/chess_elo_model_V1_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T17:42:17.468415Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game1_game1500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V0
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.261432   (epoch 9)   mode=min
  policy_loss: 2.153414   (epoch 9)   mode=min
  policy_policy_acc: 0.341760   (epoch 9)   mode=max
  val_loss: 2.886185   (epoch 3)   mode=min
  val_policy_loss: 2.771553   (epoch 3)   mode=min
  val_policy_policy_acc: 0.231532   (epoch 8)   mode=max
  val_value_loss: 0.218672   (epoch 9)   mode=min
  val_value_value_mse: 0.218832   (epoch 9)   mode=min
  value_loss: 0.215918   (epoch 9)   mode=min
  value_value_mse: 0.215916   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [3.0352, 2.9032, 2.8123, 2.7283, 2.6513, 2.5097, 2.4270, 2.3154, 2.2614]
  policy_loss : [2.9165, 2.7871, 2.6973, 2.6144, 2.5391, 2.3990, 2.3171, 2.2069, 2.1534]
  policy_policy_acc : [0.1767, 0.2075, 0.2250, 0.2432, 0.2568, 0.2868, 0.3047, 0.3306, 0.3418]
  val_loss : [2.9715, 2.9049, 2.8862, 2.8913, 2.9166, 2.9560, 2.9878, 3.0341, 3.0710]
  val_policy_loss : [2.8542, 2.7891, 2.7716, 2.7773, 2.8039, 2.8472, 2.8800, 2.9275, 2.9654]
  val_policy_policy_acc : [0.1980, 0.2110, 0.2191, 0.2229, 0.2250, 0.2293, 0.2275, 0.2315, 0.2303]
  val_value_loss : [0.2347, 0.2330, 0.2308, 0.2299, 0.2274, 0.2220, 0.2219, 0.2193, 0.2187]
  val_value_value_mse : [0.2348, 0.2331, 0.2308, 0.2300, 0.2275, 0.2222, 0.2220, 0.2195, 0.2188]
  value_loss : [0.2373, 0.2323, 0.2299, 0.2276, 0.2248, 0.2213, 0.2197, 0.2170, 0.2159]
  value_value_mse : [0.2373, 0.2323, 0.2299, 0.2276, 0.2248, 0.2213, 0.2197, 0.2170, 0.2159]

================================================================================

History file: model_versions/chess_elo_model_V2_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T17:52:16.043232Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game1501_game3000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.263954   (epoch 8)   mode=min
  policy_loss: 2.159003   (epoch 8)   mode=min
  policy_policy_acc: 0.340336   (epoch 8)   mode=max
  val_loss: 2.825699   (epoch 3)   mode=min
  val_policy_loss: 2.716552   (epoch 2)   mode=min
  val_policy_policy_acc: 0.239043   (epoch 5)   mode=max
  val_value_loss: 0.209194   (epoch 8)   mode=min
  val_value_value_mse: 0.209114   (epoch 8)   mode=min
  value_loss: 0.209709   (epoch 8)   mode=min
  value_value_mse: 0.209704   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8882, 2.7748, 2.6900, 2.6103, 2.4792, 2.4086, 2.3088, 2.2640]
  policy_loss : [2.7724, 2.6607, 2.5779, 2.5002, 2.3712, 2.3021, 2.2034, 2.1590]
  policy_policy_acc : [0.2149, 0.2374, 0.2516, 0.2690, 0.2938, 0.3088, 0.3282, 0.3403]
  val_loss : [2.8541, 2.8259, 2.8257, 2.8328, 2.8375, 2.8531, 2.8850, 2.9075]
  val_policy_loss : [2.7422, 2.7166, 2.7176, 2.7245, 2.7328, 2.7492, 2.7823, 2.8052]
  val_policy_policy_acc : [0.2202, 0.2310, 0.2296, 0.2335, 0.2390, 0.2345, 0.2356, 0.2368]
  val_value_loss : [0.2300, 0.2244, 0.2204, 0.2206, 0.2144, 0.2139, 0.2106, 0.2092]
  val_value_value_mse : [0.2300, 0.2244, 0.2204, 0.2207, 0.2145, 0.2139, 0.2106, 0.2091]
  value_loss : [0.2321, 0.2276, 0.2239, 0.2211, 0.2166, 0.2136, 0.2107, 0.2097]
  value_value_mse : [0.2320, 0.2276, 0.2239, 0.2211, 0.2167, 0.2136, 0.2107, 0.2097]

================================================================================

History file: model_versions/chess_elo_model_V3_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T18:02:42.468036Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game3001_game4500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.211183   (epoch 9)   mode=min
  policy_loss: 2.107530   (epoch 9)   mode=min
  policy_policy_acc: 0.351335   (epoch 9)   mode=max
  val_loss: 2.775561   (epoch 3)   mode=min
  val_policy_loss: 2.665119   (epoch 3)   mode=min
  val_policy_policy_acc: 0.244773   (epoch 6)   mode=max
  val_value_loss: 0.207973   (epoch 8)   mode=min
  val_value_value_mse: 0.207967   (epoch 8)   mode=min
  value_loss: 0.207342   (epoch 9)   mode=min
  value_value_mse: 0.207341   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8650, 2.7587, 2.6807, 2.6062, 2.5397, 2.4097, 2.3420, 2.2527, 2.2112]
  policy_loss : [2.7512, 2.6465, 2.5699, 2.4970, 2.4318, 2.3038, 2.2369, 2.1485, 2.1075]
  policy_policy_acc : [0.2240, 0.2434, 0.2580, 0.2709, 0.2822, 0.3093, 0.3237, 0.3436, 0.3513]
  val_loss : [2.8120, 2.7932, 2.7756, 2.8054, 2.8064, 2.8216, 2.8585, 2.8788, 2.9027]
  val_policy_loss : [2.6989, 2.6820, 2.6651, 2.6961, 2.6999, 2.7158, 2.7532, 2.7749, 2.7986]
  val_policy_policy_acc : [0.2333, 0.2379, 0.2395, 0.2388, 0.2377, 0.2448, 0.2434, 0.2439, 0.2429]
  val_value_loss : [0.2262, 0.2223, 0.2210, 0.2186, 0.2131, 0.2118, 0.2108, 0.2080, 0.2085]
  val_value_value_mse : [0.2262, 0.2223, 0.2210, 0.2186, 0.2131, 0.2118, 0.2108, 0.2080, 0.2085]
  value_loss : [0.2276, 0.2243, 0.2214, 0.2184, 0.2159, 0.2118, 0.2102, 0.2085, 0.2073]
  value_value_mse : [0.2276, 0.2243, 0.2214, 0.2184, 0.2159, 0.2118, 0.2102, 0.2085, 0.2073]

================================================================================

History file: model_versions/chess_elo_model_V4_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T18:14:58.457522Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game4501_game6000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.232742   (epoch 9)   mode=min
  policy_loss: 2.126822   (epoch 9)   mode=min
  policy_policy_acc: 0.345195   (epoch 9)   mode=max
  val_loss: 2.764779   (epoch 3)   mode=min
  val_policy_loss: 2.652715   (epoch 3)   mode=min
  val_policy_policy_acc: 0.249030   (epoch 8)   mode=max
  val_value_loss: 0.213607   (epoch 8)   mode=min
  val_value_value_mse: 0.213607   (epoch 8)   mode=min
  value_loss: 0.211842   (epoch 9)   mode=min
  value_value_mse: 0.211842   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8392, 2.7451, 2.6713, 2.6022, 2.5379, 2.4164, 2.3535, 2.2727, 2.2327]
  policy_loss : [2.7236, 2.6311, 2.5590, 2.4910, 2.4282, 2.3083, 2.2464, 2.1666, 2.1268]
  policy_policy_acc : [0.2260, 0.2420, 0.2566, 0.2682, 0.2801, 0.3042, 0.3191, 0.3361, 0.3452]
  val_loss : [2.7825, 2.7871, 2.7648, 2.7667, 2.7911, 2.7884, 2.8183, 2.8368, 2.8586]
  val_policy_loss : [2.6672, 2.6742, 2.6527, 2.6549, 2.6796, 2.6803, 2.7111, 2.7300, 2.7517]
  val_policy_policy_acc : [0.2375, 0.2324, 0.2419, 0.2392, 0.2424, 0.2458, 0.2444, 0.2490, 0.2475]
  val_value_loss : [0.2305, 0.2258, 0.2241, 0.2235, 0.2231, 0.2162, 0.2145, 0.2136, 0.2137]
  val_value_value_mse : [0.2305, 0.2258, 0.2241, 0.2235, 0.2231, 0.2162, 0.2145, 0.2136, 0.2137]
  value_loss : [0.2312, 0.2279, 0.2246, 0.2223, 0.2194, 0.2163, 0.2143, 0.2123, 0.2118]
  value_value_mse : [0.2312, 0.2279, 0.2246, 0.2223, 0.2194, 0.2163, 0.2143, 0.2123, 0.2118]

================================================================================

History file: model_versions/chess_elo_model_V5_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T18:29:43.626121Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game6001_game7500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.158641   (epoch 10)   mode=min
  policy_loss: 2.062861   (epoch 10)   mode=min
  policy_policy_acc: 0.359906   (epoch 10)   mode=max
  val_loss: 2.732946   (epoch 4)   mode=min
  val_policy_loss: 2.632593   (epoch 4)   mode=min
  val_policy_policy_acc: 0.250268   (epoch 7)   mode=max
  val_value_loss: 0.190252   (epoch 10)   mode=min
  val_value_value_mse: 0.190234   (epoch 10)   mode=min
  value_loss: 0.191538   (epoch 10)   mode=min
  value_value_mse: 0.191536   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8059, 2.7093, 2.6372, 2.5717, 2.5104, 2.4522, 2.3329, 2.2720, 2.1930, 2.1586]
  policy_loss : [2.6973, 2.6039, 2.5338, 2.4690, 2.4088, 2.3518, 2.2348, 2.1749, 2.0971, 2.0629]
  policy_policy_acc : [0.2346, 0.2519, 0.2638, 0.2764, 0.2862, 0.3002, 0.3218, 0.3351, 0.3533, 0.3599]
  val_loss : [2.7869, 2.7375, 2.7363, 2.7329, 2.7488, 2.7900, 2.7936, 2.8195, 2.8340, 2.8653]
  val_policy_loss : [2.6794, 2.6350, 2.6344, 2.6326, 2.6503, 2.6913, 2.6953, 2.7225, 2.7380, 2.7698]
  val_policy_policy_acc : [0.2341, 0.2429, 0.2462, 0.2497, 0.2469, 0.2455, 0.2503, 0.2482, 0.2472, 0.2472]
  val_value_loss : [0.2144, 0.2044, 0.2036, 0.2007, 0.1966, 0.1967, 0.1958, 0.1932, 0.1913, 0.1903]
  val_value_value_mse : [0.2144, 0.2044, 0.2036, 0.2007, 0.1966, 0.1967, 0.1958, 0.1931, 0.1912, 0.1902]
  value_loss : [0.2172, 0.2108, 0.2068, 0.2054, 0.2033, 0.2006, 0.1962, 0.1942, 0.1918, 0.1915]
  value_value_mse : [0.2172, 0.2108, 0.2068, 0.2054, 0.2033, 0.2006, 0.1962, 0.1942, 0.1918, 0.1915]

================================================================================

History file: model_versions/chess_elo_model_V6_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T18:39:42.070348Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game7501_game9000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.290728   (epoch 8)   mode=min
  policy_loss: 2.189975   (epoch 8)   mode=min
  policy_policy_acc: 0.335015   (epoch 8)   mode=max
  val_loss: 2.727877   (epoch 2)   mode=min
  val_policy_loss: 2.625673   (epoch 2)   mode=min
  val_policy_policy_acc: 0.256307   (epoch 7)   mode=max
  val_value_loss: 0.194541   (epoch 8)   mode=min
  val_value_value_mse: 0.194563   (epoch 8)   mode=min
  value_loss: 0.200344   (epoch 8)   mode=min
  value_value_mse: 0.200372   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7930, 2.6967, 2.6260, 2.5635, 2.4511, 2.3950, 2.3255, 2.2907]
  policy_loss : [2.6826, 2.5890, 2.5206, 2.4587, 2.3503, 2.2926, 2.2253, 2.1900]
  policy_policy_acc : [0.2381, 0.2541, 0.2686, 0.2786, 0.3012, 0.3127, 0.3272, 0.3350]
  val_loss : [2.7557, 2.7279, 2.7408, 2.7475, 2.7376, 2.7604, 2.7630, 2.7693]
  val_policy_loss : [2.6505, 2.6257, 2.6406, 2.6463, 2.6407, 2.6632, 2.6680, 2.6744]
  val_policy_policy_acc : [0.2488, 0.2560, 0.2491, 0.2496, 0.2559, 0.2511, 0.2563, 0.2538]
  val_value_loss : [0.2155, 0.2090, 0.2055, 0.2079, 0.1989, 0.2002, 0.1952, 0.1945]
  val_value_value_mse : [0.2154, 0.2090, 0.2055, 0.2078, 0.1989, 0.2002, 0.1953, 0.1946]
  value_loss : [0.2202, 0.2157, 0.2121, 0.2101, 0.2049, 0.2035, 0.2010, 0.2003]
  value_value_mse : [0.2202, 0.2157, 0.2121, 0.2101, 0.2050, 0.2034, 0.2010, 0.2004]

================================================================================

History file: model_versions/chess_elo_model_V7_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T18:51:18.095631Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game9001_game10500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.194870   (epoch 10)   mode=min
  policy_loss: 2.097180   (epoch 10)   mode=min
  policy_policy_acc: 0.353569   (epoch 10)   mode=max
  val_loss: 2.746445   (epoch 4)   mode=min
  val_policy_loss: 2.641800   (epoch 4)   mode=min
  val_policy_policy_acc: 0.251393   (epoch 10)   mode=max
  val_value_loss: 0.195693   (epoch 10)   mode=min
  val_value_value_mse: 0.195686   (epoch 10)   mode=min
  value_loss: 0.195295   (epoch 10)   mode=min
  value_value_mse: 0.195293   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7888, 2.7028, 2.6328, 2.5776, 2.5153, 2.4632, 2.3511, 2.2967, 2.2280, 2.1949]
  policy_loss : [2.6807, 2.5963, 2.5277, 2.4735, 2.4120, 2.3612, 2.2513, 2.1976, 2.1301, 2.0972]
  policy_policy_acc : [0.2377, 0.2519, 0.2652, 0.2747, 0.2859, 0.2972, 0.3204, 0.3316, 0.3445, 0.3536]
  val_loss : [2.7693, 2.7475, 2.7601, 2.7464, 2.7677, 2.8079, 2.8118, 2.8210, 2.8432, 2.8472]
  val_policy_loss : [2.6619, 2.6436, 2.6545, 2.6418, 2.6651, 2.7057, 2.7123, 2.7215, 2.7449, 2.7492]
  val_policy_policy_acc : [0.2423, 0.2452, 0.2467, 0.2477, 0.2465, 0.2418, 0.2492, 0.2455, 0.2504, 0.2514]
  val_value_loss : [0.2151, 0.2079, 0.2105, 0.2090, 0.2048, 0.2043, 0.1989, 0.1982, 0.1964, 0.1957]
  val_value_value_mse : [0.2152, 0.2079, 0.2106, 0.2090, 0.2048, 0.2043, 0.1989, 0.1982, 0.1964, 0.1957]
  value_loss : [0.2164, 0.2130, 0.2103, 0.2082, 0.2066, 0.2039, 0.1994, 0.1982, 0.1959, 0.1953]
  value_value_mse : [0.2164, 0.2130, 0.2103, 0.2082, 0.2066, 0.2039, 0.1994, 0.1982, 0.1959, 0.1953]

================================================================================

History file: model_versions/chess_elo_model_V8_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:02:26.323658Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game10501_game12000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.247814   (epoch 10)   mode=min
  policy_loss: 2.151643   (epoch 10)   mode=min
  policy_policy_acc: 0.341566   (epoch 10)   mode=max
  val_loss: 2.682851   (epoch 5)   mode=min
  val_policy_loss: 2.585186   (epoch 5)   mode=min
  val_policy_policy_acc: 0.266502   (epoch 7)   mode=max
  val_value_loss: 0.188074   (epoch 10)   mode=min
  val_value_value_mse: 0.188065   (epoch 10)   mode=min
  value_loss: 0.192394   (epoch 10)   mode=min
  value_value_mse: 0.192394   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7759, 2.6885, 2.6254, 2.5675, 2.4675, 2.4202, 2.3763, 2.3102, 2.2834, 2.2478]
  policy_loss : [2.6679, 2.5838, 2.5224, 2.4653, 2.3675, 2.3214, 2.2782, 2.2134, 2.1866, 2.1516]
  policy_policy_acc : [0.2418, 0.2561, 0.2659, 0.2777, 0.2980, 0.3077, 0.3158, 0.3279, 0.3359, 0.3416]
  val_loss : [2.7012, 2.6950, 2.6922, 2.7143, 2.6829, 2.6917, 2.7147, 2.7203, 2.7370, 2.7488]
  val_policy_loss : [2.5975, 2.5924, 2.5927, 2.6144, 2.5852, 2.5955, 2.6194, 2.6255, 2.6426, 2.6548]
  val_policy_policy_acc : [0.2530, 0.2571, 0.2568, 0.2530, 0.2648, 0.2662, 0.2665, 0.2625, 0.2608, 0.2614]
  val_value_loss : [0.2073, 0.2051, 0.1990, 0.1999, 0.1954, 0.1924, 0.1905, 0.1896, 0.1889, 0.1881]
  val_value_value_mse : [0.2073, 0.2051, 0.1990, 0.1999, 0.1954, 0.1924, 0.1905, 0.1896, 0.1889, 0.1881]
  value_loss : [0.2158, 0.2095, 0.2061, 0.2044, 0.1998, 0.1977, 0.1962, 0.1937, 0.1936, 0.1924]
  value_value_mse : [0.2158, 0.2095, 0.2061, 0.2045, 0.1998, 0.1977, 0.1962, 0.1937, 0.1936, 0.1924]

================================================================================

History file: model_versions/chess_elo_model_V9_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:12:28.618098Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game12001_game13500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.251614   (epoch 9)   mode=min
  policy_loss: 2.156439   (epoch 9)   mode=min
  policy_policy_acc: 0.340845   (epoch 9)   mode=max
  val_loss: 2.689827   (epoch 3)   mode=min
  val_policy_loss: 2.589972   (epoch 3)   mode=min
  val_policy_policy_acc: 0.263666   (epoch 8)   mode=max
  val_value_loss: 0.187689   (epoch 9)   mode=min
  val_value_value_mse: 0.187695   (epoch 9)   mode=min
  value_loss: 0.190331   (epoch 9)   mode=min
  value_value_mse: 0.190331   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7515, 2.6634, 2.5993, 2.5449, 2.4920, 2.3923, 2.3458, 2.2808, 2.2516]
  policy_loss : [2.6442, 2.5587, 2.4962, 2.4431, 2.3917, 2.2943, 2.2486, 2.1850, 2.1564]
  policy_policy_acc : [0.2469, 0.2641, 0.2753, 0.2841, 0.2957, 0.3127, 0.3217, 0.3367, 0.3408]
  val_loss : [2.7025, 2.6969, 2.6898, 2.7015, 2.7425, 2.7143, 2.7210, 2.7414, 2.7335]
  val_policy_loss : [2.5997, 2.5955, 2.5900, 2.6001, 2.6431, 2.6183, 2.6267, 2.6471, 2.6396]
  val_policy_policy_acc : [0.2564, 0.2545, 0.2520, 0.2549, 0.2481, 0.2594, 0.2570, 0.2637, 0.2621]
  val_value_loss : [0.2060, 0.2031, 0.1999, 0.2031, 0.1990, 0.1922, 0.1884, 0.1885, 0.1877]
  val_value_value_mse : [0.2060, 0.2032, 0.1999, 0.2031, 0.1989, 0.1922, 0.1884, 0.1885, 0.1877]
  value_loss : [0.2146, 0.2094, 0.2061, 0.2035, 0.2007, 0.1961, 0.1944, 0.1916, 0.1903]
  value_value_mse : [0.2146, 0.2094, 0.2061, 0.2035, 0.2007, 0.1961, 0.1944, 0.1916, 0.1903]

================================================================================

History file: model_versions/chess_elo_model_V10_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:20:58.113402Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game13501_game15000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.271215   (epoch 9)   mode=min
  policy_loss: 2.176704   (epoch 9)   mode=min
  policy_policy_acc: 0.336065   (epoch 9)   mode=max
  val_loss: 2.694877   (epoch 3)   mode=min
  val_policy_loss: 2.593658   (epoch 3)   mode=min
  val_policy_policy_acc: 0.267949   (epoch 8)   mode=max
  val_value_loss: 0.187883   (epoch 8)   mode=min
  val_value_value_mse: 0.187909   (epoch 8)   mode=min
  value_loss: 0.189069   (epoch 9)   mode=min
  value_value_mse: 0.189050   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7427, 2.6614, 2.6036, 2.5529, 2.5017, 2.4071, 2.3591, 2.2995, 2.2712]
  policy_loss : [2.6368, 2.5572, 2.5014, 2.4517, 2.4018, 2.3096, 2.2626, 2.2045, 2.1767]
  policy_policy_acc : [0.2498, 0.2641, 0.2736, 0.2843, 0.2917, 0.3092, 0.3206, 0.3325, 0.3361]
  val_loss : [2.7060, 2.6953, 2.6949, 2.7045, 2.7113, 2.7030, 2.7163, 2.7337, 2.7333]
  val_policy_loss : [2.6032, 2.5943, 2.5937, 2.6058, 2.6149, 2.6059, 2.6219, 2.6404, 2.6396]
  val_policy_policy_acc : [0.2617, 0.2597, 0.2625, 0.2566, 0.2609, 0.2649, 0.2629, 0.2679, 0.2668]
  val_value_loss : [0.2064, 0.2028, 0.2035, 0.1984, 0.1938, 0.1953, 0.1897, 0.1879, 0.1887]
  val_value_value_mse : [0.2065, 0.2028, 0.2035, 0.1984, 0.1938, 0.1953, 0.1898, 0.1879, 0.1887]
  value_loss : [0.2122, 0.2080, 0.2044, 0.2029, 0.2000, 0.1950, 0.1930, 0.1904, 0.1891]
  value_value_mse : [0.2121, 0.2079, 0.2043, 0.2029, 0.2000, 0.1950, 0.1930, 0.1904, 0.1891]

================================================================================

History file: model_versions/chess_elo_model_V11_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:40:33.754617Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game15001_game16500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.253505   (epoch 10)   mode=min
  policy_loss: 2.159486   (epoch 10)   mode=min
  policy_policy_acc: 0.340202   (epoch 10)   mode=max
  val_loss: 2.685347   (epoch 4)   mode=min
  val_policy_loss: 2.586524   (epoch 4)   mode=min
  val_policy_policy_acc: 0.259397   (epoch 9)   mode=max
  val_value_loss: 0.182821   (epoch 10)   mode=min
  val_value_value_mse: 0.182849   (epoch 10)   mode=min
  value_loss: 0.188109   (epoch 10)   mode=min
  value_value_mse: 0.188107   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7375, 2.6669, 2.6095, 2.5611, 2.5176, 2.4729, 2.3774, 2.3352, 2.2802, 2.2535]
  policy_loss : [2.6312, 2.5633, 2.5071, 2.4601, 2.4176, 2.3735, 2.2806, 2.2396, 2.1856, 2.1595]
  policy_policy_acc : [0.2484, 0.2636, 0.2734, 0.2805, 0.2892, 0.2971, 0.3173, 0.3248, 0.3356, 0.3402]
  val_loss : [2.6992, 2.7099, 2.6867, 2.6853, 2.7029, 2.6984, 2.6897, 2.7129, 2.7217, 2.7253]
  val_policy_loss : [2.5981, 2.6077, 2.5874, 2.5865, 2.6046, 2.6016, 2.5950, 2.6192, 2.6289, 2.6337]
  val_policy_policy_acc : [0.2536, 0.2515, 0.2557, 0.2582, 0.2497, 0.2563, 0.2538, 0.2558, 0.2594, 0.2575]
  val_value_loss : [0.2020, 0.2041, 0.1982, 0.1971, 0.1962, 0.1932, 0.1891, 0.1870, 0.1853, 0.1828]
  val_value_value_mse : [0.2020, 0.2041, 0.1982, 0.1971, 0.1963, 0.1932, 0.1891, 0.1870, 0.1853, 0.1828]
  value_loss : [0.2126, 0.2071, 0.2047, 0.2019, 0.2000, 0.1987, 0.1936, 0.1912, 0.1892, 0.1881]
  value_value_mse : [0.2126, 0.2071, 0.2047, 0.2019, 0.2000, 0.1987, 0.1936, 0.1912, 0.1892, 0.1881]

================================================================================

History file: model_versions/chess_elo_model_V12_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:50:13.320503Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game16501_game18000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.284775   (epoch 10)   mode=min
  policy_loss: 2.191383   (epoch 10)   mode=min
  policy_policy_acc: 0.335826   (epoch 10)   mode=max
  val_loss: 2.680855   (epoch 6)   mode=min
  val_policy_loss: 2.587293   (epoch 6)   mode=min
  val_policy_policy_acc: 0.260339   (epoch 9)   mode=max
  val_value_loss: 0.183063   (epoch 10)   mode=min
  val_value_value_mse: 0.183024   (epoch 10)   mode=min
  value_loss: 0.186619   (epoch 10)   mode=min
  value_value_mse: 0.186604   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7456, 2.6699, 2.6157, 2.5691, 2.5241, 2.4365, 2.3984, 2.3618, 2.3055, 2.2848]
  policy_loss : [2.6406, 2.5676, 2.5146, 2.4694, 2.4257, 2.3403, 2.3031, 2.2673, 2.2119, 2.1914]
  policy_policy_acc : [0.2472, 0.2620, 0.2724, 0.2803, 0.2904, 0.3068, 0.3138, 0.3200, 0.3319, 0.3358]
  val_loss : [2.7023, 2.6929, 2.6896, 2.6964, 2.6930, 2.6809, 2.6849, 2.7115, 2.7038, 2.7117]
  val_policy_loss : [2.6000, 2.5941, 2.5914, 2.5992, 2.5971, 2.5873, 2.5919, 2.6189, 2.6115, 2.6199]
  val_policy_policy_acc : [0.2520, 0.2513, 0.2524, 0.2485, 0.2532, 0.2556, 0.2555, 0.2579, 0.2603, 0.2597]
  val_value_loss : [0.2042, 0.1972, 0.1962, 0.1936, 0.1914, 0.1869, 0.1857, 0.1848, 0.1839, 0.1831]
  val_value_value_mse : [0.2043, 0.1971, 0.1961, 0.1936, 0.1914, 0.1869, 0.1857, 0.1848, 0.1838, 0.1830]
  value_loss : [0.2098, 0.2048, 0.2018, 0.1995, 0.1970, 0.1925, 0.1906, 0.1892, 0.1870, 0.1866]
  value_value_mse : [0.2098, 0.2048, 0.2018, 0.1995, 0.1970, 0.1925, 0.1906, 0.1892, 0.1870, 0.1866]

================================================================================

History file: model_versions/chess_elo_model_V13_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T19:59:51.974514Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game18001_game19500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.272865   (epoch 10)   mode=min
  policy_loss: 2.179070   (epoch 10)   mode=min
  policy_policy_acc: 0.337098   (epoch 10)   mode=max
  val_loss: 2.670685   (epoch 4)   mode=min
  val_policy_loss: 2.571863   (epoch 4)   mode=min
  val_policy_policy_acc: 0.268382   (epoch 9)   mode=max
  val_value_loss: 0.182077   (epoch 10)   mode=min
  val_value_value_mse: 0.182047   (epoch 10)   mode=min
  value_loss: 0.187662   (epoch 10)   mode=min
  value_value_mse: 0.187662   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7464, 2.6668, 2.6139, 2.5665, 2.5234, 2.4819, 2.3913, 2.3526, 2.2960, 2.2729]
  policy_loss : [2.6404, 2.5630, 2.5117, 2.4656, 2.4233, 2.3825, 2.2944, 2.2570, 2.2015, 2.1791]
  policy_policy_acc : [0.2468, 0.2628, 0.2703, 0.2802, 0.2898, 0.2948, 0.3114, 0.3192, 0.3329, 0.3371]
  val_loss : [2.6830, 2.6769, 2.6711, 2.6707, 2.7029, 2.6907, 2.6870, 2.6913, 2.6922, 2.6953]
  val_policy_loss : [2.5810, 2.5774, 2.5727, 2.5719, 2.6020, 2.5933, 2.5919, 2.5987, 2.5999, 2.6042]
  val_policy_policy_acc : [0.2567, 0.2628, 0.2634, 0.2646, 0.2550, 0.2602, 0.2624, 0.2657, 0.2684, 0.2670]
  val_value_loss : [0.2038, 0.1989, 0.1967, 0.1974, 0.2019, 0.1951, 0.1905, 0.1853, 0.1845, 0.1821]
  val_value_value_mse : [0.2038, 0.1989, 0.1967, 0.1975, 0.2019, 0.1951, 0.1904, 0.1853, 0.1845, 0.1820]
  value_loss : [0.2120, 0.2076, 0.2043, 0.2018, 0.2003, 0.1987, 0.1938, 0.1912, 0.1889, 0.1877]
  value_value_mse : [0.2120, 0.2076, 0.2043, 0.2018, 0.2003, 0.1987, 0.1938, 0.1912, 0.1889, 0.1877]

================================================================================

History file: model_versions/chess_elo_model_V14_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:09:32.006645Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game19501_game21000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.297119   (epoch 10)   mode=min
  policy_loss: 2.200397   (epoch 10)   mode=min
  policy_policy_acc: 0.333994   (epoch 10)   mode=max
  val_loss: 2.659142   (epoch 4)   mode=min
  val_policy_loss: 2.557626   (epoch 4)   mode=min
  val_policy_policy_acc: 0.270441   (epoch 10)   mode=max
  val_value_loss: 0.189147   (epoch 10)   mode=min
  val_value_value_mse: 0.189151   (epoch 10)   mode=min
  value_loss: 0.193430   (epoch 10)   mode=min
  value_value_mse: 0.193429   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7476, 2.6738, 2.6254, 2.5788, 2.5337, 2.4964, 2.4120, 2.3717, 2.3201, 2.2971]
  policy_loss : [2.6391, 2.5671, 2.5200, 2.4750, 2.4308, 2.3945, 2.3128, 2.2729, 2.2226, 2.2004]
  policy_policy_acc : [0.2471, 0.2620, 0.2678, 0.2797, 0.2885, 0.2932, 0.3111, 0.3191, 0.3272, 0.3340]
  val_loss : [2.6727, 2.6650, 2.6688, 2.6591, 2.6690, 2.6871, 2.6703, 2.6737, 2.6748, 2.6882]
  val_policy_loss : [2.5690, 2.5614, 2.5658, 2.5576, 2.5694, 2.5886, 2.5745, 2.5777, 2.5807, 2.5941]
  val_policy_policy_acc : [0.2609, 0.2573, 0.2638, 0.2643, 0.2619, 0.2619, 0.2677, 0.2695, 0.2668, 0.2704]
  val_value_loss : [0.2085, 0.2080, 0.2070, 0.2040, 0.1999, 0.1979, 0.1927, 0.1930, 0.1893, 0.1891]
  val_value_value_mse : [0.2085, 0.2080, 0.2070, 0.2040, 0.1999, 0.1979, 0.1927, 0.1930, 0.1893, 0.1892]
  value_loss : [0.2170, 0.2135, 0.2108, 0.2077, 0.2060, 0.2037, 0.1984, 0.1975, 0.1950, 0.1934]
  value_value_mse : [0.2170, 0.2135, 0.2108, 0.2077, 0.2059, 0.2037, 0.1984, 0.1975, 0.1950, 0.1934]

================================================================================

History file: model_versions/chess_elo_model_V15_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:19:22.979061Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game21001_game22500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.354832   (epoch 10)   mode=min
  policy_loss: 2.263931   (epoch 10)   mode=min
  policy_policy_acc: 0.319187   (epoch 10)   mode=max
  val_loss: 2.680071   (epoch 7)   mode=min
  val_policy_loss: 2.589571   (epoch 7)   mode=min
  val_policy_policy_acc: 0.266188   (epoch 10)   mode=max
  val_value_loss: 0.178810   (epoch 10)   mode=min
  val_value_value_mse: 0.178828   (epoch 10)   mode=min
  value_loss: 0.182178   (epoch 10)   mode=min
  value_value_mse: 0.182185   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.7557, 2.6835, 2.6343, 2.5913, 2.5102, 2.4770, 2.4244, 2.4013, 2.3809, 2.3548]
  policy_loss : [2.6528, 2.5832, 2.5351, 2.4935, 2.4145, 2.3825, 2.3314, 2.3094, 2.2890, 2.2639]
  policy_policy_acc : [0.2454, 0.2586, 0.2667, 0.2740, 0.2897, 0.2950, 0.3055, 0.3111, 0.3143, 0.3192]
  val_loss : [2.6966, 2.6906, 2.6987, 2.6877, 2.6847, 2.6865, 2.6801, 2.6930, 2.6839, 2.6847]
  val_policy_loss : [2.5957, 2.5918, 2.6025, 2.5936, 2.5918, 2.5945, 2.5896, 2.6026, 2.5935, 2.5955]
  val_policy_policy_acc : [0.2561, 0.2571, 0.2577, 0.2589, 0.2636, 0.2599, 0.2640, 0.2633, 0.2661, 0.2662]
  val_value_loss : [0.2018, 0.1980, 0.1928, 0.1888, 0.1860, 0.1844, 0.1814, 0.1814, 0.1812, 0.1788]
  val_value_value_mse : [0.2018, 0.1981, 0.1928, 0.1888, 0.1861, 0.1844, 0.1814, 0.1814, 0.1812, 0.1788]
  value_loss : [0.2056, 0.2007, 0.1983, 0.1957, 0.1911, 0.1889, 0.1863, 0.1842, 0.1836, 0.1822]
  value_value_mse : [0.2056, 0.2007, 0.1983, 0.1957, 0.1911, 0.1889, 0.1863, 0.1842, 0.1836, 0.1822]

================================================================================

History file: model_versions/chess_elo_model_V16_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:28:51.354778Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game22501_game24000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.346818   (epoch 10)   mode=min
  policy_loss: 2.257146   (epoch 10)   mode=min
  policy_policy_acc: 0.319816   (epoch 10)   mode=max
  val_loss: 2.641300   (epoch 5)   mode=min
  val_policy_loss: 2.550546   (epoch 5)   mode=min
  val_policy_policy_acc: 0.268242   (epoch 6)   mode=max
  val_value_loss: 0.175670   (epoch 10)   mode=min
  val_value_value_mse: 0.175628   (epoch 10)   mode=min
  value_loss: 0.179192   (epoch 10)   mode=min
  value_value_mse: 0.179190   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7507, 2.6802, 2.6342, 2.5929, 2.5110, 2.4757, 2.4451, 2.3943, 2.3740, 2.3468]
  policy_loss : [2.6492, 2.5803, 2.5362, 2.4968, 2.4174, 2.3825, 2.3527, 2.3034, 2.2837, 2.2571]
  policy_policy_acc : [0.2450, 0.2584, 0.2670, 0.2735, 0.2890, 0.2958, 0.3027, 0.3105, 0.3150, 0.3198]
  val_loss : [2.6854, 2.6726, 2.6807, 2.6922, 2.6413, 2.6532, 2.6519, 2.6568, 2.6544, 2.6533]
  val_policy_loss : [2.5867, 2.5755, 2.5855, 2.5976, 2.5505, 2.5615, 2.5618, 2.5685, 2.5655, 2.5651]
  val_policy_policy_acc : [0.2530, 0.2543, 0.2540, 0.2495, 0.2662, 0.2682, 0.2660, 0.2674, 0.2659, 0.2671]
  val_value_loss : [0.1963, 0.1935, 0.1896, 0.1885, 0.1805, 0.1826, 0.1796, 0.1758, 0.1770, 0.1757]
  val_value_value_mse : [0.1963, 0.1935, 0.1896, 0.1885, 0.1805, 0.1825, 0.1796, 0.1758, 0.1769, 0.1756]
  value_loss : [0.2030, 0.1999, 0.1959, 0.1922, 0.1872, 0.1864, 0.1847, 0.1818, 0.1806, 0.1792]
  value_value_mse : [0.2030, 0.1999, 0.1959, 0.1922, 0.1872, 0.1864, 0.1847, 0.1818, 0.1806, 0.1792]

================================================================================

History file: model_versions/chess_elo_model_V17_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:38:11.363507Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game24001_game25500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.333863   (epoch 10)   mode=min
  policy_loss: 2.244096   (epoch 10)   mode=min
  policy_policy_acc: 0.325145   (epoch 10)   mode=max
  val_loss: 2.654166   (epoch 6)   mode=min
  val_policy_loss: 2.564214   (epoch 6)   mode=min
  val_policy_policy_acc: 0.272556   (epoch 10)   mode=max
  val_value_loss: 0.172965   (epoch 10)   mode=min
  val_value_value_mse: 0.172922   (epoch 10)   mode=min
  value_loss: 0.179382   (epoch 10)   mode=min
  value_value_mse: 0.179383   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7359, 2.6714, 2.6222, 2.5791, 2.5424, 2.4661, 2.4285, 2.4027, 2.3527, 2.3339]
  policy_loss : [2.6342, 2.5715, 2.5238, 2.4821, 2.4459, 2.3731, 2.3361, 2.3109, 2.2621, 2.2441]
  policy_policy_acc : [0.2508, 0.2612, 0.2716, 0.2775, 0.2853, 0.3010, 0.3071, 0.3112, 0.3213, 0.3251]
  val_loss : [2.6925, 2.6912, 2.6788, 2.6793, 2.6885, 2.6542, 2.6641, 2.6662, 2.6644, 2.6678]
  val_policy_loss : [2.5933, 2.5960, 2.5832, 2.5858, 2.5956, 2.5642, 2.5745, 2.5775, 2.5760, 2.5811]
  val_policy_policy_acc : [0.2572, 0.2572, 0.2624, 0.2661, 0.2657, 0.2683, 0.2652, 0.2714, 0.2699, 0.2726]
  val_value_loss : [0.1974, 0.1897, 0.1910, 0.1860, 0.1855, 0.1796, 0.1785, 0.1771, 0.1765, 0.1730]
  val_value_value_mse : [0.1974, 0.1897, 0.1909, 0.1859, 0.1855, 0.1796, 0.1785, 0.1771, 0.1764, 0.1729]
  value_loss : [0.2038, 0.1995, 0.1967, 0.1941, 0.1928, 0.1864, 0.1846, 0.1839, 0.1813, 0.1794]
  value_value_mse : [0.2038, 0.1995, 0.1967, 0.1941, 0.1928, 0.1864, 0.1846, 0.1839, 0.1813, 0.1794]

================================================================================

History file: model_versions/chess_elo_model_V18_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:47:34.928365Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game25501_game27000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.340350   (epoch 10)   mode=min
  policy_loss: 2.249962   (epoch 10)   mode=min
  policy_policy_acc: 0.323946   (epoch 10)   mode=max
  val_loss: 2.626486   (epoch 6)   mode=min
  val_policy_loss: 2.536066   (epoch 6)   mode=min
  val_policy_policy_acc: 0.278986   (epoch 6)   mode=max
  val_value_loss: 0.173324   (epoch 10)   mode=min
  val_value_value_mse: 0.173272   (epoch 10)   mode=min
  value_loss: 0.180763   (epoch 10)   mode=min
  value_value_mse: 0.180733   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7353, 2.6689, 2.6230, 2.5849, 2.5478, 2.4691, 2.4373, 2.4092, 2.3593, 2.3404]
  policy_loss : [2.6333, 2.5690, 2.5247, 2.4869, 2.4521, 2.3747, 2.3447, 2.3167, 2.2684, 2.2500]
  policy_policy_acc : [0.2508, 0.2625, 0.2708, 0.2780, 0.2839, 0.3003, 0.3070, 0.3098, 0.3192, 0.3239]
  val_loss : [2.6464, 2.6682, 2.6376, 2.6470, 2.6455, 2.6265, 2.6324, 2.6368, 2.6394, 2.6471]
  val_policy_loss : [2.5512, 2.5747, 2.5434, 2.5520, 2.5541, 2.5361, 2.5433, 2.5499, 2.5527, 2.5610]
  val_policy_policy_acc : [0.2627, 0.2633, 0.2667, 0.2720, 0.2692, 0.2790, 0.2749, 0.2697, 0.2739, 0.2723]
  val_value_loss : [0.1907, 0.1885, 0.1893, 0.1912, 0.1836, 0.1818, 0.1786, 0.1747, 0.1740, 0.1733]
  val_value_value_mse : [0.1906, 0.1885, 0.1893, 0.1911, 0.1836, 0.1818, 0.1786, 0.1746, 0.1740, 0.1733]
  value_loss : [0.2046, 0.2002, 0.1965, 0.1960, 0.1931, 0.1876, 0.1859, 0.1844, 0.1816, 0.1808]
  value_value_mse : [0.2046, 0.2001, 0.1965, 0.1959, 0.1930, 0.1876, 0.1859, 0.1844, 0.1815, 0.1807]

================================================================================

History file: model_versions/chess_elo_model_V19_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T20:56:59.336575Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game27001_game28500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.365065   (epoch 10)   mode=min
  policy_loss: 2.273381   (epoch 10)   mode=min
  policy_policy_acc: 0.317030   (epoch 10)   mode=max
  val_loss: 2.637437   (epoch 5)   mode=min
  val_policy_loss: 2.550650   (epoch 5)   mode=min
  val_policy_policy_acc: 0.273331   (epoch 10)   mode=max
  val_value_loss: 0.178070   (epoch 9)   mode=min
  val_value_value_mse: 0.177967   (epoch 9)   mode=min
  value_loss: 0.183361   (epoch 10)   mode=min
  value_value_mse: 0.183362   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7392, 2.6763, 2.6355, 2.5967, 2.5169, 2.4838, 2.4578, 2.4116, 2.3917, 2.3651]
  policy_loss : [2.6356, 2.5751, 2.5355, 2.4979, 2.4212, 2.3892, 2.3636, 2.3192, 2.2993, 2.2734]
  policy_policy_acc : [0.2486, 0.2594, 0.2661, 0.2747, 0.2880, 0.2956, 0.3003, 0.3091, 0.3135, 0.3170]
  val_loss : [2.6728, 2.6589, 2.6825, 2.6587, 2.6374, 2.6440, 2.6528, 2.6435, 2.6513, 2.6510]
  val_policy_loss : [2.5754, 2.5640, 2.5882, 2.5667, 2.5506, 2.5556, 2.5666, 2.5579, 2.5661, 2.5657]
  val_policy_policy_acc : [0.2564, 0.2651, 0.2596, 0.2643, 0.2692, 0.2673, 0.2694, 0.2709, 0.2719, 0.2733]
  val_value_loss : [0.1988, 0.1941, 0.1958, 0.1908, 0.1821, 0.1836, 0.1806, 0.1787, 0.1781, 0.1786]
  val_value_value_mse : [0.1988, 0.1940, 0.1957, 0.1907, 0.1821, 0.1835, 0.1807, 0.1787, 0.1780, 0.1786]
  value_loss : [0.2070, 0.2024, 0.2001, 0.1976, 0.1915, 0.1892, 0.1884, 0.1850, 0.1849, 0.1834]
  value_value_mse : [0.2070, 0.2024, 0.2001, 0.1976, 0.1915, 0.1892, 0.1884, 0.1850, 0.1849, 0.1834]

================================================================================

History file: model_versions/chess_elo_model_V20_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:06:28.861087Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game28501_game30000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.366857   (epoch 10)   mode=min
  policy_loss: 2.273059   (epoch 10)   mode=min
  policy_policy_acc: 0.318467   (epoch 10)   mode=max
  val_loss: 2.683872   (epoch 4)   mode=min
  val_policy_loss: 2.588251   (epoch 4)   mode=min
  val_policy_policy_acc: 0.265284   (epoch 9)   mode=max
  val_value_loss: 0.180903   (epoch 9)   mode=min
  val_value_value_mse: 0.180903   (epoch 9)   mode=min
  value_loss: 0.187595   (epoch 10)   mode=min
  value_value_mse: 0.187596   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7456, 2.6890, 2.6448, 2.6107, 2.5720, 2.5418, 2.4674, 2.4355, 2.3872, 2.3669]
  policy_loss : [2.6404, 2.5853, 2.5428, 2.5100, 2.4722, 2.4426, 2.3710, 2.3400, 2.2930, 2.2731]
  policy_policy_acc : [0.2459, 0.2568, 0.2650, 0.2706, 0.2798, 0.2829, 0.2978, 0.3041, 0.3135, 0.3185]
  val_loss : [2.7145, 2.7020, 2.6967, 2.6839, 2.6978, 2.6957, 2.6889, 2.6928, 2.6928, 2.6937]
  val_policy_loss : [2.6146, 2.6010, 2.5982, 2.5883, 2.5996, 2.5988, 2.5970, 2.6006, 2.6024, 2.6026]
  val_policy_policy_acc : [0.2515, 0.2511, 0.2574, 0.2594, 0.2537, 0.2575, 0.2607, 0.2606, 0.2653, 0.2646]
  val_value_loss : [0.2000, 0.2021, 0.1970, 0.1912, 0.1963, 0.1937, 0.1838, 0.1845, 0.1809, 0.1821]
  val_value_value_mse : [0.2000, 0.2021, 0.1970, 0.1912, 0.1963, 0.1937, 0.1838, 0.1845, 0.1809, 0.1821]
  value_loss : [0.2104, 0.2075, 0.2039, 0.2015, 0.1995, 0.1984, 0.1927, 0.1910, 0.1886, 0.1876]
  value_value_mse : [0.2104, 0.2075, 0.2039, 0.2015, 0.1995, 0.1984, 0.1927, 0.1910, 0.1886, 0.1876]

================================================================================

History file: model_versions/chess_elo_model_V21_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:15:41.981372Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game30001_game31500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.365726   (epoch 10)   mode=min
  policy_loss: 2.272510   (epoch 10)   mode=min
  policy_policy_acc: 0.318195   (epoch 10)   mode=max
  val_loss: 2.666609   (epoch 10)   mode=min
  val_policy_loss: 2.571940   (epoch 4)   mode=min
  val_policy_policy_acc: 0.274301   (epoch 8)   mode=max
  val_value_loss: 0.180643   (epoch 9)   mode=min
  val_value_value_mse: 0.180557   (epoch 9)   mode=min
  value_loss: 0.186403   (epoch 10)   mode=min
  value_value_mse: 0.186404   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7420, 2.6800, 2.6395, 2.6050, 2.5686, 2.5342, 2.4615, 2.4303, 2.3831, 2.3657]
  policy_loss : [2.6371, 2.5767, 2.5376, 2.5045, 2.4697, 2.4359, 2.3651, 2.3352, 2.2890, 2.2725]
  policy_policy_acc : [0.2490, 0.2605, 0.2688, 0.2760, 0.2803, 0.2871, 0.3012, 0.3071, 0.3157, 0.3182]
  val_loss : [2.6914, 2.6816, 2.6805, 2.6707, 2.6758, 2.6918, 2.6744, 2.6742, 2.6723, 2.6666]
  val_policy_loss : [2.5899, 2.5803, 2.5764, 2.5719, 2.5774, 2.5921, 2.5775, 2.5769, 2.5778, 2.5722]
  val_policy_policy_acc : [0.2652, 0.2624, 0.2637, 0.2638, 0.2630, 0.2604, 0.2670, 0.2743, 0.2709, 0.2704]
  val_value_loss : [0.2003, 0.1963, 0.2005, 0.1929, 0.1902, 0.1926, 0.1858, 0.1875, 0.1806, 0.1809]
  val_value_value_mse : [0.2001, 0.1963, 0.2005, 0.1929, 0.1901, 0.1924, 0.1857, 0.1873, 0.1806, 0.1809]
  value_loss : [0.2099, 0.2065, 0.2040, 0.2008, 0.1978, 0.1965, 0.1927, 0.1901, 0.1881, 0.1864]
  value_value_mse : [0.2099, 0.2065, 0.2040, 0.2008, 0.1978, 0.1965, 0.1927, 0.1901, 0.1881, 0.1864]

================================================================================

History file: model_versions/chess_elo_model_V22_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:25:07.500830Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game31501_game33000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.385959   (epoch 10)   mode=min
  policy_loss: 2.288521   (epoch 10)   mode=min
  policy_policy_acc: 0.313768   (epoch 10)   mode=max
  val_loss: 2.650538   (epoch 6)   mode=min
  val_policy_loss: 2.552188   (epoch 6)   mode=min
  val_policy_policy_acc: 0.267834   (epoch 7)   mode=max
  val_value_loss: 0.188116   (epoch 10)   mode=min
  val_value_value_mse: 0.188128   (epoch 10)   mode=min
  value_loss: 0.194774   (epoch 10)   mode=min
  value_value_mse: 0.194757   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7362, 2.6841, 2.6436, 2.6078, 2.5728, 2.5042, 2.4730, 2.4468, 2.4046, 2.3860]
  policy_loss : [2.6285, 2.5787, 2.5388, 2.5043, 2.4706, 2.4037, 2.3736, 2.3485, 2.3073, 2.2885]
  policy_policy_acc : [0.2486, 0.2582, 0.2652, 0.2737, 0.2788, 0.2911, 0.2983, 0.3023, 0.3103, 0.3138]
  val_loss : [2.6785, 2.6793, 2.6616, 2.6724, 2.6631, 2.6505, 2.6533, 2.6618, 2.6602, 2.6699]
  val_policy_loss : [2.5760, 2.5740, 2.5599, 2.5717, 2.5623, 2.5522, 2.5571, 2.5667, 2.5658, 2.5760]
  val_policy_policy_acc : [0.2638, 0.2643, 0.2633, 0.2622, 0.2622, 0.2665, 0.2678, 0.2651, 0.2668, 0.2658]
  val_value_loss : [0.2045, 0.2107, 0.2033, 0.2020, 0.2016, 0.1968, 0.1926, 0.1904, 0.1888, 0.1881]
  val_value_value_mse : [0.2045, 0.2106, 0.2033, 0.2020, 0.2016, 0.1968, 0.1927, 0.1904, 0.1888, 0.1881]
  value_loss : [0.2158, 0.2106, 0.2092, 0.2070, 0.2050, 0.2014, 0.1996, 0.1973, 0.1951, 0.1948]
  value_value_mse : [0.2158, 0.2106, 0.2092, 0.2070, 0.2050, 0.2014, 0.1996, 0.1973, 0.1951, 0.1948]

================================================================================

History file: model_versions/chess_elo_model_V23_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:34:38.218180Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game33001_game34500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.394122   (epoch 10)   mode=min
  policy_loss: 2.299514   (epoch 10)   mode=min
  policy_policy_acc: 0.314730   (epoch 10)   mode=max
  val_loss: 2.652321   (epoch 6)   mode=min
  val_policy_loss: 2.556564   (epoch 6)   mode=min
  val_policy_policy_acc: 0.272439   (epoch 10)   mode=max
  val_value_loss: 0.185810   (epoch 10)   mode=min
  val_value_value_mse: 0.185765   (epoch 10)   mode=min
  value_loss: 0.189202   (epoch 9)   mode=min
  value_value_mse: 0.189202   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7421, 2.6837, 2.6427, 2.6098, 2.5747, 2.5022, 2.4751, 2.4514, 2.4065, 2.3941]
  policy_loss : [2.6377, 2.5811, 2.5413, 2.5096, 2.4750, 2.4048, 2.3784, 2.3557, 2.3118, 2.2995]
  policy_policy_acc : [0.2484, 0.2587, 0.2664, 0.2736, 0.2801, 0.2922, 0.2995, 0.3013, 0.3118, 0.3147]
  val_loss : [2.6750, 2.6653, 2.6612, 2.6617, 2.6738, 2.6523, 2.6562, 2.6680, 2.6682, 2.6621]
  val_policy_loss : [2.5725, 2.5652, 2.5632, 2.5639, 2.5763, 2.5566, 2.5619, 2.5738, 2.5753, 2.5691]
  val_policy_policy_acc : [0.2617, 0.2630, 0.2603, 0.2666, 0.2629, 0.2705, 0.2703, 0.2694, 0.2716, 0.2724]
  val_value_loss : [0.2050, 0.1998, 0.1959, 0.1956, 0.1947, 0.1914, 0.1885, 0.1885, 0.1858, 0.1858]
  val_value_value_mse : [0.2051, 0.1999, 0.1959, 0.1955, 0.1947, 0.1914, 0.1884, 0.1884, 0.1858, 0.1858]
  value_loss : [0.2088, 0.2053, 0.2029, 0.2004, 0.1994, 0.1950, 0.1935, 0.1913, 0.1892, 0.1892]
  value_value_mse : [0.2088, 0.2053, 0.2029, 0.2004, 0.1994, 0.1950, 0.1935, 0.1913, 0.1892, 0.1892]

================================================================================

History file: model_versions/chess_elo_model_V24_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:44:04.450359Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game34501_game36000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.358576   (epoch 10)   mode=min
  policy_loss: 2.261801   (epoch 10)   mode=min
  policy_policy_acc: 0.321389   (epoch 10)   mode=max
  val_loss: 2.661352   (epoch 4)   mode=min
  val_policy_loss: 2.563093   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279489   (epoch 9)   mode=max
  val_value_loss: 0.188253   (epoch 10)   mode=min
  val_value_value_mse: 0.188258   (epoch 10)   mode=min
  value_loss: 0.193687   (epoch 10)   mode=min
  value_value_mse: 0.193698   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7236, 2.6647, 2.6226, 2.5897, 2.5573, 2.5221, 2.4504, 2.4230, 2.3757, 2.3586]
  policy_loss : [2.6166, 2.5589, 2.5184, 2.4862, 2.4546, 2.4203, 2.3511, 2.3245, 2.2782, 2.2618]
  policy_policy_acc : [0.2527, 0.2646, 0.2720, 0.2772, 0.2847, 0.2900, 0.3024, 0.3095, 0.3182, 0.3214]
  val_loss : [2.6761, 2.6721, 2.6810, 2.6614, 2.6711, 2.6786, 2.6857, 2.6728, 2.6838, 2.6982]
  val_policy_loss : [2.5725, 2.5722, 2.5807, 2.5631, 2.5719, 2.5807, 2.5891, 2.5765, 2.5893, 2.6037]
  val_policy_policy_acc : [0.2640, 0.2696, 0.2671, 0.2705, 0.2697, 0.2711, 0.2735, 0.2743, 0.2795, 0.2770]
  val_value_loss : [0.2067, 0.1990, 0.2000, 0.1961, 0.1982, 0.1955, 0.1927, 0.1922, 0.1887, 0.1883]
  val_value_value_mse : [0.2067, 0.1991, 0.2000, 0.1961, 0.1982, 0.1956, 0.1927, 0.1922, 0.1887, 0.1883]
  value_loss : [0.2139, 0.2116, 0.2086, 0.2070, 0.2054, 0.2038, 0.1985, 0.1970, 0.1951, 0.1937]
  value_value_mse : [0.2139, 0.2116, 0.2086, 0.2070, 0.2054, 0.2038, 0.1985, 0.1970, 0.1951, 0.1937]

================================================================================

History file: model_versions/chess_elo_model_V25_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T21:53:26.757268Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game36001_game37500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.408222   (epoch 10)   mode=min
  policy_loss: 2.311429   (epoch 10)   mode=min
  policy_policy_acc: 0.310774   (epoch 10)   mode=max
  val_loss: 2.659966   (epoch 9)   mode=min
  val_policy_loss: 2.564272   (epoch 9)   mode=min
  val_policy_policy_acc: 0.273367   (epoch 9)   mode=max
  val_value_loss: 0.190201   (epoch 9)   mode=min
  val_value_value_mse: 0.190309   (epoch 9)   mode=min
  value_loss: 0.193553   (epoch 10)   mode=min
  value_value_mse: 0.193567   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7467, 2.6891, 2.6468, 2.6159, 2.5454, 2.5181, 2.4902, 2.4673, 2.4259, 2.4082]
  policy_loss : [2.6392, 2.5839, 2.5425, 2.5124, 2.4442, 2.4178, 2.3907, 2.3685, 2.3282, 2.3114]
  policy_policy_acc : [0.2477, 0.2575, 0.2679, 0.2745, 0.2857, 0.2912, 0.2956, 0.3000, 0.3072, 0.3108]
  val_loss : [2.6934, 2.6888, 2.7018, 2.7231, 2.6796, 2.6638, 2.6892, 2.6735, 2.6600, 2.6699]
  val_policy_loss : [2.5890, 2.5869, 2.6013, 2.6237, 2.5802, 2.5653, 2.5923, 2.5770, 2.5643, 2.5741]
  val_policy_policy_acc : [0.2627, 0.2624, 0.2638, 0.2614, 0.2688, 0.2721, 0.2708, 0.2733, 0.2734, 0.2719]
  val_value_loss : [0.2085, 0.2031, 0.2007, 0.1996, 0.1983, 0.1964, 0.1930, 0.1927, 0.1902, 0.1906]
  val_value_value_mse : [0.2085, 0.2031, 0.2009, 0.1996, 0.1983, 0.1966, 0.1931, 0.1928, 0.1903, 0.1907]
  value_loss : [0.2147, 0.2105, 0.2085, 0.2070, 0.2025, 0.2004, 0.1992, 0.1978, 0.1953, 0.1936]
  value_value_mse : [0.2147, 0.2105, 0.2085, 0.2070, 0.2024, 0.2004, 0.1992, 0.1978, 0.1953, 0.1936]

================================================================================

History file: model_versions/chess_elo_model_V26_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T22:02:43.227511Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game37501_game39000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.371580   (epoch 10)   mode=min
  policy_loss: 2.277283   (epoch 10)   mode=min
  policy_policy_acc: 0.320220   (epoch 10)   mode=max
  val_loss: 2.644410   (epoch 7)   mode=min
  val_policy_loss: 2.546614   (epoch 7)   mode=min
  val_policy_policy_acc: 0.277214   (epoch 10)   mode=max
  val_value_loss: 0.185633   (epoch 9)   mode=min
  val_value_value_mse: 0.185193   (epoch 9)   mode=min
  value_loss: 0.189165   (epoch 10)   mode=min
  value_value_mse: 0.189167   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7324, 2.6756, 2.6359, 2.6007, 2.5716, 2.5423, 2.4670, 2.4367, 2.4127, 2.3716]
  policy_loss : [2.6264, 2.5721, 2.5337, 2.4989, 2.4706, 2.4425, 2.3700, 2.3405, 2.3169, 2.2773]
  policy_policy_acc : [0.2520, 0.2632, 0.2719, 0.2776, 0.2814, 0.2890, 0.3023, 0.3081, 0.3126, 0.3202]
  val_loss : [2.6705, 2.6625, 2.6612, 2.6597, 2.6631, 2.6647, 2.6444, 2.6606, 2.6720, 2.6608]
  val_policy_loss : [2.5618, 2.5588, 2.5594, 2.5557, 2.5612, 2.5637, 2.5466, 2.5602, 2.5750, 2.5636]
  val_policy_policy_acc : [0.2666, 0.2647, 0.2696, 0.2743, 0.2706, 0.2674, 0.2769, 0.2738, 0.2770, 0.2772]
  val_value_loss : [0.2095, 0.2001, 0.1956, 0.2001, 0.1965, 0.1939, 0.1883, 0.1932, 0.1856, 0.1859]
  val_value_value_mse : [0.2091, 0.1997, 0.1952, 0.1994, 0.1958, 0.1935, 0.1879, 0.1928, 0.1852, 0.1854]
  value_loss : [0.2119, 0.2070, 0.2045, 0.2034, 0.2021, 0.1992, 0.1945, 0.1923, 0.1914, 0.1892]
  value_value_mse : [0.2119, 0.2070, 0.2046, 0.2034, 0.2021, 0.1992, 0.1944, 0.1923, 0.1914, 0.1892]

================================================================================

History file: model_versions/chess_elo_model_V27_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T22:11:56.370139Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game39001_game40500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.392035   (epoch 10)   mode=min
  policy_loss: 2.294216   (epoch 10)   mode=min
  policy_policy_acc: 0.316246   (epoch 10)   mode=max
  val_loss: 2.667110   (epoch 7)   mode=min
  val_policy_loss: 2.572191   (epoch 6)   mode=min
  val_policy_policy_acc: 0.274392   (epoch 9)   mode=max
  val_value_loss: 0.187106   (epoch 9)   mode=min
  val_value_value_mse: 0.187106   (epoch 9)   mode=min
  value_loss: 0.195617   (epoch 10)   mode=min
  value_value_mse: 0.195619   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7424, 2.6856, 2.6471, 2.6084, 2.5765, 2.5087, 2.4764, 2.4528, 2.4090, 2.3920]
  policy_loss : [2.6359, 2.5801, 2.5427, 2.5049, 2.4736, 2.4076, 2.3767, 2.3535, 2.3105, 2.2942]
  policy_policy_acc : [0.2517, 0.2631, 0.2690, 0.2760, 0.2816, 0.2952, 0.3001, 0.3058, 0.3136, 0.3162]
  val_loss : [2.6880, 2.6879, 2.6811, 2.6807, 2.6844, 2.6698, 2.6671, 2.6917, 2.6726, 2.6841]
  val_policy_loss : [2.5855, 2.5895, 2.5801, 2.5823, 2.5872, 2.5722, 2.5728, 2.5970, 2.5790, 2.5905]
  val_policy_policy_acc : [0.2632, 0.2599, 0.2599, 0.2622, 0.2628, 0.2692, 0.2740, 0.2717, 0.2744, 0.2738]
  val_value_loss : [0.2052, 0.1969, 0.2021, 0.1969, 0.1944, 0.1951, 0.1887, 0.1894, 0.1871, 0.1871]
  val_value_value_mse : [0.2052, 0.1969, 0.2021, 0.1969, 0.1944, 0.1951, 0.1887, 0.1894, 0.1871, 0.1871]
  value_loss : [0.2128, 0.2110, 0.2088, 0.2071, 0.2058, 0.2023, 0.1995, 0.1987, 0.1970, 0.1956]
  value_value_mse : [0.2128, 0.2110, 0.2088, 0.2071, 0.2058, 0.2023, 0.1995, 0.1987, 0.1970, 0.1956]

================================================================================

History file: model_versions/chess_elo_model_V28_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T22:20:44.908978Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game40501_game42000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.398967   (epoch 10)   mode=min
  policy_loss: 2.302789   (epoch 10)   mode=min
  policy_policy_acc: 0.312443   (epoch 10)   mode=max
  val_loss: 2.608772   (epoch 6)   mode=min
  val_policy_loss: 2.508262   (epoch 6)   mode=min
  val_policy_policy_acc: 0.278343   (epoch 10)   mode=max
  val_value_loss: 0.184743   (epoch 10)   mode=min
  val_value_value_mse: 0.185296   (epoch 10)   mode=min
  value_loss: 0.192351   (epoch 10)   mode=min
  value_value_mse: 0.192351   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7306, 2.6759, 2.6361, 2.6004, 2.5327, 2.4999, 2.4828, 2.4580, 2.4148, 2.3990]
  policy_loss : [2.6258, 2.5722, 2.5334, 2.4986, 2.4328, 2.4013, 2.3848, 2.3606, 2.3184, 2.3028]
  policy_policy_acc : [0.2526, 0.2612, 0.2681, 0.2769, 0.2872, 0.2948, 0.2968, 0.3012, 0.3104, 0.3124]
  val_loss : [2.6452, 2.6430, 2.6505, 2.6519, 2.6153, 2.6088, 2.7697, 2.6527, 2.6265, 2.6199]
  val_policy_loss : [2.5396, 2.5366, 2.5458, 2.5519, 2.5143, 2.5083, 2.6689, 2.5551, 2.5280, 2.5228]
  val_policy_policy_acc : [0.2656, 0.2636, 0.2697, 0.2671, 0.2762, 0.2754, 0.2780, 0.2771, 0.2774, 0.2783]
  val_value_loss : [0.2001, 0.1989, 0.1971, 0.1948, 0.1915, 0.1891, 0.1896, 0.1880, 0.1865, 0.1847]
  val_value_value_mse : [0.2008, 0.1996, 0.1977, 0.1954, 0.1921, 0.1897, 0.1902, 0.1886, 0.1871, 0.1853]
  value_loss : [0.2096, 0.2076, 0.2053, 0.2035, 0.1998, 0.1972, 0.1961, 0.1947, 0.1927, 0.1924]
  value_value_mse : [0.2096, 0.2076, 0.2053, 0.2035, 0.1998, 0.1972, 0.1961, 0.1947, 0.1927, 0.1924]

================================================================================

History file: model_versions/chess_elo_model_V29_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T22:29:10.965611Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game42001_game43500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.415669   (epoch 9)   mode=min
  policy_loss: 2.316704   (epoch 9)   mode=min
  policy_policy_acc: 0.309671   (epoch 9)   mode=max
  val_loss: 2.651679   (epoch 3)   mode=min
  val_policy_loss: 2.548750   (epoch 3)   mode=min
  val_policy_policy_acc: 0.275159   (epoch 8)   mode=max
  val_value_loss: 0.193443   (epoch 8)   mode=min
  val_value_value_mse: 0.193456   (epoch 8)   mode=min
  value_loss: 0.198424   (epoch 9)   mode=min
  value_value_mse: 0.198418   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7296, 2.6725, 2.6349, 2.6006, 2.5718, 2.5003, 2.4730, 2.4306, 2.4157]
  policy_loss : [2.6232, 2.5667, 2.5300, 2.4962, 2.4687, 2.3990, 2.3725, 2.3312, 2.3167]
  policy_policy_acc : [0.2507, 0.2618, 0.2694, 0.2772, 0.2819, 0.2918, 0.3002, 0.3076, 0.3097]
  val_loss : [2.6719, 2.6698, 2.6517, 2.6800, 2.6725, 2.6699, 2.6640, 2.6564, 2.6620]
  val_policy_loss : [2.5671, 2.5691, 2.5488, 2.5798, 2.5704, 2.5705, 2.5661, 2.5592, 2.5647]
  val_policy_policy_acc : [0.2648, 0.2642, 0.2687, 0.2624, 0.2649, 0.2696, 0.2723, 0.2752, 0.2741]
  val_value_loss : [0.2086, 0.2008, 0.2052, 0.1996, 0.2035, 0.1978, 0.1948, 0.1934, 0.1936]
  val_value_value_mse : [0.2086, 0.2008, 0.2053, 0.1996, 0.2035, 0.1978, 0.1948, 0.1935, 0.1936]
  value_loss : [0.2131, 0.2110, 0.2097, 0.2086, 0.2071, 0.2028, 0.2007, 0.1995, 0.1984]
  value_value_mse : [0.2130, 0.2109, 0.2097, 0.2087, 0.2071, 0.2028, 0.2007, 0.1995, 0.1984]

================================================================================

History file: model_versions/chess_elo_model_V30_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T22:38:18.163528Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game43501_game45000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.397099   (epoch 10)   mode=min
  policy_loss: 2.298134   (epoch 10)   mode=min
  policy_policy_acc: 0.315927   (epoch 10)   mode=max
  val_loss: 2.651756   (epoch 8)   mode=min
  val_policy_loss: 2.554008   (epoch 8)   mode=min
  val_policy_policy_acc: 0.270311   (epoch 8)   mode=max
  val_value_loss: 0.191200   (epoch 10)   mode=min
  val_value_value_mse: 0.191143   (epoch 10)   mode=min
  value_loss: 0.198068   (epoch 10)   mode=min
  value_value_mse: 0.198079   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005]
  loss : [2.7131, 2.6622, 2.6262, 2.5940, 2.5624, 2.5330, 2.4660, 2.4375, 2.4189, 2.3971]
  policy_loss : [2.6055, 2.5565, 2.5211, 2.4899, 2.4588, 2.4301, 2.3650, 2.3374, 2.3195, 2.2981]
  policy_policy_acc : [0.2571, 0.2658, 0.2711, 0.2785, 0.2845, 0.2887, 0.3044, 0.3082, 0.3103, 0.3159]
  val_loss : [2.6826, 2.6622, 2.6650, 2.6620, 2.6664, 2.6709, 2.6629, 2.6518, 2.7197, 2.7042]
  val_policy_loss : [2.5770, 2.5618, 2.5612, 2.5606, 2.5657, 2.5700, 2.5632, 2.5540, 2.6216, 2.6080]
  val_policy_policy_acc : [0.2581, 0.2638, 0.2636, 0.2643, 0.2629, 0.2669, 0.2671, 0.2703, 0.2684, 0.2665]
  val_value_loss : [0.2101, 0.2002, 0.2070, 0.2020, 0.2005, 0.2009, 0.1987, 0.1945, 0.1947, 0.1912]
  val_value_value_mse : [0.2101, 0.2002, 0.2070, 0.2020, 0.2004, 0.2009, 0.1987, 0.1945, 0.1946, 0.1911]
  value_loss : [0.2150, 0.2114, 0.2100, 0.2085, 0.2071, 0.2060, 0.2020, 0.1999, 0.1988, 0.1981]
  value_value_mse : [0.2150, 0.2114, 0.2100, 0.2085, 0.2071, 0.2060, 0.2020, 0.1999, 0.1988, 0.1981]

================================================================================

History file: model_versions/chess_elo_model_V31_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T23:27:46.747882Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game45001_game46500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.390698   (epoch 10)   mode=min
  policy_loss: 2.295236   (epoch 10)   mode=min
  policy_policy_acc: 0.315798   (epoch 10)   mode=max
  val_loss: 2.653046   (epoch 4)   mode=min
  val_policy_loss: 2.554271   (epoch 4)   mode=min
  val_policy_policy_acc: 0.283115   (epoch 9)   mode=max
  val_value_loss: 0.186419   (epoch 9)   mode=min
  val_value_value_mse: 0.186394   (epoch 9)   mode=min
  value_loss: 0.190761   (epoch 10)   mode=min
  value_value_mse: 0.190759   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7261, 2.6696, 2.6333, 2.6046, 2.5719, 2.5431, 2.4749, 2.4476, 2.4062, 2.3907]
  policy_loss : [2.6212, 2.5665, 2.5305, 2.5027, 2.4709, 2.4432, 2.3771, 2.3510, 2.3104, 2.2952]
  policy_policy_acc : [0.2506, 0.2630, 0.2695, 0.2739, 0.2798, 0.2852, 0.3008, 0.3040, 0.3142, 0.3158]
  val_loss : [2.6876, 2.6576, 2.6754, 2.6530, 2.7608, 2.7140, 2.6590, 2.6725, 2.6630, 2.7694]
  val_policy_loss : [2.5843, 2.5572, 2.5783, 2.5543, 2.6605, 2.6161, 2.5655, 2.5784, 2.5696, 2.6756]
  val_policy_policy_acc : [0.2627, 0.2669, 0.2728, 0.2751, 0.2705, 0.2792, 0.2801, 0.2797, 0.2831, 0.2825]
  val_value_loss : [0.2067, 0.2015, 0.1941, 0.1976, 0.2002, 0.1953, 0.1869, 0.1878, 0.1864, 0.1870]
  val_value_value_mse : [0.2068, 0.2015, 0.1941, 0.1976, 0.2001, 0.1953, 0.1868, 0.1877, 0.1864, 0.1869]
  value_loss : [0.2097, 0.2063, 0.2052, 0.2039, 0.2019, 0.1997, 0.1956, 0.1934, 0.1916, 0.1908]
  value_value_mse : [0.2097, 0.2062, 0.2052, 0.2040, 0.2019, 0.1997, 0.1957, 0.1934, 0.1915, 0.1908]

================================================================================

History file: model_versions/chess_elo_model_V32_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T23:35:59.990111Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game46501_game48000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.416616   (epoch 10)   mode=min
  policy_loss: 2.316976   (epoch 10)   mode=min
  policy_policy_acc: 0.313367   (epoch 10)   mode=max
  val_loss: 2.630891   (epoch 5)   mode=min
  val_policy_loss: 2.529580   (epoch 5)   mode=min
  val_policy_policy_acc: 0.280979   (epoch 10)   mode=max
  val_value_loss: 0.197698   (epoch 10)   mode=min
  val_value_value_mse: 0.197695   (epoch 10)   mode=min
  value_loss: 0.199416   (epoch 10)   mode=min
  value_value_mse: 0.199415   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7337, 2.6789, 2.6410, 2.6100, 2.5421, 2.5171, 2.4932, 2.4541, 2.4394, 2.4166]
  policy_loss : [2.6262, 2.5721, 2.5350, 2.5049, 2.4393, 2.4148, 2.3916, 2.3536, 2.3391, 2.3170]
  policy_policy_acc : [0.2529, 0.2634, 0.2713, 0.2766, 0.2886, 0.2959, 0.2994, 0.3053, 0.3101, 0.3134]
  val_loss : [2.7045, 2.6587, 2.6867, 2.7003, 2.6309, 2.7289, 2.6534, 2.6431, 2.6615, 2.6335]
  val_policy_loss : [2.5964, 2.5529, 2.5806, 2.5951, 2.5296, 2.6266, 2.5528, 2.5436, 2.5615, 2.5347]
  val_policy_policy_acc : [0.2671, 0.2730, 0.2680, 0.2715, 0.2741, 0.2754, 0.2789, 0.2760, 0.2808, 0.2810]
  val_value_loss : [0.2163, 0.2116, 0.2123, 0.2104, 0.2027, 0.2047, 0.2014, 0.1991, 0.2000, 0.1977]
  val_value_value_mse : [0.2163, 0.2116, 0.2123, 0.2104, 0.2027, 0.2047, 0.2014, 0.1991, 0.2000, 0.1977]
  value_loss : [0.2148, 0.2139, 0.2119, 0.2101, 0.2057, 0.2048, 0.2030, 0.2008, 0.2006, 0.1994]
  value_value_mse : [0.2148, 0.2139, 0.2119, 0.2101, 0.2057, 0.2048, 0.2030, 0.2008, 0.2006, 0.1994]

================================================================================

History file: model_versions/chess_elo_model_V33_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T23:44:27.124020Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game48001_game49500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.389555   (epoch 10)   mode=min
  policy_loss: 2.286091   (epoch 10)   mode=min
  policy_policy_acc: 0.316530   (epoch 10)   mode=max
  val_loss: 2.657538   (epoch 4)   mode=min
  val_policy_loss: 2.549743   (epoch 4)   mode=min
  val_policy_policy_acc: 0.273697   (epoch 7)   mode=max
  val_value_loss: 0.202734   (epoch 10)   mode=min
  val_value_value_mse: 0.202691   (epoch 10)   mode=min
  value_loss: 0.207292   (epoch 10)   mode=min
  value_value_mse: 0.207290   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7162, 2.6606, 2.6290, 2.5952, 2.5684, 2.5386, 2.4736, 2.4481, 2.4078, 2.3896]
  policy_loss : [2.6050, 2.5503, 2.5193, 2.4867, 2.4602, 2.4308, 2.3675, 2.3429, 2.3041, 2.2861]
  policy_policy_acc : [0.2557, 0.2661, 0.2725, 0.2777, 0.2829, 0.2883, 0.3006, 0.3044, 0.3127, 0.3165]
  val_loss : [2.6827, 2.6601, 2.6664, 2.6575, 2.6670, 2.7260, 2.7071, 2.6925, 2.7142, 2.7495]
  val_policy_loss : [2.5717, 2.5511, 2.5574, 2.5497, 2.5587, 2.6164, 2.6021, 2.5881, 2.6112, 2.6467]
  val_policy_policy_acc : [0.2584, 0.2646, 0.2628, 0.2654, 0.2638, 0.2663, 0.2737, 0.2720, 0.2735, 0.2733]
  val_value_loss : [0.2202, 0.2159, 0.2162, 0.2134, 0.2144, 0.2167, 0.2073, 0.2060, 0.2032, 0.2027]
  val_value_value_mse : [0.2202, 0.2158, 0.2162, 0.2134, 0.2144, 0.2167, 0.2073, 0.2059, 0.2032, 0.2027]
  value_loss : [0.2226, 0.2206, 0.2195, 0.2173, 0.2166, 0.2155, 0.2124, 0.2105, 0.2073, 0.2073]
  value_value_mse : [0.2226, 0.2206, 0.2195, 0.2173, 0.2166, 0.2155, 0.2124, 0.2105, 0.2073, 0.2073]

================================================================================

History file: model_versions/chess_elo_model_V34_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-12T23:53:03.051856Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game49501_game51000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V33
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.421268   (epoch 10)   mode=min
  policy_loss: 2.319111   (epoch 10)   mode=min
  policy_policy_acc: 0.312132   (epoch 10)   mode=max
  val_loss: 2.634570   (epoch 7)   mode=min
  val_policy_loss: 2.534204   (epoch 7)   mode=min
  val_policy_policy_acc: 0.267834   (epoch 10)   mode=max
  val_value_loss: 0.198600   (epoch 10)   mode=min
  val_value_value_mse: 0.198522   (epoch 10)   mode=min
  value_loss: 0.204344   (epoch 10)   mode=min
  value_value_mse: 0.204344   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7306, 2.6815, 2.6434, 2.6162, 2.5881, 2.5235, 2.4996, 2.4785, 2.4582, 2.4213]
  policy_loss : [2.6204, 2.5731, 2.5357, 2.5091, 2.4813, 2.4187, 2.3954, 2.3749, 2.3546, 2.3191]
  policy_policy_acc : [0.2542, 0.2627, 0.2700, 0.2745, 0.2798, 0.2939, 0.2962, 0.3010, 0.3051, 0.3121]
  val_loss : [2.6775, 2.6764, 2.6441, 2.6436, 2.6540, 2.6426, 2.6346, 2.6379, 2.7150, 2.7114]
  val_policy_loss : [2.5697, 2.5714, 2.5396, 2.5403, 2.5511, 2.5415, 2.5342, 2.5365, 2.6158, 2.6133]
  val_policy_policy_acc : [0.2575, 0.2584, 0.2645, 0.2647, 0.2633, 0.2662, 0.2641, 0.2633, 0.2646, 0.2678]
  val_value_loss : [0.2185, 0.2127, 0.2099, 0.2088, 0.2076, 0.2046, 0.2031, 0.2050, 0.2008, 0.1986]
  val_value_value_mse : [0.2184, 0.2127, 0.2099, 0.2088, 0.2075, 0.2045, 0.2030, 0.2049, 0.2008, 0.1985]
  value_loss : [0.2203, 0.2169, 0.2156, 0.2142, 0.2135, 0.2097, 0.2085, 0.2071, 0.2073, 0.2043]
  value_value_mse : [0.2203, 0.2169, 0.2156, 0.2142, 0.2135, 0.2097, 0.2085, 0.2071, 0.2073, 0.2043]

================================================================================

History file: model_versions/chess_elo_model_V35_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:01:34.779691Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game51001_game52500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V34
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.428570   (epoch 10)   mode=min
  policy_loss: 2.325328   (epoch 10)   mode=min
  policy_policy_acc: 0.311299   (epoch 10)   mode=max
  val_loss: 2.643058   (epoch 7)   mode=min
  val_policy_loss: 2.541138   (epoch 7)   mode=min
  val_policy_policy_acc: 0.274470   (epoch 8)   mode=max
  val_value_loss: 0.202152   (epoch 10)   mode=min
  val_value_value_mse: 0.202002   (epoch 10)   mode=min
  value_loss: 0.206486   (epoch 10)   mode=min
  value_value_mse: 0.206489   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7544, 2.6962, 2.6627, 2.6294, 2.6004, 2.5341, 2.5090, 2.4862, 2.4705, 2.4286]
  policy_loss : [2.6436, 2.5867, 2.5533, 2.5207, 2.4921, 2.4277, 2.4034, 2.3809, 2.3663, 2.3253]
  policy_policy_acc : [0.2509, 0.2599, 0.2658, 0.2731, 0.2788, 0.2888, 0.2946, 0.2987, 0.3031, 0.3113]
  val_loss : [2.6737, 2.6629, 2.6584, 2.6733, 2.6585, 2.6482, 2.6431, 2.6489, 2.6572, 2.6605]
  val_policy_loss : [2.5651, 2.5572, 2.5527, 2.5654, 2.5533, 2.5437, 2.5411, 2.5474, 2.5549, 2.5592]
  val_policy_policy_acc : [0.2612, 0.2667, 0.2713, 0.2676, 0.2687, 0.2737, 0.2730, 0.2745, 0.2717, 0.2729]
  val_value_loss : [0.2171, 0.2107, 0.2117, 0.2149, 0.2096, 0.2080, 0.2029, 0.2023, 0.2036, 0.2022]
  val_value_value_mse : [0.2170, 0.2106, 0.2116, 0.2148, 0.2095, 0.2078, 0.2027, 0.2022, 0.2035, 0.2020]
  value_loss : [0.2215, 0.2191, 0.2188, 0.2175, 0.2163, 0.2128, 0.2111, 0.2106, 0.2084, 0.2065]
  value_value_mse : [0.2215, 0.2191, 0.2188, 0.2175, 0.2163, 0.2128, 0.2111, 0.2106, 0.2084, 0.2065]

================================================================================

History file: model_versions/chess_elo_model_V36_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:10:20.743305Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game52501_game54000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V35
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.434882   (epoch 10)   mode=min
  policy_loss: 2.330583   (epoch 10)   mode=min
  policy_policy_acc: 0.310037   (epoch 10)   mode=max
  val_loss: 2.633363   (epoch 5)   mode=min
  val_policy_loss: 2.529116   (epoch 5)   mode=min
  val_policy_policy_acc: 0.281434   (epoch 10)   mode=max
  val_value_loss: 0.204428   (epoch 10)   mode=min
  val_value_value_mse: 0.204550   (epoch 10)   mode=min
  value_loss: 0.208547   (epoch 10)   mode=min
  value_value_mse: 0.208525   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7405, 2.6831, 2.6461, 2.5803, 2.5539, 2.5285, 2.5127, 2.4731, 2.4521, 2.4349]
  policy_loss : [2.6291, 2.5729, 2.5365, 2.4727, 2.4468, 2.4225, 2.4064, 2.3681, 2.3470, 2.3306]
  policy_policy_acc : [0.2526, 0.2633, 0.2699, 0.2836, 0.2889, 0.2920, 0.2959, 0.3037, 0.3064, 0.3100]
  val_loss : [2.6706, 2.6732, 2.6675, 2.6420, 2.6334, 2.6470, 2.6962, 2.8364, 2.8888, 2.7498]
  val_policy_loss : [2.5602, 2.5666, 2.5608, 2.5374, 2.5291, 2.5422, 2.5916, 2.7331, 2.7849, 2.6468]
  val_policy_policy_acc : [0.2745, 0.2674, 0.2758, 0.2750, 0.2789, 0.2780, 0.2769, 0.2787, 0.2777, 0.2814]
  val_value_loss : [0.2231, 0.2166, 0.2152, 0.2095, 0.2092, 0.2093, 0.2088, 0.2057, 0.2057, 0.2044]
  val_value_value_mse : [0.2231, 0.2165, 0.2153, 0.2096, 0.2093, 0.2093, 0.2089, 0.2057, 0.2058, 0.2046]
  value_loss : [0.2221, 0.2202, 0.2188, 0.2156, 0.2142, 0.2128, 0.2124, 0.2106, 0.2097, 0.2085]
  value_value_mse : [0.2221, 0.2202, 0.2188, 0.2156, 0.2142, 0.2128, 0.2125, 0.2106, 0.2097, 0.2085]

================================================================================

History file: model_versions/chess_elo_model_V37_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:19:21.753509Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game54001_game55500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V36
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.460468   (epoch 10)   mode=min
  policy_loss: 2.356124   (epoch 10)   mode=min
  policy_policy_acc: 0.304938   (epoch 10)   mode=max
  val_loss: 2.647149   (epoch 9)   mode=min
  val_policy_loss: 2.538934   (epoch 9)   mode=min
  val_policy_policy_acc: 0.276223   (epoch 8)   mode=max
  val_value_loss: 0.206236   (epoch 10)   mode=min
  val_value_value_mse: 0.205928   (epoch 10)   mode=min
  value_loss: 0.208816   (epoch 10)   mode=min
  value_value_mse: 0.208795   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003]
  loss : [2.7383, 2.6886, 2.6517, 2.5921, 2.5640, 2.5427, 2.5218, 2.4843, 2.4697, 2.4605]
  policy_loss : [2.6280, 2.5791, 2.5426, 2.4846, 2.4574, 2.4363, 2.4162, 2.3796, 2.3651, 2.3561]
  policy_policy_acc : [0.2508, 0.2615, 0.2681, 0.2805, 0.2842, 0.2875, 0.2926, 0.3005, 0.3022, 0.3049]
  val_loss : [2.6722, 2.6774, 2.6896, 2.6526, 2.6500, 2.6668, 2.6662, 2.6542, 2.6471, 2.6495]
  val_policy_loss : [2.5584, 2.5617, 2.5769, 2.5416, 2.5398, 2.5547, 2.5562, 2.5458, 2.5389, 2.5413]
  val_policy_policy_acc : [0.2637, 0.2603, 0.2657, 0.2663, 0.2702, 0.2737, 0.2717, 0.2762, 0.2737, 0.2732]
  val_value_loss : [0.2166, 0.2199, 0.2144, 0.2108, 0.2110, 0.2127, 0.2107, 0.2068, 0.2064, 0.2062]
  val_value_value_mse : [0.2166, 0.2197, 0.2147, 0.2112, 0.2106, 0.2125, 0.2103, 0.2065, 0.2062, 0.2059]
  value_loss : [0.2207, 0.2190, 0.2182, 0.2148, 0.2133, 0.2127, 0.2114, 0.2096, 0.2088, 0.2088]
  value_value_mse : [0.2207, 0.2190, 0.2182, 0.2148, 0.2133, 0.2127, 0.2114, 0.2097, 0.2089, 0.2088]

================================================================================

History file: model_versions/chess_elo_model_V38_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:28:13.224010Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game55501_game57000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V37
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.419160   (epoch 10)   mode=min
  policy_loss: 2.320887   (epoch 10)   mode=min
  policy_policy_acc: 0.310867   (epoch 10)   mode=max
  val_loss: 2.634566   (epoch 8)   mode=min
  val_policy_loss: 2.541028   (epoch 8)   mode=min
  val_policy_policy_acc: 0.270737   (epoch 6)   mode=max
  val_value_loss: 0.187711   (epoch 8)   mode=min
  val_value_value_mse: 0.187860   (epoch 8)   mode=min
  value_loss: 0.196502   (epoch 10)   mode=min
  value_value_mse: 0.196504   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]
  loss : [2.7162, 2.6676, 2.6286, 2.5984, 2.5709, 2.5062, 2.4770, 2.4562, 2.4385, 2.4192]
  policy_loss : [2.6099, 2.5632, 2.5246, 2.4952, 2.4686, 2.4057, 2.3775, 2.3573, 2.3399, 2.3209]
  policy_policy_acc : [0.2559, 0.2644, 0.2715, 0.2781, 0.2811, 0.2959, 0.3001, 0.3057, 0.3084, 0.3109]
  val_loss : [2.6533, 2.6615, 2.6483, 2.6571, 2.6480, 2.6455, 2.6434, 2.6346, 2.6585, 2.6550]
  val_policy_loss : [2.5516, 2.5625, 2.5488, 2.5577, 2.5513, 2.5492, 2.5478, 2.5410, 2.5640, 2.5603]
  val_policy_policy_acc : [0.2598, 0.2565, 0.2640, 0.2608, 0.2635, 0.2707, 0.2707, 0.2698, 0.2698, 0.2673]
  val_value_loss : [0.2035, 0.1988, 0.1993, 0.1993, 0.1939, 0.1928, 0.1917, 0.1877, 0.1902, 0.1900]
  val_value_value_mse : [0.2035, 0.1989, 0.1994, 0.1994, 0.1940, 0.1930, 0.1918, 0.1879, 0.1903, 0.1902]
  value_loss : [0.2127, 0.2090, 0.2079, 0.2063, 0.2044, 0.2010, 0.1989, 0.1978, 0.1973, 0.1965]
  value_value_mse : [0.2127, 0.2090, 0.2079, 0.2063, 0.2044, 0.2010, 0.1989, 0.1978, 0.1973, 0.1965]

================================================================================

History file: model_versions/chess_elo_model_V39_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:36:15.222272Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game57001_game58500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V38
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.422517   (epoch 9)   mode=min
  policy_loss: 2.326090   (epoch 9)   mode=min
  policy_policy_acc: 0.308632   (epoch 9)   mode=max
  val_loss: 2.638550   (epoch 3)   mode=min
  val_policy_loss: 2.539404   (epoch 3)   mode=min
  val_policy_policy_acc: 0.272149   (epoch 8)   mode=max
  val_value_loss: 0.187329   (epoch 9)   mode=min
  val_value_value_mse: 0.187349   (epoch 9)   mode=min
  value_loss: 0.192776   (epoch 9)   mode=min
  value_value_mse: 0.192781   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7197, 2.6681, 2.6346, 2.6000, 2.5722, 2.5047, 2.4786, 2.4355, 2.4225]
  policy_loss : [2.6160, 2.5643, 2.5325, 2.4986, 2.4713, 2.4055, 2.3808, 2.3385, 2.3261]
  policy_policy_acc : [0.2529, 0.2633, 0.2680, 0.2751, 0.2805, 0.2919, 0.2984, 0.3067, 0.3086]
  val_loss : [2.6596, 2.6421, 2.6385, 2.6505, 2.7318, 2.7829, 2.7575, 2.7282, 2.8139]
  val_policy_loss : [2.5587, 2.5445, 2.5394, 2.5523, 2.6344, 2.6875, 2.6622, 2.6341, 2.7201]
  val_policy_policy_acc : [0.2619, 0.2655, 0.2651, 0.2655, 0.2631, 0.2686, 0.2720, 0.2721, 0.2720]
  val_value_loss : [0.2018, 0.1953, 0.1984, 0.1964, 0.1948, 0.1906, 0.1900, 0.1882, 0.1873]
  val_value_value_mse : [0.2018, 0.1954, 0.1985, 0.1965, 0.1948, 0.1906, 0.1901, 0.1882, 0.1873]
  value_loss : [0.2072, 0.2076, 0.2043, 0.2024, 0.2024, 0.1979, 0.1956, 0.1938, 0.1928]
  value_value_mse : [0.2072, 0.2076, 0.2043, 0.2024, 0.2024, 0.1979, 0.1956, 0.1938, 0.1928]

================================================================================

History file: model_versions/chess_elo_model_V40_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:45:15.413062Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game58501_game60000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V39
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.413366   (epoch 10)   mode=min
  policy_loss: 2.311184   (epoch 10)   mode=min
  policy_policy_acc: 0.314966   (epoch 10)   mode=max
  val_loss: 2.631109   (epoch 7)   mode=min
  val_policy_loss: 2.528904   (epoch 7)   mode=min
  val_policy_policy_acc: 0.278027   (epoch 8)   mode=max
  val_value_loss: 0.200761   (epoch 10)   mode=min
  val_value_value_mse: 0.200756   (epoch 10)   mode=min
  value_loss: 0.204368   (epoch 10)   mode=min
  value_value_mse: 0.204367   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7271, 2.6754, 2.6414, 2.6106, 2.5820, 2.5189, 2.4913, 2.4726, 2.4528, 2.4134]
  policy_loss : [2.6179, 2.5669, 2.5332, 2.5032, 2.4753, 2.4138, 2.3872, 2.3691, 2.3496, 2.3112]
  policy_policy_acc : [0.2545, 0.2631, 0.2708, 0.2778, 0.2825, 0.2942, 0.2990, 0.3013, 0.3067, 0.3150]
  val_loss : [2.6679, 2.7168, 2.6476, 2.6620, 2.6633, 2.6748, 2.6311, 2.6756, 2.6552, 2.6787]
  val_policy_loss : [2.5615, 2.6107, 2.5427, 2.5557, 2.5580, 2.5727, 2.5289, 2.5734, 2.5533, 2.5779]
  val_policy_policy_acc : [0.2649, 0.2573, 0.2623, 0.2636, 0.2643, 0.2729, 0.2745, 0.2780, 0.2737, 0.2764]
  val_value_loss : [0.2120, 0.2115, 0.2092, 0.2118, 0.2099, 0.2034, 0.2036, 0.2033, 0.2029, 0.2008]
  val_value_value_mse : [0.2120, 0.2115, 0.2092, 0.2118, 0.2099, 0.2034, 0.2036, 0.2033, 0.2029, 0.2008]
  value_loss : [0.2185, 0.2169, 0.2164, 0.2148, 0.2135, 0.2102, 0.2082, 0.2071, 0.2065, 0.2044]
  value_value_mse : [0.2185, 0.2169, 0.2164, 0.2148, 0.2135, 0.2102, 0.2082, 0.2071, 0.2065, 0.2044]

================================================================================

History file: model_versions/chess_elo_model_V41_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T00:54:04.189177Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game60001_game61500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V40
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.401132   (epoch 10)   mode=min
  policy_loss: 2.303051   (epoch 10)   mode=min
  policy_policy_acc: 0.313471   (epoch 10)   mode=max
  val_loss: 2.641856   (epoch 4)   mode=min
  val_policy_loss: 2.542318   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279513   (epoch 10)   mode=max
  val_value_loss: 0.189409   (epoch 10)   mode=min
  val_value_value_mse: 0.189751   (epoch 10)   mode=min
  value_loss: 0.196110   (epoch 10)   mode=min
  value_value_mse: 0.196108   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7169, 2.6656, 2.6341, 2.6021, 2.5742, 2.5471, 2.4825, 2.4577, 2.4166, 2.4011]
  policy_loss : [2.6099, 2.5601, 2.5294, 2.4983, 2.4708, 2.4442, 2.3823, 2.3579, 2.3178, 2.3031]
  policy_policy_acc : [0.2559, 0.2653, 0.2708, 0.2757, 0.2812, 0.2857, 0.2990, 0.3035, 0.3122, 0.3135]
  val_loss : [2.6876, 2.6676, 2.6527, 2.6419, 2.6874, 2.7326, 3.0061, 3.0217, 2.8460, 2.6659]
  val_policy_loss : [2.5812, 2.5639, 2.5532, 2.5423, 2.5882, 2.6320, 2.9067, 2.9226, 2.7486, 2.5689]
  val_policy_policy_acc : [0.2548, 0.2666, 0.2715, 0.2660, 0.2706, 0.2657, 0.2778, 0.2740, 0.2766, 0.2795]
  val_value_loss : [0.2098, 0.2047, 0.1982, 0.1989, 0.1974, 0.1969, 0.1929, 0.1934, 0.1905, 0.1894]
  val_value_value_mse : [0.2101, 0.2051, 0.1986, 0.1993, 0.1977, 0.1973, 0.1933, 0.1937, 0.1908, 0.1898]
  value_loss : [0.2141, 0.2109, 0.2095, 0.2076, 0.2070, 0.2058, 0.2004, 0.1995, 0.1977, 0.1961]
  value_value_mse : [0.2141, 0.2109, 0.2095, 0.2076, 0.2070, 0.2057, 0.2004, 0.1995, 0.1977, 0.1961]

================================================================================

History file: model_versions/chess_elo_model_V42_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:00:26.026529Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game61501_game63000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V41
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.493044   (epoch 7)   mode=min
  policy_loss: 2.394936   (epoch 7)   mode=min
  policy_policy_acc: 0.297815   (epoch 7)   mode=max
  val_loss: 2.802245   (epoch 1)   mode=min
  val_policy_loss: 2.699268   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284359   (epoch 7)   mode=max
  val_value_loss: 0.194772   (epoch 7)   mode=min
  val_value_value_mse: 0.194795   (epoch 7)   mode=min
  value_loss: 0.197697   (epoch 7)   mode=min
  value_value_mse: 0.197735   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7203, 2.6683, 2.6360, 2.5693, 2.5449, 2.5071, 2.4930]
  policy_loss : [2.6148, 2.5638, 2.5327, 2.4681, 2.4446, 2.4077, 2.3949]
  policy_policy_acc : [0.2543, 0.2639, 0.2715, 0.2815, 0.2889, 0.2952, 0.2978]
  val_loss : [2.8022, 3.0913, 3.5869, 4.3502, 3.0943, 3.1127, 3.3650]
  val_policy_loss : [2.6993, 2.9883, 3.4862, 4.2488, 2.9965, 3.0147, 3.2673]
  val_policy_policy_acc : [0.2711, 0.2756, 0.2735, 0.2797, 0.2805, 0.2811, 0.2844]
  val_value_loss : [0.2072, 0.2063, 0.2005, 0.1993, 0.1960, 0.1963, 0.1948]
  val_value_value_mse : [0.2072, 0.2063, 0.2005, 0.1993, 0.1960, 0.1963, 0.1948]
  value_loss : [0.2104, 0.2082, 0.2074, 0.2023, 0.2013, 0.1991, 0.1977]
  value_value_mse : [0.2103, 0.2081, 0.2074, 0.2022, 0.2012, 0.1990, 0.1977]

================================================================================

History file: model_versions/chess_elo_model_V43_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:09:13.512220Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game63001_game64500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V42
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.427143   (epoch 10)   mode=min
  policy_loss: 2.327225   (epoch 10)   mode=min
  policy_policy_acc: 0.311369   (epoch 10)   mode=max
  val_loss: 2.599759   (epoch 7)   mode=min
  val_policy_loss: 2.501301   (epoch 7)   mode=min
  val_policy_policy_acc: 0.286501   (epoch 10)   mode=max
  val_value_loss: 0.193699   (epoch 10)   mode=min
  val_value_value_mse: 0.193712   (epoch 10)   mode=min
  value_loss: 0.200102   (epoch 10)   mode=min
  value_value_mse: 0.200113   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7155, 2.6665, 2.6342, 2.6068, 2.5452, 2.5228, 2.5019, 2.4815, 2.4641, 2.4271]
  policy_loss : [2.6075, 2.5597, 2.5277, 2.5010, 2.4427, 2.4213, 2.4006, 2.3806, 2.3634, 2.3272]
  policy_policy_acc : [0.2563, 0.2652, 0.2721, 0.2766, 0.2885, 0.2928, 0.2955, 0.3014, 0.3042, 0.3114]
  val_loss : [2.6417, 2.6300, 2.6410, 2.6340, 2.6030, 2.6025, 2.5998, 2.6065, 2.6070, 2.6071]
  val_policy_loss : [2.5351, 2.5241, 2.5391, 2.5312, 2.5029, 2.5027, 2.5013, 2.5084, 2.5092, 2.5102]
  val_policy_policy_acc : [0.2711, 0.2763, 0.2721, 0.2757, 0.2831, 0.2817, 0.2833, 0.2833, 0.2844, 0.2865]
  val_value_loss : [0.2132, 0.2116, 0.2040, 0.2057, 0.2004, 0.1994, 0.1969, 0.1960, 0.1957, 0.1937]
  val_value_value_mse : [0.2132, 0.2116, 0.2040, 0.2057, 0.2004, 0.1994, 0.1969, 0.1960, 0.1957, 0.1937]
  value_loss : [0.2162, 0.2136, 0.2127, 0.2111, 0.2053, 0.2030, 0.2025, 0.2021, 0.2014, 0.2001]
  value_value_mse : [0.2162, 0.2136, 0.2127, 0.2111, 0.2053, 0.2030, 0.2025, 0.2021, 0.2014, 0.2001]

================================================================================

History file: model_versions/chess_elo_model_V44_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:16:08.434054Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game64501_game66000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V43
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.471433   (epoch 8)   mode=min
  policy_loss: 2.369066   (epoch 8)   mode=min
  policy_policy_acc: 0.301723   (epoch 8)   mode=max
  val_loss: 2.659190   (epoch 2)   mode=min
  val_policy_loss: 2.554140   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281105   (epoch 7)   mode=max
  val_value_loss: 0.199671   (epoch 8)   mode=min
  val_value_value_mse: 0.199693   (epoch 8)   mode=min
  value_loss: 0.204821   (epoch 8)   mode=min
  value_value_mse: 0.204827   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7257, 2.6764, 2.6443, 2.6145, 2.5478, 2.5220, 2.4852, 2.4714]
  policy_loss : [2.6180, 2.5697, 2.5372, 2.5079, 2.4428, 2.4183, 2.3824, 2.3691]
  policy_policy_acc : [0.2534, 0.2610, 0.2667, 0.2730, 0.2860, 0.2904, 0.2980, 0.3017]
  val_loss : [2.6806, 2.6592, 2.7475, 2.7619, 2.7135, 2.7146, 2.7456, 2.6806]
  val_policy_loss : [2.5737, 2.5541, 2.6426, 2.6556, 2.6114, 2.6140, 2.6449, 2.5805]
  val_policy_policy_acc : [0.2636, 0.2712, 0.2669, 0.2611, 0.2754, 0.2756, 0.2811, 0.2779]
  val_value_loss : [0.2131, 0.2093, 0.2090, 0.2119, 0.2034, 0.2004, 0.2005, 0.1997]
  val_value_value_mse : [0.2131, 0.2093, 0.2090, 0.2119, 0.2034, 0.2005, 0.2005, 0.1997]
  value_loss : [0.2151, 0.2138, 0.2141, 0.2130, 0.2096, 0.2076, 0.2058, 0.2048]
  value_value_mse : [0.2151, 0.2138, 0.2141, 0.2130, 0.2096, 0.2076, 0.2057, 0.2048]

================================================================================

History file: model_versions/chess_elo_model_V45_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:24:45.823813Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game66001_game67500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V44
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.440710   (epoch 10)   mode=min
  policy_loss: 2.337287   (epoch 10)   mode=min
  policy_policy_acc: 0.307837   (epoch 10)   mode=max
  val_loss: 2.670863   (epoch 7)   mode=min
  val_policy_loss: 2.568535   (epoch 7)   mode=min
  val_policy_policy_acc: 0.272022   (epoch 10)   mode=max
  val_value_loss: 0.201987   (epoch 9)   mode=min
  val_value_value_mse: 0.201986   (epoch 9)   mode=min
  value_loss: 0.206877   (epoch 10)   mode=min
  value_value_mse: 0.206882   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7290, 2.6866, 2.6545, 2.6259, 2.5956, 2.5369, 2.5114, 2.4912, 2.4753, 2.4407]
  policy_loss : [2.6186, 2.5770, 2.5454, 2.5171, 2.4873, 2.4310, 2.4061, 2.3865, 2.3707, 2.3373]
  policy_policy_acc : [0.2567, 0.2629, 0.2687, 0.2752, 0.2791, 0.2906, 0.2947, 0.2991, 0.3027, 0.3078]
  val_loss : [2.6907, 2.6862, 2.6821, 2.7417, 2.6874, 2.6862, 2.6709, 2.6981, 2.6810, 2.6830]
  val_policy_loss : [2.5827, 2.5797, 2.5749, 2.6356, 2.5809, 2.5842, 2.5685, 2.5964, 2.5800, 2.5820]
  val_policy_policy_acc : [0.2669, 0.2618, 0.2618, 0.2619, 0.2638, 0.2666, 0.2704, 0.2705, 0.2654, 0.2720]
  val_value_loss : [0.2160, 0.2129, 0.2142, 0.2121, 0.2129, 0.2040, 0.2046, 0.2034, 0.2020, 0.2021]
  val_value_value_mse : [0.2160, 0.2129, 0.2142, 0.2121, 0.2129, 0.2040, 0.2046, 0.2034, 0.2020, 0.2021]
  value_loss : [0.2208, 0.2192, 0.2180, 0.2176, 0.2166, 0.2119, 0.2108, 0.2096, 0.2093, 0.2069]
  value_value_mse : [0.2208, 0.2192, 0.2180, 0.2176, 0.2166, 0.2119, 0.2108, 0.2096, 0.2093, 0.2069]

================================================================================

History file: model_versions/chess_elo_model_V46_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:33:31.469111Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game67501_game69000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V45
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.422962   (epoch 10)   mode=min
  policy_loss: 2.321722   (epoch 10)   mode=min
  policy_policy_acc: 0.312596   (epoch 10)   mode=max
  val_loss: 2.622730   (epoch 4)   mode=min
  val_policy_loss: 2.519203   (epoch 4)   mode=min
  val_policy_policy_acc: 0.284941   (epoch 9)   mode=max
  val_value_loss: 0.197362   (epoch 10)   mode=min
  val_value_value_mse: 0.197332   (epoch 10)   mode=min
  value_loss: 0.202600   (epoch 10)   mode=min
  value_value_mse: 0.202598   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7268, 2.6787, 2.6431, 2.6173, 2.5866, 2.5637, 2.5004, 2.4767, 2.4381, 2.4230]
  policy_loss : [2.6176, 2.5712, 2.5359, 2.5104, 2.4803, 2.4581, 2.3963, 2.3740, 2.3364, 2.3217]
  policy_policy_acc : [0.2551, 0.2634, 0.2709, 0.2743, 0.2815, 0.2838, 0.2974, 0.3021, 0.3101, 0.3126]
  val_loss : [2.6479, 2.6351, 2.6480, 2.6227, 2.7108, 2.6554, 2.6556, 2.6683, 2.6446, 2.6556]
  val_policy_loss : [2.5439, 2.5317, 2.5447, 2.5192, 2.6088, 2.5515, 2.5555, 2.5685, 2.5461, 2.5571]
  val_policy_policy_acc : [0.2688, 0.2759, 0.2710, 0.2789, 0.2763, 0.2754, 0.2799, 0.2796, 0.2849, 0.2821]
  val_value_loss : [0.2083, 0.2069, 0.2070, 0.2075, 0.2040, 0.2082, 0.2004, 0.2000, 0.1974, 0.1974]
  val_value_value_mse : [0.2083, 0.2069, 0.2070, 0.2075, 0.2040, 0.2082, 0.2004, 0.2000, 0.1974, 0.1973]
  value_loss : [0.2177, 0.2153, 0.2144, 0.2134, 0.2126, 0.2111, 0.2078, 0.2055, 0.2037, 0.2026]
  value_value_mse : [0.2177, 0.2153, 0.2144, 0.2134, 0.2126, 0.2111, 0.2078, 0.2055, 0.2037, 0.2026]

================================================================================

History file: model_versions/chess_elo_model_V47_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:42:00.348624Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game69001_game70500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V46
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.435212   (epoch 10)   mode=min
  policy_loss: 2.333755   (epoch 10)   mode=min
  policy_policy_acc: 0.311016   (epoch 10)   mode=max
  val_loss: 2.658670   (epoch 4)   mode=min
  val_policy_loss: 2.558555   (epoch 4)   mode=min
  val_policy_policy_acc: 0.274741   (epoch 9)   mode=max
  val_value_loss: 0.190518   (epoch 10)   mode=min
  val_value_value_mse: 0.190553   (epoch 10)   mode=min
  value_loss: 0.202990   (epoch 10)   mode=min
  value_value_mse: 0.202991   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7441, 2.6890, 2.6558, 2.6285, 2.6014, 2.5748, 2.5089, 2.4875, 2.4502, 2.4352]
  policy_loss : [2.6346, 2.5804, 2.5478, 2.5212, 2.4948, 2.4685, 2.4048, 2.3843, 2.3484, 2.3338]
  policy_policy_acc : [0.2534, 0.2637, 0.2687, 0.2742, 0.2790, 0.2837, 0.2978, 0.2997, 0.3062, 0.3110]
  val_loss : [2.6901, 2.6794, 2.6824, 2.6587, 2.7645, 2.8931, 2.8240, 2.7051, 2.7404, 2.7277]
  val_policy_loss : [2.5852, 2.5771, 2.5793, 2.5586, 2.6639, 2.7901, 2.7251, 2.6073, 2.6447, 2.6322]
  val_policy_policy_acc : [0.2597, 0.2668, 0.2660, 0.2715, 0.2635, 0.2649, 0.2698, 0.2719, 0.2747, 0.2732]
  val_value_loss : [0.2102, 0.2039, 0.2062, 0.1999, 0.2005, 0.2049, 0.1973, 0.1957, 0.1909, 0.1905]
  val_value_value_mse : [0.2102, 0.2039, 0.2063, 0.2000, 0.2005, 0.2049, 0.1973, 0.1957, 0.1909, 0.1906]
  value_loss : [0.2190, 0.2172, 0.2160, 0.2145, 0.2132, 0.2126, 0.2084, 0.2063, 0.2037, 0.2030]
  value_value_mse : [0.2190, 0.2172, 0.2160, 0.2145, 0.2132, 0.2126, 0.2084, 0.2063, 0.2037, 0.2030]

================================================================================

History file: model_versions/chess_elo_model_V48_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:50:20.356524Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game70501_game72000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V47
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.420876   (epoch 10)   mode=min
  policy_loss: 2.320364   (epoch 10)   mode=min
  policy_policy_acc: 0.312176   (epoch 10)   mode=max
  val_loss: 2.688091   (epoch 7)   mode=min
  val_policy_loss: 2.587139   (epoch 7)   mode=min
  val_policy_policy_acc: 0.277326   (epoch 10)   mode=max
  val_value_loss: 0.198359   (epoch 9)   mode=min
  val_value_value_mse: 0.198368   (epoch 9)   mode=min
  value_loss: 0.201021   (epoch 10)   mode=min
  value_value_mse: 0.201021   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7203, 2.6725, 2.6418, 2.6108, 2.5857, 2.5194, 2.4945, 2.4771, 2.4597, 2.4209]
  policy_loss : [2.6123, 2.5656, 2.5349, 2.5052, 2.4805, 2.4164, 2.3923, 2.3756, 2.3585, 2.3204]
  policy_policy_acc : [0.2574, 0.2673, 0.2717, 0.2770, 0.2823, 0.2942, 0.2990, 0.3021, 0.3056, 0.3122]
  val_loss : [2.8880, 2.7583, 2.7163, 2.7400, 2.7328, 2.7511, 2.6881, 2.6884, 2.7021, 2.7793]
  val_policy_loss : [2.7821, 2.6520, 2.6124, 2.6371, 2.6314, 2.6509, 2.5871, 2.5890, 2.6030, 2.6797]
  val_policy_policy_acc : [0.2633, 0.2702, 0.2656, 0.2662, 0.2660, 0.2723, 0.2755, 0.2714, 0.2742, 0.2773]
  val_value_loss : [0.2118, 0.2128, 0.2081, 0.2058, 0.2029, 0.2004, 0.2021, 0.1988, 0.1984, 0.1993]
  val_value_value_mse : [0.2118, 0.2128, 0.2081, 0.2058, 0.2029, 0.2004, 0.2021, 0.1988, 0.1984, 0.1993]
  value_loss : [0.2158, 0.2137, 0.2139, 0.2111, 0.2104, 0.2061, 0.2043, 0.2030, 0.2023, 0.2010]
  value_value_mse : [0.2158, 0.2137, 0.2139, 0.2111, 0.2104, 0.2061, 0.2043, 0.2030, 0.2023, 0.2010]

================================================================================

History file: model_versions/chess_elo_model_V49_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T01:57:52.466591Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game72001_game73500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V48
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.430664   (epoch 9)   mode=min
  policy_loss: 2.326647   (epoch 9)   mode=min
  policy_policy_acc: 0.311693   (epoch 9)   mode=max
  val_loss: 2.675325   (epoch 3)   mode=min
  val_policy_loss: 2.569651   (epoch 3)   mode=min
  val_policy_policy_acc: 0.284815   (epoch 5)   mode=max
  val_value_loss: 0.203785   (epoch 9)   mode=min
  val_value_value_mse: 0.203771   (epoch 9)   mode=min
  value_loss: 0.208306   (epoch 9)   mode=min
  value_value_mse: 0.208318   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7056, 2.6575, 2.6207, 2.5913, 2.5678, 2.5045, 2.4784, 2.4412, 2.4307]
  policy_loss : [2.5950, 2.5475, 2.5118, 2.4828, 2.4595, 2.3984, 2.3727, 2.3368, 2.3266]
  policy_policy_acc : [0.2595, 0.2677, 0.2762, 0.2817, 0.2861, 0.2972, 0.3031, 0.3097, 0.3117]
  val_loss : [2.9182, 2.8988, 2.6753, 3.1117, 2.8033, 2.7586, 2.7653, 2.7883, 2.6876]
  val_policy_loss : [2.8113, 2.7915, 2.5697, 3.0046, 2.6986, 2.6559, 2.6629, 2.6868, 2.5865]
  val_policy_policy_acc : [0.2705, 0.2716, 0.2759, 0.2788, 0.2848, 0.2826, 0.2803, 0.2816, 0.2836]
  val_value_loss : [0.2149, 0.2157, 0.2129, 0.2150, 0.2108, 0.2069, 0.2064, 0.2044, 0.2038]
  val_value_value_mse : [0.2149, 0.2157, 0.2129, 0.2149, 0.2108, 0.2069, 0.2064, 0.2044, 0.2038]
  value_loss : [0.2212, 0.2200, 0.2184, 0.2169, 0.2165, 0.2120, 0.2112, 0.2091, 0.2083]
  value_value_mse : [0.2212, 0.2200, 0.2184, 0.2169, 0.2165, 0.2120, 0.2112, 0.2091, 0.2083]

================================================================================

History file: model_versions/chess_elo_model_V50_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:05:43.125037Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game73501_game75000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V49
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.435232   (epoch 9)   mode=min
  policy_loss: 2.332962   (epoch 9)   mode=min
  policy_policy_acc: 0.310095   (epoch 9)   mode=max
  val_loss: 2.652149   (epoch 3)   mode=min
  val_policy_loss: 2.548667   (epoch 3)   mode=min
  val_policy_policy_acc: 0.285101   (epoch 6)   mode=max
  val_value_loss: 0.199719   (epoch 9)   mode=min
  val_value_value_mse: 0.199723   (epoch 9)   mode=min
  value_loss: 0.204484   (epoch 9)   mode=min
  value_value_mse: 0.204479   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7090, 2.6601, 2.6260, 2.6013, 2.5742, 2.5100, 2.4876, 2.4499, 2.4352]
  policy_loss : [2.6004, 2.5522, 2.5189, 2.4943, 2.4680, 2.4055, 2.3837, 2.3470, 2.3330]
  policy_policy_acc : [0.2604, 0.2683, 0.2734, 0.2797, 0.2838, 0.2964, 0.3007, 0.3094, 0.3101]
  val_loss : [2.8686, 2.6993, 2.6521, 2.7141, 2.6836, 2.6623, 2.6963, 2.6790, 2.6939]
  val_policy_loss : [2.7621, 2.5958, 2.5487, 2.6100, 2.5800, 2.5612, 2.5948, 2.5790, 2.5942]
  val_policy_policy_acc : [0.2719, 0.2725, 0.2760, 0.2802, 0.2739, 0.2851, 0.2813, 0.2841, 0.2847]
  val_value_loss : [0.2131, 0.2071, 0.2071, 0.2083, 0.2073, 0.2022, 0.2031, 0.2000, 0.1997]
  val_value_value_mse : [0.2131, 0.2071, 0.2071, 0.2083, 0.2073, 0.2022, 0.2031, 0.2000, 0.1997]
  value_loss : [0.2173, 0.2159, 0.2143, 0.2142, 0.2122, 0.2090, 0.2075, 0.2055, 0.2045]
  value_value_mse : [0.2173, 0.2159, 0.2143, 0.2142, 0.2122, 0.2090, 0.2075, 0.2055, 0.2045]

================================================================================

History file: model_versions/chess_elo_model_V51_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:11:37.520908Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game75001_game76500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V50
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.500626   (epoch 7)   mode=min
  policy_loss: 2.396299   (epoch 7)   mode=min
  policy_policy_acc: 0.298218   (epoch 7)   mode=max
  val_loss: 2.638820   (epoch 6)   mode=min
  val_policy_loss: 2.533818   (epoch 1)   mode=min
  val_policy_policy_acc: 0.291214   (epoch 7)   mode=max
  val_value_loss: 0.203929   (epoch 7)   mode=min
  val_value_value_mse: 0.203914   (epoch 7)   mode=min
  value_loss: 0.208652   (epoch 7)   mode=min
  value_value_mse: 0.208653   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7166, 2.6665, 2.6368, 2.5747, 2.5479, 2.5139, 2.5006]
  policy_loss : [2.6072, 2.5583, 2.5283, 2.4682, 2.4424, 2.4089, 2.3963]
  policy_policy_acc : [0.2561, 0.2672, 0.2735, 0.2845, 0.2892, 0.2957, 0.2982]
  val_loss : [2.6423, 2.7469, 2.7351, 2.6410, 2.6459, 2.6388, 2.6570]
  val_policy_loss : [2.5338, 2.6355, 2.6245, 2.5355, 2.5399, 2.5343, 2.5532]
  val_policy_policy_acc : [0.2774, 0.2707, 0.2785, 0.2846, 0.2882, 0.2909, 0.2912]
  val_value_loss : [0.2137, 0.2189, 0.2173, 0.2072, 0.2082, 0.2053, 0.2039]
  val_value_value_mse : [0.2138, 0.2190, 0.2173, 0.2072, 0.2082, 0.2053, 0.2039]
  value_loss : [0.2189, 0.2164, 0.2170, 0.2130, 0.2110, 0.2098, 0.2087]
  value_value_mse : [0.2189, 0.2164, 0.2170, 0.2130, 0.2110, 0.2098, 0.2087]

================================================================================

History file: model_versions/chess_elo_model_V52_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:20:28.622936Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game76501_game78000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V51
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.459471   (epoch 10)   mode=min
  policy_loss: 2.355815   (epoch 10)   mode=min
  policy_policy_acc: 0.308866   (epoch 10)   mode=max
  val_loss: 2.619740   (epoch 8)   mode=min
  val_policy_loss: 2.529050   (epoch 8)   mode=min
  val_policy_policy_acc: 0.282949   (epoch 6)   mode=max
  val_value_loss: 0.201410   (epoch 10)   mode=min
  val_value_value_mse: 0.201547   (epoch 10)   mode=min
  value_loss: 0.207327   (epoch 10)   mode=min
  value_value_mse: 0.207326   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]
  loss : [2.6981, 2.6565, 2.6245, 2.5639, 2.5398, 2.5063, 2.4903, 2.4764, 2.4689, 2.4595]
  policy_loss : [2.5888, 2.5479, 2.5164, 2.4575, 2.4341, 2.4017, 2.3860, 2.3722, 2.3652, 2.3558]
  policy_policy_acc : [0.2645, 0.2731, 0.2773, 0.2877, 0.2945, 0.3007, 0.3025, 0.3077, 0.3078, 0.3089]
  val_loss : [2.6505, 2.6612, 2.6508, 2.6596, 2.6757, 2.6367, 2.6340, 2.6197, 2.6277, 2.7256]
  val_policy_loss : [2.5519, 2.5636, 2.5578, 2.5638, 2.5838, 2.5456, 2.5427, 2.5290, 2.5375, 2.6357]
  val_policy_policy_acc : [0.2724, 0.2743, 0.2673, 0.2764, 0.2756, 0.2829, 0.2804, 0.2796, 0.2793, 0.2806]
  val_value_loss : [0.2121, 0.2121, 0.2097, 0.2102, 0.2059, 0.2045, 0.2040, 0.2022, 0.2027, 0.2014]
  val_value_value_mse : [0.2121, 0.2121, 0.2098, 0.2103, 0.2061, 0.2045, 0.2042, 0.2023, 0.2028, 0.2015]
  value_loss : [0.2187, 0.2173, 0.2164, 0.2129, 0.2114, 0.2092, 0.2085, 0.2085, 0.2074, 0.2073]
  value_value_mse : [0.2187, 0.2173, 0.2164, 0.2129, 0.2114, 0.2092, 0.2085, 0.2085, 0.2074, 0.2073]

================================================================================

History file: model_versions/chess_elo_model_V53_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:28:54.996616Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game78001_game79500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V52
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.489033   (epoch 10)   mode=min
  policy_loss: 2.386366   (epoch 10)   mode=min
  policy_policy_acc: 0.301460   (epoch 10)   mode=max
  val_loss: 2.614834   (epoch 8)   mode=min
  val_policy_loss: 2.509576   (epoch 8)   mode=min
  val_policy_policy_acc: 0.282839   (epoch 9)   mode=max
  val_value_loss: 0.201535   (epoch 8)   mode=min
  val_value_value_mse: 0.201392   (epoch 8)   mode=min
  value_loss: 0.205321   (epoch 10)   mode=min
  value_value_mse: 0.205321   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]
  loss : [2.7329, 2.6914, 2.6540, 2.5906, 2.5692, 2.5368, 2.5189, 2.5072, 2.4976, 2.4890]
  policy_loss : [2.6239, 2.5827, 2.5456, 2.4846, 2.4640, 2.4326, 2.4154, 2.4039, 2.3945, 2.3864]
  policy_policy_acc : [0.2545, 0.2640, 0.2716, 0.2828, 0.2862, 0.2942, 0.2954, 0.2984, 0.3003, 0.3015]
  val_loss : [2.6535, 2.6637, 2.6673, 2.6755, 2.6737, 2.6752, 2.6201, 2.6148, 2.6194, 2.6226]
  val_policy_loss : [2.5437, 2.5497, 2.5565, 2.5673, 2.5653, 2.5688, 2.5140, 2.5096, 2.5134, 2.5171]
  val_policy_policy_acc : [0.2717, 0.2694, 0.2689, 0.2726, 0.2770, 0.2787, 0.2800, 0.2795, 0.2828, 0.2820]
  val_value_loss : [0.2115, 0.2195, 0.2127, 0.2073, 0.2086, 0.2040, 0.2030, 0.2015, 0.2025, 0.2016]
  val_value_value_mse : [0.2115, 0.2195, 0.2127, 0.2073, 0.2085, 0.2039, 0.2029, 0.2014, 0.2023, 0.2015]
  value_loss : [0.2180, 0.2175, 0.2168, 0.2121, 0.2102, 0.2083, 0.2070, 0.2066, 0.2062, 0.2053]
  value_value_mse : [0.2180, 0.2175, 0.2168, 0.2121, 0.2102, 0.2083, 0.2070, 0.2066, 0.2062, 0.2053]

================================================================================

History file: model_versions/chess_elo_model_V54_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:36:30.021420Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game79501_game81000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V53
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.442137   (epoch 9)   mode=min
  policy_loss: 2.341056   (epoch 9)   mode=min
  policy_policy_acc: 0.309400   (epoch 9)   mode=max
  val_loss: 2.634284   (epoch 3)   mode=min
  val_policy_loss: 2.532509   (epoch 3)   mode=min
  val_policy_policy_acc: 0.285785   (epoch 9)   mode=max
  val_value_loss: 0.198669   (epoch 9)   mode=min
  val_value_value_mse: 0.198626   (epoch 9)   mode=min
  value_loss: 0.201924   (epoch 9)   mode=min
  value_value_mse: 0.201950   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7053, 2.6575, 2.6291, 2.6033, 2.5722, 2.5119, 2.4895, 2.4495, 2.4421]
  policy_loss : [2.5981, 2.5515, 2.5236, 2.4980, 2.4677, 2.4095, 2.3869, 2.3478, 2.3411]
  policy_policy_acc : [0.2590, 0.2688, 0.2736, 0.2785, 0.2834, 0.2951, 0.3008, 0.3076, 0.3094]
  val_loss : [2.6465, 2.6755, 2.6343, 2.6653, 2.9071, 2.6657, 2.6836, 2.6562, 2.6864]
  val_policy_loss : [2.5438, 2.5743, 2.5325, 2.5633, 2.8067, 2.5672, 2.5843, 2.5571, 2.5880]
  val_policy_policy_acc : [0.2725, 0.2719, 0.2739, 0.2739, 0.2771, 0.2816, 0.2825, 0.2855, 0.2858]
  val_value_loss : [0.2078, 0.2051, 0.2064, 0.2062, 0.2025, 0.1992, 0.2008, 0.2003, 0.1987]
  val_value_value_mse : [0.2078, 0.2051, 0.2064, 0.2062, 0.2025, 0.1992, 0.2008, 0.2003, 0.1986]
  value_loss : [0.2133, 0.2131, 0.2118, 0.2112, 0.2100, 0.2068, 0.2058, 0.2028, 0.2019]
  value_value_mse : [0.2133, 0.2131, 0.2118, 0.2111, 0.2100, 0.2068, 0.2058, 0.2028, 0.2020]

================================================================================

History file: model_versions/chess_elo_model_V55_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:45:01.050530Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game81001_game82500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V54
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.464319   (epoch 10)   mode=min
  policy_loss: 2.365036   (epoch 10)   mode=min
  policy_policy_acc: 0.303938   (epoch 10)   mode=max
  val_loss: 2.634465   (epoch 4)   mode=min
  val_policy_loss: 2.535516   (epoch 4)   mode=min
  val_policy_policy_acc: 0.280545   (epoch 8)   mode=max
  val_value_loss: 0.192852   (epoch 9)   mode=min
  val_value_value_mse: 0.192842   (epoch 9)   mode=min
  value_loss: 0.198668   (epoch 10)   mode=min
  value_value_mse: 0.198648   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7177, 2.6736, 2.6410, 2.5812, 2.5563, 2.5371, 2.5035, 2.4877, 2.4694, 2.4643]
  policy_loss : [2.6108, 2.5673, 2.5358, 2.4786, 2.4541, 2.4356, 2.4031, 2.3879, 2.3705, 2.3650]
  policy_policy_acc : [0.2545, 0.2644, 0.2711, 0.2812, 0.2852, 0.2898, 0.2959, 0.2999, 0.3028, 0.3039]
  val_loss : [2.6463, 2.7479, 2.6532, 2.6345, 2.6960, 2.7919, 2.9173, 2.9382, 2.7584, 2.9500]
  val_policy_loss : [2.5430, 2.6455, 2.5510, 2.5355, 2.5969, 2.6942, 2.8200, 2.8410, 2.6621, 2.8531]
  val_policy_policy_acc : [0.2611, 0.2621, 0.2649, 0.2739, 0.2753, 0.2702, 0.2777, 0.2805, 0.2784, 0.2786]
  val_value_loss : [0.2071, 0.2051, 0.2051, 0.1983, 0.1984, 0.1956, 0.1946, 0.1941, 0.1929, 0.1934]
  val_value_value_mse : [0.2071, 0.2051, 0.2051, 0.1983, 0.1984, 0.1955, 0.1946, 0.1941, 0.1928, 0.1934]
  value_loss : [0.2133, 0.2119, 0.2103, 0.2056, 0.2046, 0.2033, 0.2011, 0.1995, 0.1988, 0.1987]
  value_value_mse : [0.2132, 0.2119, 0.2103, 0.2056, 0.2045, 0.2032, 0.2011, 0.1995, 0.1988, 0.1986]

================================================================================

History file: model_versions/chess_elo_model_V56_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T02:53:26.620452Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game82501_game84000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V55
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.469629   (epoch 10)   mode=min
  policy_loss: 2.370131   (epoch 10)   mode=min
  policy_policy_acc: 0.304052   (epoch 10)   mode=max
  val_loss: 2.650977   (epoch 4)   mode=min
  val_policy_loss: 2.548394   (epoch 4)   mode=min
  val_policy_policy_acc: 0.272557   (epoch 5)   mode=max
  val_value_loss: 0.194784   (epoch 10)   mode=min
  val_value_value_mse: 0.194617   (epoch 10)   mode=min
  value_loss: 0.198940   (epoch 9)   mode=min
  value_value_mse: 0.198950   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7192, 2.6753, 2.6466, 2.5863, 2.5631, 2.5423, 2.5088, 2.4957, 2.4750, 2.4696]
  policy_loss : [2.6123, 2.5693, 2.5414, 2.4832, 2.4609, 2.4411, 2.4081, 2.3954, 2.3755, 2.3701]
  policy_policy_acc : [0.2555, 0.2640, 0.2690, 0.2799, 0.2840, 0.2895, 0.2955, 0.2987, 0.3012, 0.3041]
  val_loss : [2.7190, 3.3531, 2.7458, 2.6510, 2.7023, 2.8111, 4.3623, 4.6456, 2.9926, 3.5723]
  val_policy_loss : [2.6117, 3.2469, 2.6425, 2.5484, 2.6004, 2.7085, 4.2580, 4.5408, 2.8913, 3.4702]
  val_policy_policy_acc : [0.2618, 0.2625, 0.2616, 0.2688, 0.2726, 0.2660, 0.2686, 0.2709, 0.2715, 0.2716]
  val_value_loss : [0.2100, 0.2044, 0.2020, 0.2005, 0.1990, 0.1996, 0.1955, 0.1953, 0.1959, 0.1948]
  val_value_value_mse : [0.2100, 0.2043, 0.2018, 0.2004, 0.1988, 0.1995, 0.1954, 0.1952, 0.1958, 0.1946]
  value_loss : [0.2140, 0.2118, 0.2102, 0.2062, 0.2047, 0.2026, 0.2014, 0.2007, 0.1989, 0.1994]
  value_value_mse : [0.2140, 0.2118, 0.2101, 0.2062, 0.2047, 0.2026, 0.2015, 0.2007, 0.1990, 0.1994]

================================================================================

History file: model_versions/chess_elo_model_V57_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:02:09.949460Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game84001_game85500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V56
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.429909   (epoch 10)   mode=min
  policy_loss: 2.331449   (epoch 10)   mode=min
  policy_policy_acc: 0.311431   (epoch 10)   mode=max
  val_loss: 2.698390   (epoch 5)   mode=min
  val_policy_loss: 2.600460   (epoch 5)   mode=min
  val_policy_policy_acc: 0.269216   (epoch 10)   mode=max
  val_value_loss: 0.191406   (epoch 10)   mode=min
  val_value_value_mse: 0.191686   (epoch 10)   mode=min
  value_loss: 0.196823   (epoch 10)   mode=min
  value_value_mse: 0.196825   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003]
  loss : [2.7089, 2.6651, 2.6381, 2.6153, 2.5887, 2.5644, 2.5437, 2.4846, 2.4594, 2.4299]
  policy_loss : [2.6033, 2.5602, 2.5337, 2.5113, 2.4852, 2.4616, 2.4418, 2.3843, 2.3605, 2.3314]
  policy_policy_acc : [0.2575, 0.2672, 0.2717, 0.2768, 0.2814, 0.2850, 0.2879, 0.2993, 0.3040, 0.3114]
  val_loss : [2.8321, 2.7804, 2.7327, 2.7227, 2.6984, 3.6611, 4.0156, 3.5171, 3.0809, 3.8610]
  val_policy_loss : [2.7297, 2.6780, 2.6333, 2.6226, 2.6005, 3.5594, 3.9114, 3.4181, 2.9828, 3.7629]
  val_policy_policy_acc : [0.2552, 0.2535, 0.2581, 0.2568, 0.2594, 0.2641, 0.2544, 0.2682, 0.2649, 0.2692]
  val_value_loss : [0.2058, 0.2062, 0.2001, 0.2026, 0.1969, 0.1990, 0.2027, 0.1954, 0.1953, 0.1914]
  val_value_value_mse : [0.2060, 0.2064, 0.2004, 0.2027, 0.1972, 0.1992, 0.2029, 0.1957, 0.1956, 0.1917]
  value_loss : [0.2111, 0.2098, 0.2089, 0.2079, 0.2067, 0.2055, 0.2039, 0.2009, 0.1978, 0.1968]
  value_value_mse : [0.2111, 0.2099, 0.2089, 0.2079, 0.2067, 0.2055, 0.2039, 0.2009, 0.1978, 0.1968]

================================================================================

History file: model_versions/chess_elo_model_V58_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:09:57.311988Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game85501_game87000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V57
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.470723   (epoch 9)   mode=min
  policy_loss: 2.369045   (epoch 9)   mode=min
  policy_policy_acc: 0.303635   (epoch 9)   mode=max
  val_loss: 2.676058   (epoch 3)   mode=min
  val_policy_loss: 2.572080   (epoch 3)   mode=min
  val_policy_policy_acc: 0.282139   (epoch 7)   mode=max
  val_value_loss: 0.199628   (epoch 8)   mode=min
  val_value_value_mse: 0.199557   (epoch 8)   mode=min
  value_loss: 0.203363   (epoch 9)   mode=min
  value_value_mse: 0.203364   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7272, 2.6824, 2.6485, 2.6240, 2.5969, 2.5411, 2.5197, 2.4817, 2.4707]
  policy_loss : [2.6192, 2.5747, 2.5416, 2.5173, 2.4909, 2.4368, 2.4165, 2.3795, 2.3690]
  policy_policy_acc : [0.2555, 0.2638, 0.2699, 0.2741, 0.2798, 0.2891, 0.2934, 0.3018, 0.3036]
  val_loss : [2.7065, 2.6945, 2.6761, 2.6863, 2.6792, 2.6995, 2.6902, 2.7407, 2.7238]
  val_policy_loss : [2.5998, 2.5875, 2.5721, 2.5815, 2.5730, 2.5975, 2.5895, 2.6404, 2.6234]
  val_policy_policy_acc : [0.2686, 0.2714, 0.2712, 0.2699, 0.2756, 0.2793, 0.2821, 0.2820, 0.2802]
  val_value_loss : [0.2126, 0.2134, 0.2072, 0.2088, 0.2115, 0.2031, 0.2005, 0.1996, 0.2001]
  val_value_value_mse : [0.2125, 0.2134, 0.2071, 0.2087, 0.2115, 0.2030, 0.2004, 0.1996, 0.2001]
  value_loss : [0.2161, 0.2153, 0.2138, 0.2133, 0.2121, 0.2087, 0.2065, 0.2044, 0.2034]
  value_value_mse : [0.2161, 0.2153, 0.2138, 0.2133, 0.2121, 0.2087, 0.2065, 0.2044, 0.2034]

================================================================================

History file: model_versions/chess_elo_model_V59_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:18:34.870517Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game87001_game88500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V58
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.474995   (epoch 10)   mode=min
  policy_loss: 2.372839   (epoch 10)   mode=min
  policy_policy_acc: 0.300675   (epoch 10)   mode=max
  val_loss: 2.674819   (epoch 5)   mode=min
  val_policy_loss: 2.574010   (epoch 5)   mode=min
  val_policy_policy_acc: 0.285982   (epoch 10)   mode=max
  val_value_loss: 0.198421   (epoch 9)   mode=min
  val_value_value_mse: 0.198421   (epoch 9)   mode=min
  value_loss: 0.204261   (epoch 10)   mode=min
  value_value_mse: 0.204265   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7283, 2.6857, 2.6557, 2.5989, 2.5747, 2.5520, 2.5392, 2.5085, 2.4915, 2.4750]
  policy_loss : [2.6204, 2.5784, 2.5485, 2.4940, 2.4703, 2.4484, 2.4358, 2.4058, 2.3893, 2.3728]
  policy_policy_acc : [0.2529, 0.2627, 0.2665, 0.2789, 0.2829, 0.2880, 0.2883, 0.2949, 0.2968, 0.3007]
  val_loss : [2.7307, 3.3182, 3.3561, 2.8549, 2.6748, 2.8584, 3.3933, 3.8798, 3.5140, 3.2166]
  val_policy_loss : [2.6264, 3.2136, 3.2526, 2.7530, 2.5740, 2.7582, 3.2932, 3.7797, 3.4148, 3.1170]
  val_policy_policy_acc : [0.2741, 0.2720, 0.2735, 0.2839, 0.2828, 0.2812, 0.2823, 0.2841, 0.2839, 0.2860]
  val_value_loss : [0.2085, 0.2092, 0.2070, 0.2037, 0.2016, 0.2004, 0.2003, 0.2002, 0.1984, 0.1991]
  val_value_value_mse : [0.2085, 0.2092, 0.2070, 0.2037, 0.2016, 0.2004, 0.2003, 0.2002, 0.1984, 0.1991]
  value_loss : [0.2159, 0.2146, 0.2143, 0.2098, 0.2089, 0.2072, 0.2068, 0.2052, 0.2045, 0.2043]
  value_value_mse : [0.2159, 0.2146, 0.2143, 0.2098, 0.2089, 0.2072, 0.2068, 0.2052, 0.2045, 0.2043]

================================================================================

History file: model_versions/chess_elo_model_V60_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:27:08.623133Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game88501_game90000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V59
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.491372   (epoch 10)   mode=min
  policy_loss: 2.390184   (epoch 10)   mode=min
  policy_policy_acc: 0.299496   (epoch 10)   mode=max
  val_loss: 2.638083   (epoch 7)   mode=min
  val_policy_loss: 2.537488   (epoch 7)   mode=min
  val_policy_policy_acc: 0.280143   (epoch 10)   mode=max
  val_value_loss: 0.196453   (epoch 10)   mode=min
  val_value_value_mse: 0.196457   (epoch 10)   mode=min
  value_loss: 0.202194   (epoch 10)   mode=min
  value_value_mse: 0.202203   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.7266, 2.6806, 2.6526, 2.5949, 2.5749, 2.5422, 2.5309, 2.5179, 2.5080, 2.4914]
  policy_loss : [2.6189, 2.5737, 2.5456, 2.4897, 2.4705, 2.4396, 2.4281, 2.4155, 2.4061, 2.3902]
  policy_policy_acc : [0.2529, 0.2614, 0.2676, 0.2761, 0.2831, 0.2896, 0.2910, 0.2940, 0.2977, 0.2995]
  val_loss : [2.6601, 2.6960, 2.6864, 2.6690, 2.6634, 2.6383, 2.6381, 2.6430, 2.6773, 2.6758]
  val_policy_loss : [2.5547, 2.5920, 2.5817, 2.5657, 2.5629, 2.5383, 2.5375, 2.5438, 2.5777, 2.5776]
  val_policy_policy_acc : [0.2694, 0.2683, 0.2676, 0.2711, 0.2734, 0.2746, 0.2759, 0.2784, 0.2775, 0.2801]
  val_value_loss : [0.2109, 0.2080, 0.2094, 0.2066, 0.2009, 0.2001, 0.2011, 0.1983, 0.1993, 0.1965]
  val_value_value_mse : [0.2109, 0.2080, 0.2094, 0.2066, 0.2010, 0.2001, 0.2012, 0.1983, 0.1993, 0.1965]
  value_loss : [0.2156, 0.2143, 0.2140, 0.2106, 0.2085, 0.2050, 0.2057, 0.2048, 0.2036, 0.2022]
  value_value_mse : [0.2156, 0.2143, 0.2140, 0.2106, 0.2085, 0.2050, 0.2058, 0.2048, 0.2036, 0.2022]

================================================================================

History file: model_versions/chess_elo_model_V61_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:35:59.572440Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game90001_game91500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V60
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.458667   (epoch 10)   mode=min
  policy_loss: 2.354613   (epoch 10)   mode=min
  policy_policy_acc: 0.305478   (epoch 10)   mode=max
  val_loss: 2.621581   (epoch 5)   mode=min
  val_policy_loss: 2.516410   (epoch 5)   mode=min
  val_policy_policy_acc: 0.279647   (epoch 10)   mode=max
  val_value_loss: 0.204346   (epoch 10)   mode=min
  val_value_value_mse: 0.204333   (epoch 10)   mode=min
  value_loss: 0.208116   (epoch 10)   mode=min
  value_value_mse: 0.208117   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7210, 2.6776, 2.6425, 2.6219, 2.5620, 2.5410, 2.5234, 2.4897, 2.4775, 2.4587]
  policy_loss : [2.6111, 2.5685, 2.5342, 2.5138, 2.4551, 2.4350, 2.4180, 2.3853, 2.3732, 2.3546]
  policy_policy_acc : [0.2553, 0.2624, 0.2693, 0.2717, 0.2837, 0.2895, 0.2926, 0.2984, 0.3021, 0.3055]
  val_loss : [2.6522, 2.6498, 2.6703, 2.6551, 2.6216, 2.6334, 2.6788, 2.6380, 2.6457, 2.6431]
  val_policy_loss : [2.5440, 2.5404, 2.5640, 2.5493, 2.5164, 2.5288, 2.5746, 2.5352, 2.5435, 2.5409]
  val_policy_policy_acc : [0.2666, 0.2641, 0.2648, 0.2717, 0.2752, 0.2739, 0.2742, 0.2778, 0.2789, 0.2796]
  val_value_loss : [0.2166, 0.2189, 0.2128, 0.2119, 0.2105, 0.2091, 0.2086, 0.2057, 0.2044, 0.2043]
  val_value_value_mse : [0.2166, 0.2189, 0.2128, 0.2119, 0.2105, 0.2091, 0.2086, 0.2057, 0.2044, 0.2043]
  value_loss : [0.2199, 0.2182, 0.2166, 0.2163, 0.2137, 0.2121, 0.2110, 0.2089, 0.2086, 0.2081]
  value_value_mse : [0.2199, 0.2182, 0.2166, 0.2163, 0.2137, 0.2121, 0.2110, 0.2089, 0.2086, 0.2081]

================================================================================

History file: model_versions/chess_elo_model_V62_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:44:30.615908Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game91501_game93000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V61
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.469111   (epoch 10)   mode=min
  policy_loss: 2.363360   (epoch 10)   mode=min
  policy_policy_acc: 0.303357   (epoch 10)   mode=max
  val_loss: 2.609646   (epoch 7)   mode=min
  val_policy_loss: 2.504431   (epoch 7)   mode=min
  val_policy_policy_acc: 0.282821   (epoch 7)   mode=max
  val_value_loss: 0.206665   (epoch 10)   mode=min
  val_value_value_mse: 0.206685   (epoch 10)   mode=min
  value_loss: 0.211505   (epoch 10)   mode=min
  value_value_mse: 0.211505   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.7156, 2.6703, 2.6385, 2.5827, 2.5588, 2.5428, 2.5117, 2.4989, 2.4858, 2.4691]
  policy_loss : [2.6038, 2.5591, 2.5279, 2.4738, 2.4507, 2.4351, 2.4049, 2.3925, 2.3796, 2.3634]
  policy_policy_acc : [0.2557, 0.2651, 0.2699, 0.2812, 0.2850, 0.2898, 0.2953, 0.2966, 0.3004, 0.3034]
  val_loss : [2.6442, 2.6509, 2.6581, 2.6198, 2.6385, 2.6226, 2.6096, 2.6198, 2.6772, 2.6277]
  val_policy_loss : [2.5323, 2.5396, 2.5491, 2.5126, 2.5317, 2.5161, 2.5044, 2.5145, 2.5711, 2.5237]
  val_policy_policy_acc : [0.2720, 0.2678, 0.2719, 0.2792, 0.2814, 0.2752, 0.2828, 0.2808, 0.2800, 0.2807]
  val_value_loss : [0.2215, 0.2212, 0.2158, 0.2129, 0.2119, 0.2114, 0.2087, 0.2092, 0.2105, 0.2067]
  val_value_value_mse : [0.2215, 0.2213, 0.2159, 0.2129, 0.2119, 0.2114, 0.2088, 0.2092, 0.2105, 0.2067]
  value_loss : [0.2236, 0.2225, 0.2211, 0.2178, 0.2162, 0.2155, 0.2137, 0.2128, 0.2126, 0.2115]
  value_value_mse : [0.2236, 0.2225, 0.2211, 0.2178, 0.2162, 0.2155, 0.2137, 0.2128, 0.2126, 0.2115]

================================================================================

History file: model_versions/chess_elo_model_V63_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T03:52:58.789482Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game93001_game94500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V62
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.453953   (epoch 10)   mode=min
  policy_loss: 2.347677   (epoch 10)   mode=min
  policy_policy_acc: 0.305720   (epoch 10)   mode=max
  val_loss: 2.620233   (epoch 8)   mode=min
  val_policy_loss: 2.515219   (epoch 8)   mode=min
  val_policy_policy_acc: 0.286371   (epoch 8)   mode=max
  val_value_loss: 0.206761   (epoch 10)   mode=min
  val_value_value_mse: 0.206749   (epoch 10)   mode=min
  value_loss: 0.212666   (epoch 10)   mode=min
  value_value_mse: 0.212668   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005]
  loss : [2.7043, 2.6585, 2.6324, 2.6056, 2.5813, 2.5238, 2.5042, 2.4866, 2.4707, 2.4540]
  policy_loss : [2.5945, 2.5484, 2.5226, 2.4958, 2.4718, 2.4158, 2.3967, 2.3798, 2.3640, 2.3477]
  policy_policy_acc : [0.2578, 0.2666, 0.2720, 0.2774, 0.2808, 0.2913, 0.2956, 0.2992, 0.3019, 0.3057]
  val_loss : [2.6400, 2.6669, 2.6297, 2.6304, 2.6504, 2.6470, 2.6276, 2.6202, 2.6256, 2.6280]
  val_policy_loss : [2.5321, 2.5599, 2.5228, 2.5229, 2.5448, 2.5424, 2.5212, 2.5152, 2.5202, 2.5249]
  val_policy_policy_acc : [0.2745, 0.2683, 0.2749, 0.2767, 0.2732, 0.2810, 0.2849, 0.2864, 0.2828, 0.2826]
  val_value_loss : [0.2169, 0.2156, 0.2156, 0.2166, 0.2118, 0.2100, 0.2134, 0.2105, 0.2114, 0.2068]
  val_value_value_mse : [0.2169, 0.2156, 0.2156, 0.2166, 0.2117, 0.2100, 0.2134, 0.2104, 0.2114, 0.2067]
  value_loss : [0.2197, 0.2201, 0.2196, 0.2195, 0.2188, 0.2160, 0.2150, 0.2137, 0.2134, 0.2127]
  value_value_mse : [0.2197, 0.2201, 0.2196, 0.2195, 0.2188, 0.2160, 0.2150, 0.2137, 0.2134, 0.2127]

================================================================================

History file: model_versions/chess_elo_model_V64_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:00:47.185839Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game94501_game96000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V63
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.475979   (epoch 9)   mode=min
  policy_loss: 2.369747   (epoch 9)   mode=min
  policy_policy_acc: 0.304024   (epoch 9)   mode=max
  val_loss: 2.636729   (epoch 3)   mode=min
  val_policy_loss: 2.530794   (epoch 3)   mode=min
  val_policy_policy_acc: 0.281545   (epoch 9)   mode=max
  val_value_loss: 0.208897   (epoch 8)   mode=min
  val_value_value_mse: 0.208897   (epoch 8)   mode=min
  value_loss: 0.212418   (epoch 9)   mode=min
  value_value_mse: 0.212415   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7220, 2.6735, 2.6441, 2.6170, 2.5969, 2.5400, 2.5213, 2.4859, 2.4760]
  policy_loss : [2.6120, 2.5638, 2.5345, 2.5078, 2.4879, 2.4320, 2.4143, 2.3793, 2.3697]
  policy_policy_acc : [0.2560, 0.2659, 0.2716, 0.2763, 0.2820, 0.2907, 0.2957, 0.3018, 0.3040]
  val_loss : [2.6627, 2.6472, 2.6367, 2.6587, 2.6648, 2.7109, 2.6784, 2.6913, 2.6835]
  val_policy_loss : [2.5548, 2.5397, 2.5308, 2.5517, 2.5575, 2.6059, 2.5732, 2.5868, 2.5786]
  val_policy_policy_acc : [0.2688, 0.2671, 0.2704, 0.2705, 0.2732, 0.2746, 0.2760, 0.2780, 0.2815]
  val_value_loss : [0.2158, 0.2150, 0.2119, 0.2142, 0.2146, 0.2102, 0.2104, 0.2089, 0.2097]
  val_value_value_mse : [0.2158, 0.2150, 0.2119, 0.2142, 0.2146, 0.2102, 0.2104, 0.2089, 0.2097]
  value_loss : [0.2199, 0.2193, 0.2191, 0.2186, 0.2179, 0.2161, 0.2140, 0.2132, 0.2124]
  value_value_mse : [0.2199, 0.2193, 0.2191, 0.2186, 0.2179, 0.2161, 0.2140, 0.2132, 0.2124]

================================================================================

History file: model_versions/chess_elo_model_V65_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:06:44.346700Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game96001_game97500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V64
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.526597   (epoch 7)   mode=min
  policy_loss: 2.418671   (epoch 7)   mode=min
  policy_policy_acc: 0.294496   (epoch 7)   mode=max
  val_loss: 2.696166   (epoch 1)   mode=min
  val_policy_loss: 2.589385   (epoch 1)   mode=min
  val_policy_policy_acc: 0.274143   (epoch 7)   mode=max
  val_value_loss: 0.214420   (epoch 7)   mode=min
  val_value_value_mse: 0.214311   (epoch 7)   mode=min
  value_loss: 0.214168   (epoch 7)   mode=min
  value_value_mse: 0.214211   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7232, 2.6863, 2.6524, 2.5936, 2.5705, 2.5365, 2.5266]
  policy_loss : [2.6136, 2.5759, 2.5448, 2.4853, 2.4625, 2.4304, 2.4187]
  policy_policy_acc : [0.2576, 0.2653, 0.2712, 0.2831, 0.2854, 0.2912, 0.2945]
  val_loss : [2.6962, 3.0581, 3.2092, 3.9847, 3.0636, 2.8023, 3.9111]
  val_policy_loss : [2.5894, 2.9506, 3.1002, 3.8755, 2.9584, 2.6980, 3.8042]
  val_policy_policy_acc : [0.2609, 0.2606, 0.2662, 0.2681, 0.2710, 0.2726, 0.2741]
  val_value_loss : [0.2224, 0.2217, 0.2222, 0.2182, 0.2151, 0.2151, 0.2144]
  val_value_value_mse : [0.2224, 0.2216, 0.2222, 0.2181, 0.2151, 0.2150, 0.2143]
  value_loss : [0.2226, 0.2210, 0.2206, 0.2184, 0.2172, 0.2144, 0.2142]
  value_value_mse : [0.2227, 0.2209, 0.2207, 0.2183, 0.2173, 0.2144, 0.2142]

================================================================================

History file: model_versions/chess_elo_model_V66_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:14:21.475622Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game97501_game99000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V65
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.479284   (epoch 9)   mode=min
  policy_loss: 2.375056   (epoch 9)   mode=min
  policy_policy_acc: 0.302359   (epoch 9)   mode=max
  val_loss: 2.691268   (epoch 3)   mode=min
  val_policy_loss: 2.583174   (epoch 3)   mode=min
  val_policy_policy_acc: 0.274080   (epoch 8)   mode=max
  val_value_loss: 0.204788   (epoch 8)   mode=min
  val_value_value_mse: 0.204704   (epoch 8)   mode=min
  value_loss: 0.208351   (epoch 9)   mode=min
  value_value_mse: 0.208225   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7306, 2.6874, 2.6596, 2.6334, 2.6049, 2.5498, 2.5271, 2.4919, 2.4793]
  policy_loss : [2.6188, 2.5772, 2.5500, 2.5244, 2.4970, 2.4431, 2.4211, 2.3883, 2.3751]
  policy_policy_acc : [0.2539, 0.2630, 0.2685, 0.2720, 0.2773, 0.2878, 0.2925, 0.2990, 0.3024]
  val_loss : [2.7127, 2.8301, 2.6913, 2.7869, 2.8606, 2.8671, 2.8695, 2.7348, 2.6944]
  val_policy_loss : [2.6030, 2.7201, 2.5832, 2.6885, 2.7546, 2.7626, 2.7697, 2.6550, 2.6207]
  val_policy_policy_acc : [0.2623, 0.2569, 0.2617, 0.2655, 0.2602, 0.2708, 0.2706, 0.2741, 0.2740]
  val_value_loss : [0.2203, 0.2203, 0.2172, 0.2136, 0.2133, 0.2102, 0.2090, 0.2048, 0.2057]
  val_value_value_mse : [0.2202, 0.2202, 0.2171, 0.2135, 0.2130, 0.2100, 0.2089, 0.2047, 0.2057]
  value_loss : [0.2240, 0.2211, 0.2206, 0.2180, 0.2160, 0.2134, 0.2114, 0.2089, 0.2084]
  value_value_mse : [0.2240, 0.2211, 0.2207, 0.2179, 0.2160, 0.2133, 0.2113, 0.2088, 0.2082]

================================================================================

History file: model_versions/chess_elo_model_V67_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:23:00.427019Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game99001_game100500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V66
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.434210   (epoch 10)   mode=min
  policy_loss: 2.334741   (epoch 10)   mode=min
  policy_policy_acc: 0.308757   (epoch 10)   mode=max
  val_loss: 2.614957   (epoch 5)   mode=min
  val_policy_loss: 2.514407   (epoch 5)   mode=min
  val_policy_policy_acc: 0.280303   (epoch 10)   mode=max
  val_value_loss: 0.196648   (epoch 9)   mode=min
  val_value_value_mse: 0.196653   (epoch 9)   mode=min
  value_loss: 0.198872   (epoch 10)   mode=min
  value_value_mse: 0.198875   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003]
  loss : [2.7177, 2.6742, 2.6382, 2.6127, 2.5904, 2.5652, 2.5506, 2.4904, 2.4693, 2.4342]
  policy_loss : [2.6103, 2.5676, 2.5331, 2.5077, 2.4863, 2.4626, 2.4477, 2.3894, 2.3687, 2.3347]
  policy_policy_acc : [0.2551, 0.2637, 0.2704, 0.2768, 0.2802, 0.2833, 0.2862, 0.2984, 0.3030, 0.3088]
  val_loss : [2.6842, 2.6315, 2.6300, 2.6278, 2.6150, 2.6655, 2.6460, 2.6253, 2.6280, 2.6364]
  val_policy_loss : [2.5769, 2.5277, 2.5242, 2.5232, 2.5144, 2.5635, 2.5447, 2.5260, 2.5295, 2.5379]
  val_policy_policy_acc : [0.2632, 0.2673, 0.2694, 0.2701, 0.2717, 0.2722, 0.2688, 0.2725, 0.2760, 0.2803]
  val_value_loss : [0.2144, 0.2073, 0.2113, 0.2089, 0.2007, 0.2038, 0.2023, 0.1982, 0.1966, 0.1967]
  val_value_value_mse : [0.2144, 0.2073, 0.2113, 0.2089, 0.2007, 0.2038, 0.2023, 0.1982, 0.1967, 0.1967]
  value_loss : [0.2149, 0.2133, 0.2102, 0.2099, 0.2082, 0.2053, 0.2056, 0.2023, 0.2013, 0.1989]
  value_value_mse : [0.2149, 0.2133, 0.2102, 0.2099, 0.2082, 0.2053, 0.2056, 0.2023, 0.2013, 0.1989]

================================================================================

History file: model_versions/chess_elo_model_V68_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:31:16.314738Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game100501_game102000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V67
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.439758   (epoch 10)   mode=min
  policy_loss: 2.338470   (epoch 10)   mode=min
  policy_policy_acc: 0.308240   (epoch 10)   mode=max
  val_loss: 2.614944   (epoch 5)   mode=min
  val_policy_loss: 2.514164   (epoch 5)   mode=min
  val_policy_policy_acc: 0.282900   (epoch 9)   mode=max
  val_value_loss: 0.197269   (epoch 10)   mode=min
  val_value_value_mse: 0.197255   (epoch 10)   mode=min
  value_loss: 0.202809   (epoch 10)   mode=min
  value_value_mse: 0.202809   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6984, 2.6492, 2.6212, 2.5574, 2.5394, 2.5192, 2.5055, 2.4700, 2.4572, 2.4398]
  policy_loss : [2.5890, 2.5412, 2.5141, 2.4524, 2.4351, 2.4157, 2.4021, 2.3679, 2.3552, 2.3385]
  policy_policy_acc : [0.2588, 0.2692, 0.2745, 0.2861, 0.2909, 0.2927, 0.2969, 0.3037, 0.3072, 0.3082]
  val_loss : [2.6452, 2.6551, 2.6463, 2.6161, 2.6149, 2.6214, 2.6237, 2.6260, 2.6155, 2.6182]
  val_policy_loss : [2.5396, 2.5491, 2.5439, 2.5154, 2.5142, 2.5212, 2.5247, 2.5271, 2.5170, 2.5201]
  val_policy_policy_acc : [0.2687, 0.2710, 0.2763, 0.2763, 0.2797, 0.2776, 0.2802, 0.2812, 0.2829, 0.2803]
  val_value_loss : [0.2124, 0.2128, 0.2057, 0.2024, 0.2022, 0.2012, 0.1990, 0.1986, 0.1980, 0.1973]
  val_value_value_mse : [0.2124, 0.2128, 0.2058, 0.2024, 0.2022, 0.2012, 0.1990, 0.1986, 0.1980, 0.1973]
  value_loss : [0.2191, 0.2160, 0.2145, 0.2099, 0.2087, 0.2070, 0.2066, 0.2043, 0.2041, 0.2028]
  value_value_mse : [0.2191, 0.2160, 0.2145, 0.2099, 0.2087, 0.2070, 0.2066, 0.2043, 0.2041, 0.2028]

================================================================================

History file: model_versions/chess_elo_model_V69_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:37:53.966801Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game102001_game103500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V68
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.486965   (epoch 8)   mode=min
  policy_loss: 2.389620   (epoch 8)   mode=min
  policy_policy_acc: 0.300613   (epoch 8)   mode=max
  val_loss: 2.660501   (epoch 2)   mode=min
  val_policy_loss: 2.561190   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281382   (epoch 5)   mode=max
  val_value_loss: 0.186112   (epoch 7)   mode=min
  val_value_value_mse: 0.186105   (epoch 7)   mode=min
  value_loss: 0.194680   (epoch 8)   mode=min
  value_value_mse: 0.194675   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7179, 2.6728, 2.6427, 2.6129, 2.5535, 2.5331, 2.4993, 2.4870]
  policy_loss : [2.6128, 2.5692, 2.5402, 2.5115, 2.4538, 2.4346, 2.4016, 2.3896]
  policy_policy_acc : [0.2580, 0.2656, 0.2691, 0.2761, 0.2884, 0.2903, 0.2974, 0.3006]
  val_loss : [2.7266, 2.6605, 2.7296, 5.0681, 6.1308, 10.6118, 4.1201, 7.8429]
  val_policy_loss : [2.6273, 2.5612, 2.6335, 4.9654, 6.0290, 10.4982, 4.0245, 7.7376]
  val_policy_policy_acc : [0.2664, 0.2713, 0.2731, 0.2741, 0.2814, 0.2797, 0.2809, 0.2806]
  val_value_loss : [0.2020, 0.2013, 0.1953, 0.1959, 0.1885, 0.1883, 0.1861, 0.1862]
  val_value_value_mse : [0.2021, 0.2012, 0.1953, 0.1959, 0.1885, 0.1882, 0.1861, 0.1862]
  value_loss : [0.2101, 0.2073, 0.2052, 0.2028, 0.1993, 0.1970, 0.1953, 0.1947]
  value_value_mse : [0.2101, 0.2073, 0.2052, 0.2028, 0.1993, 0.1970, 0.1953, 0.1947]

================================================================================

History file: model_versions/chess_elo_model_V70_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:45:54.948910Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game103501_game105000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V69
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.479865   (epoch 10)   mode=min
  policy_loss: 2.379621   (epoch 10)   mode=min
  policy_policy_acc: 0.303788   (epoch 10)   mode=max
  val_loss: 2.617577   (epoch 6)   mode=min
  val_policy_loss: 2.517779   (epoch 6)   mode=min
  val_policy_policy_acc: 0.280253   (epoch 10)   mode=max
  val_value_loss: 0.195728   (epoch 9)   mode=min
  val_value_value_mse: 0.195732   (epoch 9)   mode=min
  value_loss: 0.200391   (epoch 10)   mode=min
  value_value_mse: 0.200401   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7296, 2.6879, 2.6587, 2.5977, 2.5768, 2.5579, 2.5406, 2.5249, 2.4904, 2.4799]
  policy_loss : [2.6217, 2.5810, 2.5529, 2.4937, 2.4737, 2.4550, 2.4387, 2.4236, 2.3897, 2.3796]
  policy_policy_acc : [0.2570, 0.2638, 0.2702, 0.2815, 0.2859, 0.2896, 0.2932, 0.2949, 0.3020, 0.3038]
  val_loss : [2.6518, 2.6546, 2.6766, 2.6423, 2.6327, 2.6176, 2.6862, 2.7873, 2.6999, 2.6754]
  val_policy_loss : [2.5476, 2.5513, 2.5749, 2.5418, 2.5331, 2.5178, 2.5868, 2.6886, 2.6024, 2.5772]
  val_policy_policy_acc : [0.2742, 0.2695, 0.2719, 0.2754, 0.2756, 0.2782, 0.2769, 0.2756, 0.2790, 0.2803]
  val_value_loss : [0.2094, 0.2074, 0.2039, 0.2019, 0.2002, 0.2005, 0.1994, 0.1977, 0.1957, 0.1970]
  val_value_value_mse : [0.2094, 0.2074, 0.2039, 0.2019, 0.2002, 0.2006, 0.1994, 0.1977, 0.1957, 0.1971]
  value_loss : [0.2158, 0.2136, 0.2113, 0.2077, 0.2062, 0.2055, 0.2036, 0.2028, 0.2016, 0.2004]
  value_value_mse : [0.2158, 0.2136, 0.2113, 0.2077, 0.2062, 0.2055, 0.2036, 0.2028, 0.2016, 0.2004]

================================================================================

History file: model_versions/chess_elo_model_V71_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T04:54:17.402305Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game105001_game106500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V70
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.456772   (epoch 10)   mode=min
  policy_loss: 2.356856   (epoch 10)   mode=min
  policy_policy_acc: 0.306161   (epoch 10)   mode=max
  val_loss: 2.598360   (epoch 5)   mode=min
  val_policy_loss: 2.500522   (epoch 5)   mode=min
  val_policy_policy_acc: 0.288672   (epoch 10)   mode=max
  val_value_loss: 0.192626   (epoch 10)   mode=min
  val_value_value_mse: 0.192622   (epoch 10)   mode=min
  value_loss: 0.199848   (epoch 10)   mode=min
  value_value_mse: 0.199845   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7099, 2.6671, 2.6349, 2.5788, 2.5538, 2.5361, 2.5216, 2.4897, 2.4761, 2.4568]
  policy_loss : [2.6030, 2.5618, 2.5302, 2.4759, 2.4512, 2.4344, 2.4200, 2.3888, 2.3759, 2.3569]
  policy_policy_acc : [0.2563, 0.2657, 0.2726, 0.2843, 0.2869, 0.2918, 0.2957, 0.3013, 0.3048, 0.3062]
  val_loss : [2.6303, 2.6304, 2.6447, 2.6107, 2.5984, 2.6063, 2.6056, 2.5985, 2.6037, 2.6047]
  val_policy_loss : [2.5281, 2.5284, 2.5413, 2.5125, 2.5005, 2.5079, 2.5088, 2.5016, 2.5074, 2.5085]
  val_policy_policy_acc : [0.2798, 0.2792, 0.2822, 0.2810, 0.2867, 0.2842, 0.2846, 0.2865, 0.2867, 0.2887]
  val_value_loss : [0.2045, 0.2041, 0.2067, 0.1965, 0.1958, 0.1969, 0.1937, 0.1940, 0.1929, 0.1926]
  val_value_value_mse : [0.2045, 0.2040, 0.2067, 0.1965, 0.1958, 0.1969, 0.1937, 0.1939, 0.1929, 0.1926]
  value_loss : [0.2136, 0.2109, 0.2095, 0.2058, 0.2050, 0.2034, 0.2033, 0.2017, 0.2003, 0.1998]
  value_value_mse : [0.2136, 0.2109, 0.2095, 0.2058, 0.2050, 0.2034, 0.2033, 0.2017, 0.2003, 0.1998]

================================================================================

History file: model_versions/chess_elo_model_V72_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:02:40.691102Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game106501_game108000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V71
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.431991   (epoch 10)   mode=min
  policy_loss: 2.332545   (epoch 10)   mode=min
  policy_policy_acc: 0.311864   (epoch 10)   mode=max
  val_loss: 2.612882   (epoch 10)   mode=min
  val_policy_loss: 2.517068   (epoch 10)   mode=min
  val_policy_policy_acc: 0.283621   (epoch 9)   mode=max
  val_value_loss: 0.191612   (epoch 10)   mode=min
  val_value_value_mse: 0.191592   (epoch 10)   mode=min
  value_loss: 0.198768   (epoch 9)   mode=min
  value_value_mse: 0.198771   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6977, 2.6537, 2.6237, 2.5984, 2.5748, 2.5531, 2.5001, 2.4795, 2.4450, 2.4320]
  policy_loss : [2.5915, 2.5484, 2.5191, 2.4946, 2.4717, 2.4504, 2.3992, 2.3791, 2.3456, 2.3325]
  policy_policy_acc : [0.2615, 0.2700, 0.2743, 0.2788, 0.2837, 0.2881, 0.2977, 0.3010, 0.3099, 0.3119]
  val_loss : [2.6329, 2.6188, 2.6251, 2.6181, 2.6275, 2.6315, 2.6187, 2.6506, 2.6213, 2.6129]
  val_policy_loss : [2.5283, 2.5181, 2.5229, 2.5172, 2.5275, 2.5329, 2.5196, 2.5520, 2.5247, 2.5171]
  val_policy_policy_acc : [0.2721, 0.2743, 0.2751, 0.2774, 0.2731, 0.2758, 0.2752, 0.2808, 0.2836, 0.2816]
  val_value_loss : [0.2089, 0.2012, 0.2046, 0.2017, 0.1998, 0.1973, 0.1983, 0.1972, 0.1933, 0.1916]
  val_value_value_mse : [0.2089, 0.2012, 0.2046, 0.2017, 0.1998, 0.1973, 0.1982, 0.1971, 0.1933, 0.1916]
  value_loss : [0.2125, 0.2106, 0.2093, 0.2076, 0.2064, 0.2053, 0.2017, 0.2008, 0.1988, 0.1989]
  value_value_mse : [0.2125, 0.2106, 0.2093, 0.2076, 0.2064, 0.2053, 0.2017, 0.2008, 0.1988, 0.1989]

================================================================================

History file: model_versions/chess_elo_model_V73_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:10:59.911872Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game108001_game109500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V72
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451306   (epoch 10)   mode=min
  policy_loss: 2.351252   (epoch 10)   mode=min
  policy_policy_acc: 0.306778   (epoch 10)   mode=max
  val_loss: 2.622066   (epoch 5)   mode=min
  val_policy_loss: 2.522138   (epoch 5)   mode=min
  val_policy_policy_acc: 0.283170   (epoch 8)   mode=max
  val_value_loss: 0.192722   (epoch 10)   mode=min
  val_value_value_mse: 0.192696   (epoch 10)   mode=min
  value_loss: 0.200078   (epoch 10)   mode=min
  value_value_mse: 0.200084   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7169, 2.6701, 2.6395, 2.6119, 2.5524, 2.5311, 2.5135, 2.4845, 2.4669, 2.4513]
  policy_loss : [2.6093, 2.5641, 2.5339, 2.5072, 2.4494, 2.4289, 2.4119, 2.3838, 2.3662, 2.3513]
  policy_policy_acc : [0.2561, 0.2643, 0.2713, 0.2750, 0.2876, 0.2900, 0.2926, 0.2987, 0.3014, 0.3068]
  val_loss : [2.6604, 2.6418, 2.6564, 2.6476, 2.6221, 2.6311, 2.6514, 2.6320, 2.6338, 2.6366]
  val_policy_loss : [2.5569, 2.5381, 2.5542, 2.5468, 2.5221, 2.5322, 2.5524, 2.5333, 2.5362, 2.5392]
  val_policy_policy_acc : [0.2661, 0.2780, 0.2689, 0.2715, 0.2751, 0.2754, 0.2747, 0.2832, 0.2797, 0.2807]
  val_value_loss : [0.2059, 0.2067, 0.2030, 0.2013, 0.1982, 0.1955, 0.1964, 0.1955, 0.1931, 0.1927]
  val_value_value_mse : [0.2059, 0.2066, 0.2030, 0.2013, 0.1982, 0.1955, 0.1964, 0.1955, 0.1931, 0.1927]
  value_loss : [0.2152, 0.2121, 0.2110, 0.2094, 0.2059, 0.2044, 0.2031, 0.2016, 0.2010, 0.2001]
  value_value_mse : [0.2152, 0.2121, 0.2110, 0.2094, 0.2059, 0.2044, 0.2031, 0.2016, 0.2010, 0.2001]

================================================================================

History file: model_versions/chess_elo_model_V74_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:19:30.185258Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game109501_game111000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V73
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.447999   (epoch 10)   mode=min
  policy_loss: 2.348881   (epoch 10)   mode=min
  policy_policy_acc: 0.308351   (epoch 10)   mode=max
  val_loss: 2.639551   (epoch 5)   mode=min
  val_policy_loss: 2.539530   (epoch 5)   mode=min
  val_policy_policy_acc: 0.278593   (epoch 9)   mode=max
  val_value_loss: 0.191821   (epoch 10)   mode=min
  val_value_value_mse: 0.191635   (epoch 10)   mode=min
  value_loss: 0.198277   (epoch 10)   mode=min
  value_value_mse: 0.198279   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7062, 2.6646, 2.6309, 2.6057, 2.5490, 2.5277, 2.5101, 2.4749, 2.4643, 2.4480]
  policy_loss : [2.6001, 2.5596, 2.5269, 2.5026, 2.4474, 2.4265, 2.4096, 2.3752, 2.3651, 2.3489]
  policy_policy_acc : [0.2600, 0.2680, 0.2755, 0.2793, 0.2904, 0.2940, 0.2978, 0.3039, 0.3056, 0.3084]
  val_loss : [2.7077, 2.6529, 2.6835, 2.6691, 2.6396, 2.6519, 2.6473, 2.6426, 2.6663, 2.7009]
  val_policy_loss : [2.6037, 2.5509, 2.5822, 2.5644, 2.5395, 2.5533, 2.5492, 2.5455, 2.5687, 2.6037]
  val_policy_policy_acc : [0.2636, 0.2779, 0.2722, 0.2688, 0.2751, 0.2746, 0.2732, 0.2776, 0.2786, 0.2775]
  val_value_loss : [0.2065, 0.2021, 0.2005, 0.2067, 0.1976, 0.1952, 0.1936, 0.1919, 0.1923, 0.1918]
  val_value_value_mse : [0.2064, 0.2019, 0.2002, 0.2065, 0.1974, 0.1950, 0.1934, 0.1916, 0.1921, 0.1916]
  value_loss : [0.2122, 0.2100, 0.2079, 0.2063, 0.2031, 0.2023, 0.2009, 0.1993, 0.1983, 0.1983]
  value_value_mse : [0.2122, 0.2100, 0.2079, 0.2062, 0.2031, 0.2023, 0.2009, 0.1993, 0.1983, 0.1983]

================================================================================

History file: model_versions/chess_elo_model_V75_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:28:02.059912Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game111001_game112500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V74
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.436716   (epoch 10)   mode=min
  policy_loss: 2.339750   (epoch 10)   mode=min
  policy_policy_acc: 0.309076   (epoch 10)   mode=max
  val_loss: 2.812432   (epoch 4)   mode=min
  val_policy_loss: 2.717875   (epoch 4)   mode=min
  val_policy_policy_acc: 0.276970   (epoch 10)   mode=max
  val_value_loss: 0.186100   (epoch 10)   mode=min
  val_value_value_mse: 0.185859   (epoch 10)   mode=min
  value_loss: 0.193981   (epoch 10)   mode=min
  value_value_mse: 0.193964   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7042, 2.6598, 2.6307, 2.6056, 2.5804, 2.5583, 2.4989, 2.4826, 2.4482, 2.4367]
  policy_loss : [2.5997, 2.5556, 2.5279, 2.5034, 2.4791, 2.4575, 2.4003, 2.3848, 2.3508, 2.3397]
  policy_policy_acc : [0.2571, 0.2684, 0.2719, 0.2771, 0.2820, 0.2867, 0.2978, 0.3006, 0.3061, 0.3091]
  val_loss : [2.9205, 2.9218, 2.9213, 2.8124, 2.9942, 3.1037, 3.4844, 3.2518, 3.3354, 3.4974]
  val_policy_loss : [2.8225, 2.8223, 2.8226, 2.7179, 2.8990, 3.0059, 3.3902, 3.1580, 3.2418, 3.4035]
  val_policy_policy_acc : [0.2677, 0.2623, 0.2659, 0.2698, 0.2621, 0.2637, 0.2698, 0.2724, 0.2727, 0.2770]
  val_value_loss : [0.1989, 0.2018, 0.2001, 0.1936, 0.1925, 0.1956, 0.1881, 0.1881, 0.1872, 0.1861]
  val_value_value_mse : [0.1988, 0.2017, 0.1999, 0.1934, 0.1923, 0.1954, 0.1879, 0.1879, 0.1870, 0.1859]
  value_loss : [0.2090, 0.2084, 0.2056, 0.2043, 0.2027, 0.2014, 0.1973, 0.1957, 0.1949, 0.1940]
  value_value_mse : [0.2090, 0.2084, 0.2056, 0.2043, 0.2027, 0.2014, 0.1973, 0.1957, 0.1949, 0.1940]

================================================================================

History file: model_versions/chess_elo_model_V76_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:36:11.079004Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game112501_game114000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V75
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.452523   (epoch 10)   mode=min
  policy_loss: 2.357748   (epoch 10)   mode=min
  policy_policy_acc: 0.305472   (epoch 10)   mode=max
  val_loss: 2.612805   (epoch 5)   mode=min
  val_policy_loss: 2.519905   (epoch 5)   mode=min
  val_policy_policy_acc: 0.280562   (epoch 10)   mode=max
  val_value_loss: 0.181734   (epoch 10)   mode=min
  val_value_value_mse: 0.181717   (epoch 10)   mode=min
  value_loss: 0.189454   (epoch 10)   mode=min
  value_value_mse: 0.189463   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7143, 2.6646, 2.6327, 2.6075, 2.5513, 2.5302, 2.5102, 2.4821, 2.4719, 2.4525]
  policy_loss : [2.6112, 2.5627, 2.5323, 2.5076, 2.4537, 2.4329, 2.4136, 2.3864, 2.3764, 2.3577]
  policy_policy_acc : [0.2568, 0.2639, 0.2710, 0.2757, 0.2890, 0.2906, 0.2950, 0.2984, 0.3008, 0.3055]
  val_loss : [2.6699, 2.6481, 2.6524, 2.6568, 2.6128, 2.6259, 2.6255, 2.6196, 2.6260, 2.6343]
  val_policy_loss : [2.5701, 2.5532, 2.5580, 2.5619, 2.5199, 2.5339, 2.5334, 2.5277, 2.5348, 2.5435]
  val_policy_policy_acc : [0.2616, 0.2735, 0.2685, 0.2713, 0.2734, 0.2735, 0.2775, 0.2798, 0.2791, 0.2806]
  val_value_loss : [0.1998, 0.1899, 0.1889, 0.1898, 0.1859, 0.1842, 0.1844, 0.1838, 0.1826, 0.1817]
  val_value_value_mse : [0.1999, 0.1899, 0.1889, 0.1898, 0.1859, 0.1842, 0.1844, 0.1838, 0.1826, 0.1817]
  value_loss : [0.2059, 0.2038, 0.2009, 0.1998, 0.1953, 0.1945, 0.1932, 0.1914, 0.1910, 0.1895]
  value_value_mse : [0.2059, 0.2038, 0.2009, 0.1998, 0.1953, 0.1945, 0.1932, 0.1914, 0.1910, 0.1895]

================================================================================

History file: model_versions/chess_elo_model_V77_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:44:23.005365Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game114001_game115500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V76
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.442878   (epoch 10)   mode=min
  policy_loss: 2.347045   (epoch 10)   mode=min
  policy_policy_acc: 0.308337   (epoch 10)   mode=max
  val_loss: 2.604968   (epoch 7)   mode=min
  val_policy_loss: 2.509892   (epoch 7)   mode=min
  val_policy_policy_acc: 0.285332   (epoch 7)   mode=max
  val_value_loss: 0.187184   (epoch 10)   mode=min
  val_value_value_mse: 0.187194   (epoch 10)   mode=min
  value_loss: 0.192938   (epoch 10)   mode=min
  value_value_mse: 0.192960   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7022, 2.6594, 2.6297, 2.6035, 2.5794, 2.5245, 2.5056, 2.4911, 2.4756, 2.4429]
  policy_loss : [2.5976, 2.5563, 2.5276, 2.5023, 2.4782, 2.4260, 2.4075, 2.3934, 2.3780, 2.3470]
  policy_policy_acc : [0.2610, 0.2689, 0.2744, 0.2789, 0.2831, 0.2947, 0.2968, 0.2992, 0.3023, 0.3083]
  val_loss : [2.6418, 2.6260, 2.6214, 2.6234, 2.6208, 2.6080, 2.6050, 2.6090, 2.6218, 2.6090]
  val_policy_loss : [2.5423, 2.5264, 2.5205, 2.5263, 2.5230, 2.5126, 2.5099, 2.5148, 2.5274, 2.5152]
  val_policy_policy_acc : [0.2711, 0.2757, 0.2793, 0.2773, 0.2806, 0.2831, 0.2853, 0.2834, 0.2805, 0.2825]
  val_value_loss : [0.1986, 0.1990, 0.2014, 0.1940, 0.1951, 0.1905, 0.1899, 0.1881, 0.1882, 0.1872]
  val_value_value_mse : [0.1986, 0.1990, 0.2014, 0.1940, 0.1952, 0.1905, 0.1899, 0.1881, 0.1882, 0.1872]
  value_loss : [0.2089, 0.2066, 0.2044, 0.2028, 0.2015, 0.1978, 0.1964, 0.1957, 0.1952, 0.1929]
  value_value_mse : [0.2088, 0.2066, 0.2044, 0.2028, 0.2015, 0.1978, 0.1964, 0.1957, 0.1952, 0.1930]

================================================================================

History file: model_versions/chess_elo_model_V78_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T05:52:40.318855Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game115501_game117000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V77
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.487846   (epoch 10)   mode=min
  policy_loss: 2.390001   (epoch 10)   mode=min
  policy_policy_acc: 0.299764   (epoch 10)   mode=max
  val_loss: 2.708443   (epoch 6)   mode=min
  val_policy_loss: 2.612723   (epoch 6)   mode=min
  val_policy_policy_acc: 0.278776   (epoch 9)   mode=max
  val_value_loss: 0.190673   (epoch 10)   mode=min
  val_value_value_mse: 0.190684   (epoch 10)   mode=min
  value_loss: 0.195711   (epoch 10)   mode=min
  value_value_mse: 0.195709   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7180, 2.6730, 2.6442, 2.5891, 2.5657, 2.5343, 2.5186, 2.5124, 2.4940, 2.4878]
  policy_loss : [2.6144, 2.5704, 2.5421, 2.4888, 2.4663, 2.4356, 2.4200, 2.4143, 2.3961, 2.3900]
  policy_policy_acc : [0.2554, 0.2627, 0.2692, 0.2791, 0.2843, 0.2892, 0.2933, 0.2950, 0.2965, 0.2998]
  val_loss : [2.7441, 2.7569, 2.9819, 2.7637, 2.9336, 2.7084, 2.7447, 2.8738, 2.7252, 2.7103]
  val_policy_loss : [2.6444, 2.6590, 2.8833, 2.6665, 2.8373, 2.6127, 2.6490, 2.7780, 2.6300, 2.6150]
  val_policy_policy_acc : [0.2616, 0.2632, 0.2617, 0.2711, 0.2674, 0.2707, 0.2735, 0.2719, 0.2788, 0.2772]
  val_value_loss : [0.1995, 0.1962, 0.1966, 0.1941, 0.1921, 0.1917, 0.1911, 0.1914, 0.1908, 0.1907]
  val_value_value_mse : [0.1996, 0.1962, 0.1967, 0.1942, 0.1921, 0.1917, 0.1912, 0.1914, 0.1908, 0.1907]
  value_loss : [0.2073, 0.2051, 0.2043, 0.2006, 0.1989, 0.1975, 0.1971, 0.1962, 0.1960, 0.1957]
  value_value_mse : [0.2073, 0.2052, 0.2043, 0.2006, 0.1989, 0.1975, 0.1971, 0.1962, 0.1960, 0.1957]

================================================================================

History file: model_versions/chess_elo_model_V79_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:01:09.699951Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game117001_game118500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V78
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.468390   (epoch 10)   mode=min
  policy_loss: 2.373302   (epoch 10)   mode=min
  policy_policy_acc: 0.302531   (epoch 10)   mode=max
  val_loss: 2.592273   (epoch 9)   mode=min
  val_policy_loss: 2.500605   (epoch 9)   mode=min
  val_policy_policy_acc: 0.284868   (epoch 10)   mode=max
  val_value_loss: 0.186420   (epoch 10)   mode=min
  val_value_value_mse: 0.186318   (epoch 10)   mode=min
  value_loss: 0.191389   (epoch 10)   mode=min
  value_value_mse: 0.191397   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7080, 2.6703, 2.6431, 2.6204, 2.5597, 2.5400, 2.5253, 2.5101, 2.4787, 2.4684]
  policy_loss : [2.6037, 2.5678, 2.5413, 2.5192, 2.4607, 2.4415, 2.4273, 2.4132, 2.3826, 2.3733]
  policy_policy_acc : [0.2579, 0.2650, 0.2700, 0.2740, 0.2849, 0.2888, 0.2936, 0.2959, 0.3012, 0.3025]
  val_loss : [2.6493, 2.6292, 2.6343, 2.6443, 2.6024, 2.5943, 2.6236, 2.6025, 2.5923, 2.6051]
  val_policy_loss : [2.5499, 2.5294, 2.5376, 2.5444, 2.5085, 2.5017, 2.5308, 2.5107, 2.5006, 2.5136]
  val_policy_policy_acc : [0.2704, 0.2772, 0.2747, 0.2741, 0.2784, 0.2800, 0.2799, 0.2809, 0.2847, 0.2849]
  val_value_loss : [0.2014, 0.2018, 0.1961, 0.2019, 0.1900, 0.1884, 0.1889, 0.1866, 0.1866, 0.1864]
  val_value_value_mse : [0.2013, 0.2017, 0.1960, 0.2018, 0.1899, 0.1883, 0.1888, 0.1865, 0.1865, 0.1863]
  value_loss : [0.2088, 0.2056, 0.2037, 0.2020, 0.1984, 0.1965, 0.1959, 0.1938, 0.1925, 0.1914]
  value_value_mse : [0.2088, 0.2056, 0.2036, 0.2020, 0.1985, 0.1965, 0.1959, 0.1938, 0.1925, 0.1914]

================================================================================

History file: model_versions/chess_elo_model_V80_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:09:14.995491Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game118501_game120000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V79
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.445071   (epoch 10)   mode=min
  policy_loss: 2.347481   (epoch 10)   mode=min
  policy_policy_acc: 0.307353   (epoch 10)   mode=max
  val_loss: 2.649834   (epoch 7)   mode=min
  val_policy_loss: 2.554081   (epoch 7)   mode=min
  val_policy_policy_acc: 0.271803   (epoch 10)   mode=max
  val_value_loss: 0.186589   (epoch 10)   mode=min
  val_value_value_mse: 0.186519   (epoch 10)   mode=min
  value_loss: 0.195190   (epoch 10)   mode=min
  value_value_mse: 0.195190   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7179, 2.6756, 2.6456, 2.6184, 2.5931, 2.5725, 2.5163, 2.4967, 2.4790, 2.4451]
  policy_loss : [2.6120, 2.5714, 2.5422, 2.5157, 2.4916, 2.4717, 2.4171, 2.3979, 2.3807, 2.3475]
  policy_policy_acc : [0.2550, 0.2636, 0.2699, 0.2730, 0.2787, 0.2826, 0.2928, 0.2971, 0.3004, 0.3074]
  val_loss : [2.6675, 2.6599, 2.7528, 2.6565, 2.7451, 2.7300, 2.6498, 2.7178, 2.6888, 2.6725]
  val_policy_loss : [2.5664, 2.5605, 2.6507, 2.5590, 2.6459, 2.6332, 2.5541, 2.6224, 2.5945, 2.5794]
  val_policy_policy_acc : [0.2638, 0.2660, 0.2658, 0.2629, 0.2652, 0.2635, 0.2663, 0.2702, 0.2667, 0.2718]
  val_value_loss : [0.2005, 0.1969, 0.2026, 0.1936, 0.1966, 0.1915, 0.1904, 0.1888, 0.1919, 0.1866]
  val_value_value_mse : [0.2005, 0.1968, 0.2026, 0.1935, 0.1965, 0.1915, 0.1903, 0.1887, 0.1917, 0.1865]
  value_loss : [0.2117, 0.2085, 0.2067, 0.2054, 0.2029, 0.2016, 0.1985, 0.1977, 0.1967, 0.1952]
  value_value_mse : [0.2117, 0.2085, 0.2067, 0.2054, 0.2029, 0.2016, 0.1985, 0.1977, 0.1967, 0.1952]

================================================================================

History file: model_versions/chess_elo_model_V81_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:17:45.214530Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game120001_game121500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V80
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.474232   (epoch 10)   mode=min
  policy_loss: 2.377029   (epoch 10)   mode=min
  policy_policy_acc: 0.299272   (epoch 10)   mode=max
  val_loss: 2.622340   (epoch 8)   mode=min
  val_policy_loss: 2.528104   (epoch 8)   mode=min
  val_policy_policy_acc: 0.278680   (epoch 7)   mode=max
  val_value_loss: 0.187101   (epoch 10)   mode=min
  val_value_value_mse: 0.187081   (epoch 10)   mode=min
  value_loss: 0.194162   (epoch 10)   mode=min
  value_value_mse: 0.194163   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003]
  loss : [2.7222, 2.6755, 2.6426, 2.6219, 2.5644, 2.5425, 2.5311, 2.4944, 2.4851, 2.4742]
  policy_loss : [2.6178, 2.5723, 2.5412, 2.5208, 2.4649, 2.4436, 2.4322, 2.3966, 2.3874, 2.3770]
  policy_policy_acc : [0.2508, 0.2633, 0.2684, 0.2726, 0.2824, 0.2850, 0.2893, 0.2971, 0.2974, 0.2993]
  val_loss : [2.6706, 2.6583, 2.6550, 2.6575, 2.6304, 2.6584, 2.6337, 2.6223, 2.6282, 2.6702]
  val_policy_loss : [2.5703, 2.5568, 2.5578, 2.5591, 2.5345, 2.5637, 2.5389, 2.5281, 2.5345, 2.5765]
  val_policy_policy_acc : [0.2605, 0.2659, 0.2704, 0.2666, 0.2685, 0.2742, 0.2787, 0.2748, 0.2785, 0.2782]
  val_value_loss : [0.2009, 0.2032, 0.1942, 0.1972, 0.1921, 0.1893, 0.1893, 0.1882, 0.1872, 0.1871]
  val_value_value_mse : [0.2009, 0.2031, 0.1941, 0.1972, 0.1919, 0.1892, 0.1893, 0.1881, 0.1872, 0.1871]
  value_loss : [0.2086, 0.2056, 0.2038, 0.2028, 0.1987, 0.1981, 0.1973, 0.1957, 0.1953, 0.1942]
  value_value_mse : [0.2086, 0.2056, 0.2038, 0.2027, 0.1987, 0.1981, 0.1973, 0.1958, 0.1953, 0.1942]

================================================================================

History file: model_versions/chess_elo_model_V82_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:26:04.938945Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game121501_game123000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V81
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.481314   (epoch 10)   mode=min
  policy_loss: 2.383295   (epoch 10)   mode=min
  policy_policy_acc: 0.300367   (epoch 10)   mode=max
  val_loss: 2.607556   (epoch 10)   mode=min
  val_policy_loss: 2.512387   (epoch 10)   mode=min
  val_policy_policy_acc: 0.281508   (epoch 8)   mode=max
  val_value_loss: 0.190064   (epoch 9)   mode=min
  val_value_value_mse: 0.190054   (epoch 9)   mode=min
  value_loss: 0.195691   (epoch 9)   mode=min
  value_value_mse: 0.195690   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7234, 2.6802, 2.6519, 2.5925, 2.5710, 2.5534, 2.5366, 2.5253, 2.4930, 2.4813]
  policy_loss : [2.6179, 2.5764, 2.5488, 2.4908, 2.4704, 2.4535, 2.4375, 2.4261, 2.3954, 2.3833]
  policy_policy_acc : [0.2545, 0.2643, 0.2695, 0.2804, 0.2849, 0.2850, 0.2904, 0.2922, 0.2989, 0.3004]
  val_loss : [2.6374, 2.6449, 2.6399, 2.6162, 2.6138, 2.6094, 2.6156, 2.6302, 2.6102, 2.6076]
  val_policy_loss : [2.5354, 2.5414, 2.5407, 2.5186, 2.5175, 2.5131, 2.5203, 2.5351, 2.5152, 2.5124]
  val_policy_policy_acc : [0.2679, 0.2667, 0.2727, 0.2777, 0.2741, 0.2769, 0.2778, 0.2815, 0.2787, 0.2801]
  val_value_loss : [0.2039, 0.2070, 0.1983, 0.1952, 0.1927, 0.1925, 0.1905, 0.1901, 0.1901, 0.1903]
  val_value_value_mse : [0.2039, 0.2070, 0.1983, 0.1952, 0.1927, 0.1925, 0.1905, 0.1901, 0.1901, 0.1903]
  value_loss : [0.2109, 0.2080, 0.2059, 0.2029, 0.2007, 0.1993, 0.1984, 0.1982, 0.1957, 0.1959]
  value_value_mse : [0.2109, 0.2080, 0.2059, 0.2029, 0.2007, 0.1993, 0.1984, 0.1982, 0.1957, 0.1959]

================================================================================

History file: model_versions/chess_elo_model_V83_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:34:25.042455Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game123001_game124500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V82
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.466470   (epoch 10)   mode=min
  policy_loss: 2.370700   (epoch 10)   mode=min
  policy_policy_acc: 0.302702   (epoch 10)   mode=max
  val_loss: 2.607034   (epoch 10)   mode=min
  val_policy_loss: 2.515359   (epoch 10)   mode=min
  val_policy_policy_acc: 0.285957   (epoch 9)   mode=max
  val_value_loss: 0.181944   (epoch 9)   mode=min
  val_value_value_mse: 0.181314   (epoch 9)   mode=min
  value_loss: 0.191498   (epoch 10)   mode=min
  value_value_mse: 0.191497   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7047, 2.6593, 2.6307, 2.5793, 2.5538, 2.5350, 2.5030, 2.4905, 2.4747, 2.4665]
  policy_loss : [2.6023, 2.5583, 2.5303, 2.4811, 2.4559, 2.4376, 2.4065, 2.3946, 2.3789, 2.3707]
  policy_policy_acc : [0.2557, 0.2647, 0.2705, 0.2816, 0.2848, 0.2889, 0.2945, 0.2974, 0.3005, 0.3027]
  val_loss : [2.6344, 2.6360, 2.6337, 2.6115, 2.6168, 2.6366, 2.6212, 2.6182, 2.6186, 2.6070]
  val_policy_loss : [2.5361, 2.5404, 2.5390, 2.5185, 2.5229, 2.5440, 2.5295, 2.5267, 2.5271, 2.5154]
  val_policy_policy_acc : [0.2734, 0.2736, 0.2760, 0.2824, 0.2783, 0.2803, 0.2851, 0.2838, 0.2860, 0.2860]
  val_value_loss : [0.1974, 0.1912, 0.1913, 0.1865, 0.1873, 0.1845, 0.1830, 0.1826, 0.1819, 0.1826]
  val_value_value_mse : [0.1970, 0.1907, 0.1908, 0.1860, 0.1867, 0.1839, 0.1824, 0.1819, 0.1813, 0.1820]
  value_loss : [0.2047, 0.2022, 0.2008, 0.1963, 0.1958, 0.1948, 0.1929, 0.1918, 0.1916, 0.1915]
  value_value_mse : [0.2047, 0.2022, 0.2008, 0.1963, 0.1958, 0.1948, 0.1929, 0.1918, 0.1916, 0.1915]

================================================================================

History file: model_versions/chess_elo_model_V84_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:42:53.255398Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game124501_game126000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V83
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.454653   (epoch 10)   mode=min
  policy_loss: 2.355536   (epoch 10)   mode=min
  policy_policy_acc: 0.305925   (epoch 10)   mode=max
  val_loss: 2.598474   (epoch 7)   mode=min
  val_policy_loss: 2.498840   (epoch 7)   mode=min
  val_policy_policy_acc: 0.285072   (epoch 10)   mode=max
  val_value_loss: 0.194734   (epoch 10)   mode=min
  val_value_value_mse: 0.194745   (epoch 10)   mode=min
  value_loss: 0.198315   (epoch 10)   mode=min
  value_value_mse: 0.198311   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003]
  loss : [2.7102, 2.6728, 2.6405, 2.6176, 2.5948, 2.5793, 2.5202, 2.4960, 2.4852, 2.4547]
  policy_loss : [2.6041, 2.5680, 2.5359, 2.5141, 2.4920, 2.4769, 2.4199, 2.3958, 2.3855, 2.3555]
  policy_policy_acc : [0.2566, 0.2644, 0.2685, 0.2742, 0.2800, 0.2828, 0.2933, 0.2972, 0.2988, 0.3059]
  val_loss : [2.6304, 2.6242, 2.6316, 2.6195, 2.6417, 2.6539, 2.5985, 2.6145, 2.6109, 2.6012]
  val_policy_loss : [2.5243, 2.5215, 2.5284, 2.5184, 2.5384, 2.5509, 2.4988, 2.5160, 2.5121, 2.5032]
  val_policy_policy_acc : [0.2712, 0.2672, 0.2710, 0.2707, 0.2710, 0.2773, 0.2830, 0.2796, 0.2822, 0.2851]
  val_value_loss : [0.2111, 0.2046, 0.2053, 0.2009, 0.2055, 0.2048, 0.1982, 0.1959, 0.1965, 0.1947]
  val_value_value_mse : [0.2111, 0.2046, 0.2053, 0.2009, 0.2055, 0.2049, 0.1982, 0.1959, 0.1966, 0.1947]
  value_loss : [0.2122, 0.2097, 0.2090, 0.2071, 0.2058, 0.2048, 0.2006, 0.2005, 0.1993, 0.1983]
  value_value_mse : [0.2122, 0.2096, 0.2090, 0.2071, 0.2058, 0.2048, 0.2006, 0.2005, 0.1993, 0.1983]

================================================================================

History file: model_versions/chess_elo_model_V85_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:51:44.603800Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game126001_game127500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V84
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.501021   (epoch 10)   mode=min
  policy_loss: 2.403527   (epoch 10)   mode=min
  policy_policy_acc: 0.296603   (epoch 10)   mode=max
  val_loss: 2.624374   (epoch 6)   mode=min
  val_policy_loss: 2.529091   (epoch 6)   mode=min
  val_policy_policy_acc: 0.280900   (epoch 9)   mode=max
  val_value_loss: 0.187839   (epoch 9)   mode=min
  val_value_value_mse: 0.187853   (epoch 9)   mode=min
  value_loss: 0.194894   (epoch 10)   mode=min
  value_value_mse: 0.194897   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7277, 2.6895, 2.6574, 2.6000, 2.5803, 2.5478, 2.5310, 2.5254, 2.5068, 2.5010]
  policy_loss : [2.6231, 2.5861, 2.5551, 2.4995, 2.4804, 2.4490, 2.4324, 2.4271, 2.4089, 2.4035]
  policy_policy_acc : [0.2539, 0.2618, 0.2666, 0.2780, 0.2828, 0.2883, 0.2920, 0.2931, 0.2961, 0.2966]
  val_loss : [2.6546, 2.6705, 2.8561, 2.6624, 2.7933, 2.6244, 2.6513, 2.6485, 2.6803, 2.6290]
  val_policy_loss : [2.5535, 2.5714, 2.7573, 2.5652, 2.6963, 2.5291, 2.5560, 2.5535, 2.5860, 2.5346]
  val_policy_policy_acc : [0.2694, 0.2667, 0.2664, 0.2709, 0.2726, 0.2761, 0.2775, 0.2779, 0.2809, 0.2800]
  val_value_loss : [0.2006, 0.1971, 0.1962, 0.1934, 0.1928, 0.1896, 0.1898, 0.1891, 0.1878, 0.1880]
  val_value_value_mse : [0.2005, 0.1972, 0.1961, 0.1934, 0.1928, 0.1896, 0.1898, 0.1891, 0.1879, 0.1880]
  value_loss : [0.2090, 0.2067, 0.2044, 0.2011, 0.1997, 0.1976, 0.1971, 0.1967, 0.1960, 0.1949]
  value_value_mse : [0.2090, 0.2067, 0.2044, 0.2011, 0.1997, 0.1976, 0.1971, 0.1967, 0.1960, 0.1949]

================================================================================

History file: model_versions/chess_elo_model_V86_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T06:59:21.259893Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game127501_game129000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V85
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.484577   (epoch 9)   mode=min
  policy_loss: 2.385939   (epoch 9)   mode=min
  policy_policy_acc: 0.300372   (epoch 9)   mode=max
  val_loss: 2.628089   (epoch 3)   mode=min
  val_policy_loss: 2.530941   (epoch 3)   mode=min
  val_policy_policy_acc: 0.276228   (epoch 8)   mode=max
  val_value_loss: 0.190986   (epoch 6)   mode=min
  val_value_value_mse: 0.190961   (epoch 6)   mode=min
  value_loss: 0.197481   (epoch 9)   mode=min
  value_value_mse: 0.197485   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7127, 2.6762, 2.6494, 2.6242, 2.5987, 2.5492, 2.5243, 2.4968, 2.4846]
  policy_loss : [2.6071, 2.5717, 2.5459, 2.5209, 2.4964, 2.4488, 2.4241, 2.3977, 2.3859]
  policy_policy_acc : [0.2576, 0.2636, 0.2669, 0.2740, 0.2796, 0.2884, 0.2915, 0.2993, 0.3004]
  val_loss : [2.6675, 2.6622, 2.6281, 2.6530, 2.6391, 2.6291, 2.6936, 2.6641, 2.6314]
  val_policy_loss : [2.5642, 2.5599, 2.5309, 2.5540, 2.5391, 2.5336, 2.5978, 2.5683, 2.5356]
  val_policy_policy_acc : [0.2630, 0.2636, 0.2694, 0.2640, 0.2701, 0.2738, 0.2717, 0.2762, 0.2759]
  val_value_loss : [0.2067, 0.2048, 0.1944, 0.1980, 0.2000, 0.1910, 0.1916, 0.1916, 0.1917]
  val_value_value_mse : [0.2067, 0.2048, 0.1943, 0.1980, 0.1999, 0.1910, 0.1915, 0.1915, 0.1917]
  value_loss : [0.2111, 0.2087, 0.2072, 0.2064, 0.2045, 0.2011, 0.2004, 0.1981, 0.1975]
  value_value_mse : [0.2111, 0.2087, 0.2072, 0.2065, 0.2045, 0.2011, 0.2004, 0.1981, 0.1975]

================================================================================

History file: model_versions/chess_elo_model_V87_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:07:39.385608Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game129001_game130500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V86
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.478542   (epoch 10)   mode=min
  policy_loss: 2.383931   (epoch 10)   mode=min
  policy_policy_acc: 0.302531   (epoch 10)   mode=max
  val_loss: 2.612602   (epoch 4)   mode=min
  val_policy_loss: 2.520290   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279859   (epoch 10)   mode=max
  val_value_loss: 0.179386   (epoch 10)   mode=min
  val_value_value_mse: 0.179364   (epoch 10)   mode=min
  value_loss: 0.189296   (epoch 10)   mode=min
  value_value_mse: 0.189301   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7065, 2.6653, 2.6333, 2.5801, 2.5573, 2.5433, 2.5103, 2.5000, 2.4837, 2.4785]
  policy_loss : [2.6045, 2.5649, 2.5336, 2.4824, 2.4602, 2.4468, 2.4147, 2.4047, 2.3889, 2.3839]
  policy_policy_acc : [0.2597, 0.2672, 0.2733, 0.2845, 0.2890, 0.2913, 0.2978, 0.2996, 0.3025, 0.3025]
  val_loss : [2.6582, 2.8419, 2.6737, 2.6126, 2.8596, 2.7067, 2.6155, 2.8355, 2.7781, 2.7770]
  val_policy_loss : [2.5623, 2.7466, 2.5767, 2.5203, 2.7666, 2.6147, 2.5249, 2.7445, 2.6878, 2.6867]
  val_policy_policy_acc : [0.2624, 0.2620, 0.2692, 0.2750, 0.2737, 0.2780, 0.2784, 0.2791, 0.2775, 0.2799]
  val_value_loss : [0.1923, 0.1902, 0.1943, 0.1848, 0.1852, 0.1832, 0.1811, 0.1809, 0.1796, 0.1794]
  val_value_value_mse : [0.1923, 0.1903, 0.1943, 0.1848, 0.1851, 0.1833, 0.1811, 0.1809, 0.1796, 0.1794]
  value_loss : [0.2040, 0.2007, 0.1993, 0.1955, 0.1941, 0.1932, 0.1911, 0.1904, 0.1897, 0.1893]
  value_value_mse : [0.2040, 0.2007, 0.1993, 0.1955, 0.1941, 0.1932, 0.1911, 0.1904, 0.1897, 0.1893]

================================================================================

History file: model_versions/chess_elo_model_V88_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:15:24.941365Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game130501_game132000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V87
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.475976   (epoch 9)   mode=min
  policy_loss: 2.383019   (epoch 9)   mode=min
  policy_policy_acc: 0.300958   (epoch 9)   mode=max
  val_loss: 2.639095   (epoch 3)   mode=min
  val_policy_loss: 2.546316   (epoch 3)   mode=min
  val_policy_policy_acc: 0.280432   (epoch 9)   mode=max
  val_value_loss: 0.177709   (epoch 8)   mode=min
  val_value_value_mse: 0.177514   (epoch 8)   mode=min
  value_loss: 0.185736   (epoch 9)   mode=min
  value_value_mse: 0.185750   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7012, 2.6622, 2.6371, 2.6130, 2.5938, 2.5381, 2.5186, 2.4876, 2.4760]
  policy_loss : [2.6005, 2.5630, 2.5383, 2.5155, 2.4968, 2.4432, 2.4247, 2.3946, 2.3830]
  policy_policy_acc : [0.2601, 0.2667, 0.2705, 0.2774, 0.2782, 0.2886, 0.2944, 0.3004, 0.3010]
  val_loss : [2.6971, 2.7191, 2.6391, 2.7563, 2.7766, 3.2394, 3.5565, 4.0359, 4.0193]
  val_policy_loss : [2.6024, 2.6262, 2.5463, 2.6654, 2.6856, 3.1495, 3.4648, 3.9445, 3.9269]
  val_policy_policy_acc : [0.2654, 0.2726, 0.2709, 0.2699, 0.2707, 0.2756, 0.2780, 0.2780, 0.2804]
  val_value_loss : [0.1939, 0.1890, 0.1904, 0.1848, 0.1838, 0.1795, 0.1823, 0.1777, 0.1792]
  val_value_value_mse : [0.1939, 0.1889, 0.1901, 0.1848, 0.1837, 0.1794, 0.1821, 0.1775, 0.1791]
  value_loss : [0.2015, 0.1983, 0.1972, 0.1951, 0.1938, 0.1894, 0.1882, 0.1859, 0.1857]
  value_value_mse : [0.2015, 0.1983, 0.1972, 0.1951, 0.1938, 0.1895, 0.1882, 0.1859, 0.1858]

================================================================================

History file: model_versions/chess_elo_model_V89_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:23:08.278067Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game132001_game133500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V88
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.491671   (epoch 9)   mode=min
  policy_loss: 2.393554   (epoch 9)   mode=min
  policy_policy_acc: 0.301158   (epoch 9)   mode=max
  val_loss: 2.625412   (epoch 4)   mode=min
  val_policy_loss: 2.530401   (epoch 3)   mode=min
  val_policy_policy_acc: 0.287008   (epoch 9)   mode=max
  val_value_loss: 0.187598   (epoch 9)   mode=min
  val_value_value_mse: 0.187753   (epoch 9)   mode=min
  value_loss: 0.196236   (epoch 9)   mode=min
  value_value_mse: 0.196236   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7182, 2.6786, 2.6533, 2.6275, 2.6068, 2.5514, 2.5324, 2.5034, 2.4917]
  policy_loss : [2.6127, 2.5743, 2.5499, 2.5252, 2.5051, 2.4512, 2.4329, 2.4052, 2.3936]
  policy_policy_acc : [0.2557, 0.2642, 0.2674, 0.2728, 0.2770, 0.2876, 0.2909, 0.2952, 0.3012]
  val_loss : [2.6510, 2.6833, 2.6262, 2.6254, 2.6809, 2.6743, 2.7420, 2.6912, 2.7600]
  val_policy_loss : [2.5540, 2.5862, 2.5304, 2.5308, 2.5872, 2.5827, 2.6512, 2.6010, 2.6701]
  val_policy_policy_acc : [0.2800, 0.2800, 0.2799, 0.2780, 0.2794, 0.2843, 0.2837, 0.2854, 0.2870]
  val_value_loss : [0.1990, 0.2009, 0.1973, 0.1951, 0.1946, 0.1908, 0.1908, 0.1884, 0.1876]
  val_value_value_mse : [0.1991, 0.2011, 0.1975, 0.1952, 0.1947, 0.1908, 0.1910, 0.1885, 0.1878]
  value_loss : [0.2110, 0.2086, 0.2069, 0.2047, 0.2034, 0.2003, 0.1989, 0.1963, 0.1962]
  value_value_mse : [0.2110, 0.2086, 0.2069, 0.2047, 0.2034, 0.2003, 0.1989, 0.1963, 0.1962]

================================================================================

History file: model_versions/chess_elo_model_V90_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:29:00.161787Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game133501_game135000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V89
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.533091   (epoch 7)   mode=min
  policy_loss: 2.434469   (epoch 7)   mode=min
  policy_policy_acc: 0.290926   (epoch 7)   mode=max
  val_loss: 2.658165   (epoch 1)   mode=min
  val_policy_loss: 2.557057   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290248   (epoch 6)   mode=max
  val_value_loss: 0.188812   (epoch 7)   mode=min
  val_value_value_mse: 0.188615   (epoch 7)   mode=min
  value_loss: 0.197520   (epoch 7)   mode=min
  value_value_mse: 0.197520   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7198, 2.6795, 2.6542, 2.5979, 2.5752, 2.5456, 2.5331]
  policy_loss : [2.6141, 2.5758, 2.5512, 2.4970, 2.4752, 2.4462, 2.4345]
  policy_policy_acc : [0.2554, 0.2633, 0.2684, 0.2785, 0.2822, 0.2882, 0.2909]
  val_loss : [2.6582, 2.7880, 2.7978, 2.8165, 3.0340, 2.6835, 2.7219]
  val_policy_loss : [2.5571, 2.6904, 2.7019, 2.7216, 2.9387, 2.5903, 2.6285]
  val_policy_policy_acc : [0.2728, 0.2764, 0.2778, 0.2862, 0.2900, 0.2902, 0.2893]
  val_value_loss : [0.2027, 0.1972, 0.1952, 0.1929, 0.1919, 0.1902, 0.1888]
  val_value_value_mse : [0.2024, 0.1970, 0.1951, 0.1926, 0.1916, 0.1900, 0.1886]
  value_loss : [0.2111, 0.2076, 0.2061, 0.2018, 0.2003, 0.1984, 0.1975]
  value_value_mse : [0.2111, 0.2076, 0.2061, 0.2018, 0.2003, 0.1984, 0.1975]

================================================================================

History file: model_versions/chess_elo_model_V91_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:35:59.373189Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game135001_game136500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V90
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.509575   (epoch 8)   mode=min
  policy_loss: 2.418137   (epoch 8)   mode=min
  policy_policy_acc: 0.294097   (epoch 8)   mode=max
  val_loss: 2.640227   (epoch 2)   mode=min
  val_policy_loss: 2.548858   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285771   (epoch 7)   mode=max
  val_value_loss: 0.172890   (epoch 7)   mode=min
  val_value_value_mse: 0.172675   (epoch 7)   mode=min
  value_loss: 0.182947   (epoch 8)   mode=min
  value_value_mse: 0.182944   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7143, 2.6819, 2.6513, 2.6258, 2.5718, 2.5536, 2.5221, 2.5096]
  policy_loss : [2.6145, 2.5838, 2.5542, 2.5300, 2.4775, 2.4605, 2.4301, 2.4181]
  policy_policy_acc : [0.2558, 0.2627, 0.2676, 0.2722, 0.2824, 0.2864, 0.2929, 0.2941]
  val_loss : [2.6542, 2.6402, 2.6424, 2.6506, 3.1822, 2.9395, 2.8080, 2.6564]
  val_policy_loss : [2.5623, 2.5489, 2.5543, 2.5626, 3.0932, 2.8535, 2.7231, 2.5711]
  val_policy_policy_acc : [0.2717, 0.2751, 0.2726, 0.2767, 0.2807, 0.2810, 0.2858, 0.2845]
  val_value_loss : [0.1883, 0.1856, 0.1791, 0.1815, 0.1792, 0.1748, 0.1729, 0.1746]
  val_value_value_mse : [0.1881, 0.1854, 0.1790, 0.1814, 0.1792, 0.1747, 0.1727, 0.1746]
  value_loss : [0.1996, 0.1962, 0.1942, 0.1915, 0.1886, 0.1863, 0.1839, 0.1829]
  value_value_mse : [0.1996, 0.1962, 0.1942, 0.1915, 0.1886, 0.1863, 0.1839, 0.1829]

================================================================================

History file: model_versions/chess_elo_model_V92_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:43:27.288238Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game136501_game138000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V91
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.481788   (epoch 9)   mode=min
  policy_loss: 2.382360   (epoch 9)   mode=min
  policy_policy_acc: 0.303471   (epoch 9)   mode=max
  val_loss: 2.651571   (epoch 3)   mode=min
  val_policy_loss: 2.548442   (epoch 3)   mode=min
  val_policy_policy_acc: 0.280028   (epoch 9)   mode=max
  val_value_loss: 0.192893   (epoch 9)   mode=min
  val_value_value_mse: 0.192866   (epoch 9)   mode=min
  value_loss: 0.198845   (epoch 9)   mode=min
  value_value_mse: 0.198846   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7095, 2.6704, 2.6438, 2.6188, 2.5995, 2.5450, 2.5265, 2.4913, 2.4818]
  policy_loss : [2.6028, 2.5654, 2.5392, 2.5155, 2.4965, 2.4440, 2.4256, 2.3916, 2.3824]
  policy_policy_acc : [0.2606, 0.2663, 0.2729, 0.2770, 0.2815, 0.2907, 0.2945, 0.3009, 0.3035]
  val_loss : [2.6845, 2.6862, 2.6516, 2.6665, 2.6969, 2.6889, 2.7830, 2.7972, 2.7885]
  val_policy_loss : [2.5766, 2.5848, 2.5484, 2.5628, 2.5942, 2.5888, 2.6843, 2.6990, 2.6907]
  val_policy_policy_acc : [0.2700, 0.2651, 0.2774, 0.2711, 0.2719, 0.2764, 0.2785, 0.2775, 0.2800]
  val_value_loss : [0.2138, 0.2006, 0.2035, 0.2059, 0.2031, 0.1979, 0.1949, 0.1936, 0.1929]
  val_value_value_mse : [0.2138, 0.2006, 0.2034, 0.2058, 0.2031, 0.1979, 0.1949, 0.1936, 0.1929]
  value_loss : [0.2133, 0.2102, 0.2093, 0.2066, 0.2061, 0.2021, 0.2019, 0.1994, 0.1988]
  value_value_mse : [0.2133, 0.2102, 0.2093, 0.2066, 0.2061, 0.2021, 0.2019, 0.1994, 0.1988]

================================================================================

History file: model_versions/chess_elo_model_V93_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:51:06.257365Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game138001_game139500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V92
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.483921   (epoch 9)   mode=min
  policy_loss: 2.387407   (epoch 9)   mode=min
  policy_policy_acc: 0.301233   (epoch 9)   mode=max
  val_loss: 2.653514   (epoch 3)   mode=min
  val_policy_loss: 2.554175   (epoch 3)   mode=min
  val_policy_policy_acc: 0.282562   (epoch 9)   mode=max
  val_value_loss: 0.185387   (epoch 9)   mode=min
  val_value_value_mse: 0.185423   (epoch 9)   mode=min
  value_loss: 0.192970   (epoch 9)   mode=min
  value_value_mse: 0.192968   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7029, 2.6695, 2.6418, 2.6183, 2.5986, 2.5428, 2.5269, 2.4937, 2.4839]
  policy_loss : [2.5993, 2.5663, 2.5396, 2.5170, 2.4980, 2.4444, 2.4291, 2.3969, 2.3874]
  policy_policy_acc : [0.2599, 0.2682, 0.2714, 0.2756, 0.2803, 0.2887, 0.2926, 0.3005, 0.3012]
  val_loss : [2.6655, 2.6645, 2.6535, 2.7383, 2.7399, 2.6630, 2.6603, 2.8386, 2.7101]
  val_policy_loss : [2.5663, 2.5658, 2.5542, 2.6423, 2.6440, 2.5684, 2.5653, 2.7443, 2.6168]
  val_policy_policy_acc : [0.2651, 0.2686, 0.2738, 0.2724, 0.2733, 0.2730, 0.2804, 0.2800, 0.2826]
  val_value_loss : [0.1977, 0.1962, 0.1976, 0.1908, 0.1903, 0.1880, 0.1891, 0.1871, 0.1854]
  val_value_value_mse : [0.1977, 0.1962, 0.1976, 0.1908, 0.1903, 0.1880, 0.1891, 0.1871, 0.1854]
  value_loss : [0.2073, 0.2062, 0.2044, 0.2027, 0.2013, 0.1969, 0.1954, 0.1937, 0.1930]
  value_value_mse : [0.2073, 0.2062, 0.2044, 0.2027, 0.2013, 0.1969, 0.1954, 0.1937, 0.1930]

================================================================================

History file: model_versions/chess_elo_model_V94_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T07:59:21.510561Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game139501_game141000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V93
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.478102   (epoch 10)   mode=min
  policy_loss: 2.382105   (epoch 10)   mode=min
  policy_policy_acc: 0.301684   (epoch 10)   mode=max
  val_loss: 2.645451   (epoch 5)   mode=min
  val_policy_loss: 2.550293   (epoch 5)   mode=min
  val_policy_policy_acc: 0.276178   (epoch 8)   mode=max
  val_value_loss: 0.183922   (epoch 10)   mode=min
  val_value_value_mse: 0.183704   (epoch 10)   mode=min
  value_loss: 0.191937   (epoch 10)   mode=min
  value_value_mse: 0.191933   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7146, 2.6748, 2.6457, 2.5916, 2.5710, 2.5545, 2.5355, 2.5063, 2.4935, 2.4781]
  policy_loss : [2.6107, 2.5727, 2.5443, 2.4923, 2.4721, 2.4564, 2.4378, 2.4098, 2.3974, 2.3821]
  policy_policy_acc : [0.2583, 0.2649, 0.2703, 0.2818, 0.2849, 0.2889, 0.2929, 0.2974, 0.2987, 0.3017]
  val_loss : [2.6736, 2.7596, 2.6993, 2.7595, 2.6455, 3.0760, 3.2399, 3.2496, 3.0434, 3.4271]
  val_policy_loss : [2.5741, 2.6581, 2.6010, 2.6636, 2.5503, 2.9791, 3.1434, 3.1540, 2.9486, 3.3317]
  val_policy_policy_acc : [0.2642, 0.2600, 0.2624, 0.2711, 0.2721, 0.2698, 0.2735, 0.2762, 0.2751, 0.2740]
  val_value_loss : [0.1954, 0.1983, 0.1928, 0.1881, 0.1879, 0.1885, 0.1874, 0.1855, 0.1844, 0.1839]
  val_value_value_mse : [0.1950, 0.1982, 0.1925, 0.1878, 0.1876, 0.1884, 0.1869, 0.1853, 0.1841, 0.1837]
  value_loss : [0.2077, 0.2043, 0.2027, 0.1984, 0.1978, 0.1961, 0.1952, 0.1930, 0.1922, 0.1919]
  value_value_mse : [0.2077, 0.2043, 0.2027, 0.1984, 0.1978, 0.1961, 0.1952, 0.1930, 0.1922, 0.1919]

================================================================================

History file: model_versions/chess_elo_model_V95_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:07:51.859770Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game141001_game142500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V94
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.464925   (epoch 10)   mode=min
  policy_loss: 2.367683   (epoch 10)   mode=min
  policy_policy_acc: 0.304811   (epoch 10)   mode=max
  val_loss: 2.672591   (epoch 4)   mode=min
  val_policy_loss: 2.570438   (epoch 4)   mode=min
  val_policy_policy_acc: 0.274058   (epoch 9)   mode=max
  val_value_loss: 0.190801   (epoch 10)   mode=min
  val_value_value_mse: 0.190798   (epoch 10)   mode=min
  value_loss: 0.194605   (epoch 10)   mode=min
  value_value_mse: 0.194613   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7127, 2.6759, 2.6474, 2.6256, 2.6027, 2.5821, 2.5259, 2.5077, 2.4774, 2.4649]
  policy_loss : [2.6075, 2.5721, 2.5443, 2.5235, 2.5013, 2.4807, 2.4269, 2.4094, 2.3799, 2.3677]
  policy_policy_acc : [0.2587, 0.2669, 0.2707, 0.2749, 0.2776, 0.2822, 0.2941, 0.2956, 0.3012, 0.3048]
  val_loss : [2.7788, 2.7336, 2.8252, 2.6726, 2.7778, 3.1238, 2.9370, 3.5522, 3.2766, 3.5997]
  val_policy_loss : [2.6760, 2.6345, 2.7258, 2.5704, 2.6779, 3.0268, 2.8395, 3.4563, 3.1807, 3.5042]
  val_policy_policy_acc : [0.2601, 0.2643, 0.2675, 0.2612, 0.2644, 0.2663, 0.2699, 0.2691, 0.2741, 0.2712]
  val_value_loss : [0.2055, 0.1982, 0.1988, 0.2042, 0.1997, 0.1938, 0.1947, 0.1915, 0.1917, 0.1908]
  val_value_value_mse : [0.2055, 0.1981, 0.1988, 0.2042, 0.1997, 0.1938, 0.1947, 0.1915, 0.1917, 0.1908]
  value_loss : [0.2104, 0.2078, 0.2062, 0.2046, 0.2028, 0.2027, 0.1979, 0.1969, 0.1951, 0.1946]
  value_value_mse : [0.2104, 0.2078, 0.2062, 0.2046, 0.2028, 0.2027, 0.1979, 0.1969, 0.1951, 0.1946]

================================================================================

History file: model_versions/chess_elo_model_V96_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:13:51.753891Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game142501_game144000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V95
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.509478   (epoch 7)   mode=min
  policy_loss: 2.413425   (epoch 7)   mode=min
  policy_policy_acc: 0.295563   (epoch 7)   mode=max
  val_loss: 2.667199   (epoch 1)   mode=min
  val_policy_loss: 2.569448   (epoch 1)   mode=min
  val_policy_policy_acc: 0.281829   (epoch 7)   mode=max
  val_value_loss: 0.181822   (epoch 7)   mode=min
  val_value_value_mse: 0.181837   (epoch 7)   mode=min
  value_loss: 0.191911   (epoch 7)   mode=min
  value_value_mse: 0.191896   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6949, 2.6562, 2.6251, 2.5742, 2.5512, 2.5212, 2.5095]
  policy_loss : [2.5927, 2.5544, 2.5250, 2.4759, 2.4542, 2.4253, 2.4134]
  policy_policy_acc : [0.2591, 0.2670, 0.2723, 0.2829, 0.2870, 0.2928, 0.2956]
  val_loss : [2.6672, 2.7045, 3.3027, 3.0039, 3.1103, 2.8458, 2.8183]
  val_policy_loss : [2.5694, 2.6088, 3.2073, 2.9095, 3.0156, 2.7542, 2.7270]
  val_policy_policy_acc : [0.2648, 0.2654, 0.2710, 0.2748, 0.2723, 0.2773, 0.2818]
  val_value_loss : [0.1949, 0.1909, 0.1888, 0.1874, 0.1879, 0.1824, 0.1818]
  val_value_value_mse : [0.1949, 0.1909, 0.1888, 0.1874, 0.1880, 0.1824, 0.1818]
  value_loss : [0.2052, 0.2025, 0.1998, 0.1962, 0.1941, 0.1924, 0.1919]
  value_value_mse : [0.2052, 0.2025, 0.1998, 0.1962, 0.1940, 0.1923, 0.1919]

================================================================================

History file: model_versions/chess_elo_model_V97_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:20:31.938813Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game144001_game145500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V96
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.505607   (epoch 8)   mode=min
  policy_loss: 2.405286   (epoch 8)   mode=min
  policy_policy_acc: 0.298994   (epoch 8)   mode=max
  val_loss: 2.643909   (epoch 2)   mode=min
  val_policy_loss: 2.541674   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287617   (epoch 8)   mode=max
  val_value_loss: 0.193447   (epoch 7)   mode=min
  val_value_value_mse: 0.193453   (epoch 7)   mode=min
  value_loss: 0.200656   (epoch 8)   mode=min
  value_value_mse: 0.200654   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7123, 2.6742, 2.6454, 2.6223, 2.5701, 2.5473, 2.5167, 2.5056]
  policy_loss : [2.6050, 2.5680, 2.5399, 2.5181, 2.4677, 2.4453, 2.4157, 2.4053]
  policy_policy_acc : [0.2626, 0.2688, 0.2736, 0.2760, 0.2880, 0.2922, 0.2973, 0.2990]
  val_loss : [2.7421, 2.6439, 2.9816, 2.6539, 4.3300, 2.7017, 3.1558, 2.7220]
  val_policy_loss : [2.6388, 2.5417, 2.8794, 2.5549, 4.2292, 2.6032, 3.0575, 2.6241]
  val_policy_policy_acc : [0.2765, 0.2768, 0.2810, 0.2776, 0.2807, 0.2794, 0.2839, 0.2876]
  val_value_loss : [0.2045, 0.2026, 0.2017, 0.1962, 0.1957, 0.1948, 0.1934, 0.1937]
  val_value_value_mse : [0.2045, 0.2026, 0.2017, 0.1962, 0.1957, 0.1948, 0.1935, 0.1937]
  value_loss : [0.2146, 0.2124, 0.2110, 0.2084, 0.2048, 0.2041, 0.2019, 0.2007]
  value_value_mse : [0.2146, 0.2124, 0.2110, 0.2084, 0.2048, 0.2041, 0.2019, 0.2007]

================================================================================

History file: model_versions/chess_elo_model_V98_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:29:20.267827Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game145501_game147000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V97
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.490788   (epoch 10)   mode=min
  policy_loss: 2.393380   (epoch 10)   mode=min
  policy_policy_acc: 0.296765   (epoch 10)   mode=max
  val_loss: 2.698162   (epoch 6)   mode=min
  val_policy_loss: 2.603821   (epoch 6)   mode=min
  val_policy_policy_acc: 0.276331   (epoch 10)   mode=max
  val_value_loss: 0.185086   (epoch 10)   mode=min
  val_value_value_mse: 0.185087   (epoch 10)   mode=min
  value_loss: 0.194875   (epoch 10)   mode=min
  value_value_mse: 0.194867   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7170, 2.6789, 2.6550, 2.5966, 2.5764, 2.5582, 2.5461, 2.5310, 2.5040, 2.4908]
  policy_loss : [2.6116, 2.5747, 2.5519, 2.4959, 2.4760, 2.4585, 2.4467, 2.4325, 2.4059, 2.3934]
  policy_policy_acc : [0.2556, 0.2618, 0.2650, 0.2781, 0.2800, 0.2840, 0.2870, 0.2887, 0.2937, 0.2968]
  val_loss : [2.9369, 3.2084, 3.2581, 3.2413, 2.7604, 2.6982, 2.8936, 2.9518, 2.9611, 3.5503]
  val_policy_loss : [2.8325, 3.1092, 3.1610, 3.1442, 2.6654, 2.6038, 2.7994, 2.8580, 2.8677, 3.4569]
  val_policy_policy_acc : [0.2660, 0.2687, 0.2610, 0.2710, 0.2745, 0.2716, 0.2727, 0.2740, 0.2733, 0.2763]
  val_value_loss : [0.2081, 0.1972, 0.1932, 0.1930, 0.1894, 0.1884, 0.1878, 0.1868, 0.1859, 0.1851]
  val_value_value_mse : [0.2081, 0.1972, 0.1932, 0.1929, 0.1894, 0.1884, 0.1878, 0.1868, 0.1859, 0.1851]
  value_loss : [0.2106, 0.2084, 0.2060, 0.2014, 0.2010, 0.1995, 0.1985, 0.1971, 0.1961, 0.1949]
  value_value_mse : [0.2106, 0.2083, 0.2060, 0.2014, 0.2010, 0.1995, 0.1985, 0.1970, 0.1961, 0.1949]

================================================================================

History file: model_versions/chess_elo_model_V99_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:36:04.467831Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game147001_game148500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V98
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.491430   (epoch 8)   mode=min
  policy_loss: 2.396273   (epoch 8)   mode=min
  policy_policy_acc: 0.296730   (epoch 8)   mode=max
  val_loss: 2.610166   (epoch 2)   mode=min
  val_policy_loss: 2.514411   (epoch 2)   mode=min
  val_policy_policy_acc: 0.292127   (epoch 7)   mode=max
  val_value_loss: 0.183795   (epoch 8)   mode=min
  val_value_value_mse: 0.183797   (epoch 8)   mode=min
  value_loss: 0.190553   (epoch 8)   mode=min
  value_value_mse: 0.190549   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6978, 2.6578, 2.6310, 2.6065, 2.5555, 2.5354, 2.5030, 2.4914]
  policy_loss : [2.5957, 2.5572, 2.5315, 2.5077, 2.4583, 2.4389, 2.4073, 2.3963]
  policy_policy_acc : [0.2580, 0.2663, 0.2725, 0.2752, 0.2862, 0.2889, 0.2947, 0.2967]
  val_loss : [2.8383, 2.6102, 2.7564, 2.6924, 3.5270, 3.9794, 3.0492, 3.8707]
  val_policy_loss : [2.7401, 2.5144, 2.6624, 2.5972, 3.4337, 3.8870, 2.9568, 3.7786]
  val_policy_policy_acc : [0.2775, 0.2813, 0.2800, 0.2829, 0.2892, 0.2851, 0.2921, 0.2903]
  val_value_loss : [0.1962, 0.1915, 0.1880, 0.1903, 0.1862, 0.1844, 0.1848, 0.1838]
  val_value_value_mse : [0.1962, 0.1915, 0.1880, 0.1903, 0.1862, 0.1844, 0.1848, 0.1838]
  value_loss : [0.2040, 0.2015, 0.1992, 0.1978, 0.1946, 0.1929, 0.1914, 0.1906]
  value_value_mse : [0.2040, 0.2015, 0.1992, 0.1978, 0.1946, 0.1929, 0.1914, 0.1905]

================================================================================

History file: model_versions/chess_elo_model_V100_history.npy
Total runs: 1
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-13T08:42:13.004329Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  dataset: all_positions_jul2014_npz/positions_jul2014_game148501_game150000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V99
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.513519   (epoch 7)   mode=min
  policy_loss: 2.417849   (epoch 7)   mode=min
  policy_policy_acc: 0.293528   (epoch 7)   mode=max
  val_loss: 2.638942   (epoch 1)   mode=min
  val_policy_loss: 2.541273   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287721   (epoch 7)   mode=max
  val_value_loss: 0.183151   (epoch 6)   mode=min
  val_value_value_mse: 0.183299   (epoch 6)   mode=min
  value_loss: 0.191273   (epoch 7)   mode=min
  value_value_mse: 0.191280   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6904, 2.6538, 2.6295, 2.5761, 2.5551, 2.5252, 2.5135]
  policy_loss : [2.5893, 2.5532, 2.5298, 2.4787, 2.4585, 2.4294, 2.4178]
  policy_policy_acc : [0.2605, 0.2664, 0.2715, 0.2815, 0.2849, 0.2916, 0.2935]
  val_loss : [2.6389, 2.6724, 2.6523, 2.6944, 2.6862, 2.8538, 2.7668]
  val_policy_loss : [2.5413, 2.5750, 2.5580, 2.6000, 2.5933, 2.7616, 2.6743]
  val_policy_policy_acc : [0.2762, 0.2808, 0.2723, 0.2796, 0.2856, 0.2847, 0.2877]
  val_value_loss : [0.1955, 0.1952, 0.1887, 0.1886, 0.1856, 0.1832, 0.1842]
  val_value_value_mse : [0.1957, 0.1954, 0.1888, 0.1888, 0.1857, 0.1833, 0.1843]
  value_loss : [0.2023, 0.2012, 0.1995, 0.1951, 0.1934, 0.1916, 0.1913]
  value_value_mse : [0.2023, 0.2012, 0.1995, 0.1951, 0.1934, 0.1916, 0.1913]

================================================================================

