History file: model_versions/chess_elo_model_V1_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:07:02.321999Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game1_game1500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V0
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.238796   (epoch 9)   mode=min
  policy_loss: 2.136368   (epoch 9)   mode=min
  policy_policy_acc: 0.348023   (epoch 9)   mode=max
  policy_top10_acc: 0.867112   (epoch 9)   mode=max
  policy_top5_acc: 0.716673   (epoch 9)   mode=max
  val_loss: 2.928215   (epoch 3)   mode=min
  val_policy_loss: 2.813998   (epoch 3)   mode=min
  val_policy_policy_acc: 0.214286   (epoch 8)   mode=max
  val_policy_top10_acc: 0.710689   (epoch 8)   mode=max
  val_policy_top5_acc: 0.537463   (epoch 8)   mode=max
  val_value_loss: 0.227656   (epoch 8)   mode=min
  val_value_value_mse: 0.227671   (epoch 8)   mode=min
  value_loss: 0.204887   (epoch 9)   mode=min
  value_value_mse: 0.204897   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [3.0200, 2.8847, 2.7912, 2.7088, 2.6289, 2.4861, 2.4047, 2.2972, 2.2388]
  policy_loss : [2.9038, 2.7704, 2.6789, 2.5991, 2.5201, 2.3793, 2.2999, 2.1937, 2.1364]
  policy_policy_acc : [0.1805, 0.2118, 0.2315, 0.2477, 0.2638, 0.2957, 0.3115, 0.3347, 0.3480]
  policy_top10_acc : [0.6640, 0.7116, 0.7416, 0.7657, 0.7876, 0.8189, 0.8366, 0.8559, 0.8671]
  policy_top5_acc : [0.4799, 0.5323, 0.5612, 0.5879, 0.6131, 0.6517, 0.6755, 0.7020, 0.7167]
  val_loss : [2.9715, 2.9447, 2.9282, 2.9601, 2.9590, 3.0116, 3.0437, 3.1031, 3.1507]
  val_policy_loss : [2.8558, 2.8294, 2.8140, 2.8451, 2.8447, 2.8974, 2.9296, 2.9892, 3.0362]
  val_policy_policy_acc : [0.1883, 0.2012, 0.2099, 0.2119, 0.2129, 0.2140, 0.2123, 0.2143, 0.2136]
  val_policy_top10_acc : [0.6770, 0.6896, 0.6993, 0.7043, 0.7075, 0.7083, 0.7078, 0.7107, 0.7091]
  val_policy_top5_acc : [0.4957, 0.5085, 0.5203, 0.5216, 0.5263, 0.5296, 0.5309, 0.5375, 0.5333]
  val_value_loss : [0.2316, 0.2305, 0.2284, 0.2300, 0.2285, 0.2284, 0.2281, 0.2277, 0.2290]
  val_value_value_mse : [0.2316, 0.2305, 0.2284, 0.2300, 0.2285, 0.2284, 0.2281, 0.2277, 0.2290]
  value_loss : [0.2324, 0.2284, 0.2246, 0.2195, 0.2175, 0.2133, 0.2097, 0.2069, 0.2049]
  value_value_mse : [0.2324, 0.2284, 0.2246, 0.2195, 0.2175, 0.2133, 0.2097, 0.2069, 0.2049]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:16:04.341277Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game1501_game3000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.256605   (epoch 8)   mode=min
  policy_loss: 2.158215   (epoch 8)   mode=min
  policy_policy_acc: 0.339599   (epoch 8)   mode=max
  policy_top10_acc: 0.861671   (epoch 8)   mode=max
  policy_top5_acc: 0.714258   (epoch 8)   mode=max
  val_loss: 2.858630   (epoch 2)   mode=min
  val_policy_loss: 2.746455   (epoch 2)   mode=min
  val_policy_policy_acc: 0.231369   (epoch 7)   mode=max
  val_policy_top10_acc: 0.719081   (epoch 6)   mode=max
  val_policy_top5_acc: 0.549950   (epoch 5)   mode=max
  val_value_loss: 0.221967   (epoch 7)   mode=min
  val_value_value_mse: 0.221998   (epoch 7)   mode=min
  value_loss: 0.196754   (epoch 8)   mode=min
  value_value_mse: 0.196754   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8719, 2.7703, 2.6912, 2.6153, 2.4812, 2.4024, 2.3070, 2.2566]
  policy_loss : [2.7590, 2.6611, 2.5829, 2.5095, 2.3784, 2.3017, 2.2078, 2.1582]
  policy_policy_acc : [0.2176, 0.2355, 0.2519, 0.2660, 0.2940, 0.3081, 0.3291, 0.3396]
  policy_top10_acc : [0.7116, 0.7420, 0.7663, 0.7877, 0.8182, 0.8343, 0.8538, 0.8617]
  policy_top5_acc : [0.5372, 0.5688, 0.5929, 0.6168, 0.6534, 0.6749, 0.7003, 0.7143]
  val_loss : [2.8998, 2.8586, 2.8780, 2.9067, 2.9041, 2.9470, 2.9694, 2.9945]
  val_policy_loss : [2.7863, 2.7465, 2.7667, 2.7932, 2.7929, 2.8356, 2.8583, 2.8830]
  val_policy_policy_acc : [0.2150, 0.2213, 0.2185, 0.2208, 0.2262, 0.2275, 0.2314, 0.2247]
  val_policy_top10_acc : [0.7014, 0.7112, 0.7113, 0.7126, 0.7177, 0.7191, 0.7181, 0.7181]
  val_policy_top5_acc : [0.5251, 0.5375, 0.5404, 0.5399, 0.5500, 0.5485, 0.5489, 0.5478]
  val_value_loss : [0.2268, 0.2243, 0.2225, 0.2268, 0.2221, 0.2225, 0.2220, 0.2229]
  val_value_value_mse : [0.2268, 0.2243, 0.2225, 0.2269, 0.2221, 0.2226, 0.2220, 0.2229]
  value_loss : [0.2259, 0.2183, 0.2166, 0.2115, 0.2055, 0.2013, 0.1983, 0.1968]
  value_value_mse : [0.2259, 0.2183, 0.2166, 0.2115, 0.2055, 0.2013, 0.1983, 0.1968]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:23:48.690303Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game3001_game4500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.335862   (epoch 7)   mode=min
  policy_loss: 2.234896   (epoch 7)   mode=min
  policy_policy_acc: 0.325097   (epoch 7)   mode=max
  policy_top10_acc: 0.846018   (epoch 7)   mode=max
  policy_top5_acc: 0.692991   (epoch 7)   mode=max
  val_loss: 2.823190   (epoch 1)   mode=min
  val_policy_loss: 2.711651   (epoch 1)   mode=min
  val_policy_policy_acc: 0.232567   (epoch 4)   mode=max
  val_policy_top10_acc: 0.729570   (epoch 4)   mode=max
  val_policy_top5_acc: 0.558741   (epoch 4)   mode=max
  val_value_loss: 0.220413   (epoch 3)   mode=min
  val_value_value_mse: 0.220440   (epoch 3)   mode=min
  value_loss: 0.201941   (epoch 7)   mode=min
  value_value_mse: 0.201951   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8432, 2.7481, 2.6690, 2.5427, 2.4753, 2.3803, 2.3359]
  policy_loss : [2.7321, 2.6397, 2.5632, 2.4381, 2.3722, 2.2783, 2.2349]
  policy_policy_acc : [0.2294, 0.2450, 0.2597, 0.2834, 0.2969, 0.3164, 0.3251]
  policy_top10_acc : [0.7183, 0.7477, 0.7700, 0.8023, 0.8182, 0.8373, 0.8460]
  policy_top5_acc : [0.5476, 0.5741, 0.5980, 0.6367, 0.6538, 0.6801, 0.6930]
  val_loss : [2.8232, 2.8258, 2.8304, 2.8457, 2.8598, 2.8924, 2.9186]
  val_policy_loss : [2.7117, 2.7152, 2.7200, 2.7338, 2.7477, 2.7811, 2.8065]
  val_policy_policy_acc : [0.2233, 0.2279, 0.2217, 0.2326, 0.2298, 0.2292, 0.2270]
  val_policy_top10_acc : [0.7233, 0.7245, 0.7249, 0.7296, 0.7272, 0.7294, 0.7271]
  val_policy_top5_acc : [0.5501, 0.5499, 0.5458, 0.5587, 0.5556, 0.5575, 0.5571]
  val_value_loss : [0.2229, 0.2209, 0.2204, 0.2234, 0.2237, 0.2221, 0.2236]
  val_value_value_mse : [0.2229, 0.2210, 0.2204, 0.2234, 0.2238, 0.2221, 0.2236]
  value_loss : [0.2228, 0.2166, 0.2126, 0.2087, 0.2060, 0.2041, 0.2019]
  value_value_mse : [0.2228, 0.2166, 0.2126, 0.2088, 0.2060, 0.2041, 0.2020]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:32:46.784909Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game4501_game6000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.280981   (epoch 8)   mode=min
  policy_loss: 2.179049   (epoch 8)   mode=min
  policy_policy_acc: 0.333440   (epoch 8)   mode=max
  policy_top10_acc: 0.858600   (epoch 8)   mode=max
  policy_top5_acc: 0.705438   (epoch 8)   mode=max
  val_loss: 2.808172   (epoch 2)   mode=min
  val_policy_loss: 2.696369   (epoch 2)   mode=min
  val_policy_policy_acc: 0.234266   (epoch 4)   mode=max
  val_policy_top10_acc: 0.732368   (epoch 4)   mode=max
  val_policy_top5_acc: 0.561139   (epoch 4)   mode=max
  val_value_loss: 0.222018   (epoch 6)   mode=min
  val_value_value_mse: 0.222044   (epoch 6)   mode=min
  value_loss: 0.203699   (epoch 8)   mode=min
  value_value_mse: 0.203695   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.8345, 2.7511, 2.6760, 2.6066, 2.4823, 2.4139, 2.3259, 2.2810]
  policy_loss : [2.7209, 2.6398, 2.5665, 2.4985, 2.3763, 2.3098, 2.2230, 2.1790]
  policy_policy_acc : [0.2245, 0.2401, 0.2535, 0.2666, 0.2906, 0.3053, 0.3242, 0.3334]
  policy_top10_acc : [0.7207, 0.7482, 0.7708, 0.7891, 0.8175, 0.8328, 0.8492, 0.8586]
  policy_top5_acc : [0.5459, 0.5723, 0.5959, 0.6167, 0.6519, 0.6709, 0.6933, 0.7054]
  val_loss : [2.8136, 2.8082, 2.8258, 2.8353, 2.8744, 2.8826, 2.9455, 2.9632]
  val_policy_loss : [2.7006, 2.6964, 2.7146, 2.7240, 2.7628, 2.7716, 2.8342, 2.8515]
  val_policy_policy_acc : [0.2282, 0.2321, 0.2272, 0.2343, 0.2263, 0.2288, 0.2312, 0.2299]
  val_policy_top10_acc : [0.7286, 0.7283, 0.7263, 0.7324, 0.7256, 0.7271, 0.7238, 0.7281]
  val_policy_top5_acc : [0.5502, 0.5602, 0.5581, 0.5611, 0.5560, 0.5542, 0.5573, 0.5582]
  val_value_loss : [0.2259, 0.2234, 0.2222, 0.2226, 0.2231, 0.2220, 0.2225, 0.2234]
  val_value_value_mse : [0.2259, 0.2234, 0.2222, 0.2226, 0.2231, 0.2220, 0.2226, 0.2234]
  value_loss : [0.2270, 0.2226, 0.2192, 0.2161, 0.2120, 0.2083, 0.2056, 0.2037]
  value_value_mse : [0.2270, 0.2226, 0.2192, 0.2161, 0.2120, 0.2083, 0.2056, 0.2037]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:43:18.154334Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game6001_game7500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.259804   (epoch 8)   mode=min
  policy_loss: 2.164819   (epoch 8)   mode=min
  policy_policy_acc: 0.340378   (epoch 8)   mode=max
  policy_top10_acc: 0.860747   (epoch 8)   mode=max
  policy_top5_acc: 0.709138   (epoch 8)   mode=max
  val_loss: 2.783530   (epoch 2)   mode=min
  val_policy_loss: 2.674989   (epoch 2)   mode=min
  val_policy_policy_acc: 0.240060   (epoch 7)   mode=max
  val_policy_top10_acc: 0.739560   (epoch 6)   mode=max
  val_policy_top5_acc: 0.566633   (epoch 8)   mode=max
  val_value_loss: 0.216770   (epoch 2)   mode=min
  val_value_value_mse: 0.216786   (epoch 2)   mode=min
  value_loss: 0.189888   (epoch 8)   mode=min
  value_value_mse: 0.189886   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7924, 2.7160, 2.6433, 2.5726, 2.4533, 2.3867, 2.3016, 2.2598]
  policy_loss : [2.6860, 2.6117, 2.5411, 2.4722, 2.3545, 2.2899, 2.2060, 2.1648]
  policy_policy_acc : [0.2355, 0.2483, 0.2631, 0.2748, 0.2990, 0.3128, 0.3292, 0.3404]
  policy_top10_acc : [0.7319, 0.7536, 0.7767, 0.7957, 0.8236, 0.8373, 0.8543, 0.8607]
  policy_top5_acc : [0.5611, 0.5840, 0.6047, 0.6267, 0.6604, 0.6784, 0.6989, 0.7091]
  val_loss : [2.7888, 2.7835, 2.7939, 2.7988, 2.8346, 2.8692, 2.8898, 2.9156]
  val_policy_loss : [2.6791, 2.6750, 2.6840, 2.6897, 2.7251, 2.7593, 2.7790, 2.8048]
  val_policy_policy_acc : [0.2372, 0.2381, 0.2388, 0.2388, 0.2395, 0.2377, 0.2401, 0.2365]
  val_policy_top10_acc : [0.7339, 0.7367, 0.7386, 0.7355, 0.7373, 0.7396, 0.7381, 0.7364]
  val_policy_top5_acc : [0.5562, 0.5626, 0.5629, 0.5657, 0.5658, 0.5637, 0.5633, 0.5666]
  val_value_loss : [0.2192, 0.2168, 0.2197, 0.2179, 0.2187, 0.2193, 0.2211, 0.2209]
  val_value_value_mse : [0.2192, 0.2168, 0.2197, 0.2179, 0.2187, 0.2193, 0.2211, 0.2210]
  value_loss : [0.2130, 0.2080, 0.2044, 0.2006, 0.1971, 0.1936, 0.1911, 0.1899]
  value_value_mse : [0.2130, 0.2080, 0.2044, 0.2006, 0.1971, 0.1936, 0.1911, 0.1899]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T16:52:02.231395Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game7501_game9000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.321472   (epoch 7)   mode=min
  policy_loss: 2.223043   (epoch 7)   mode=min
  policy_policy_acc: 0.327921   (epoch 7)   mode=max
  policy_top10_acc: 0.851142   (epoch 7)   mode=max
  policy_top5_acc: 0.694865   (epoch 7)   mode=max
  val_loss: 2.762416   (epoch 1)   mode=min
  val_policy_loss: 2.652857   (epoch 1)   mode=min
  val_policy_policy_acc: 0.245754   (epoch 6)   mode=max
  val_policy_top10_acc: 0.749351   (epoch 1)   mode=max
  val_policy_top5_acc: 0.573427   (epoch 6)   mode=max
  val_value_loss: 0.217286   (epoch 6)   mode=min
  val_value_value_mse: 0.217316   (epoch 6)   mode=min
  value_loss: 0.196866   (epoch 7)   mode=min
  value_value_mse: 0.196865   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7788, 2.6990, 2.6238, 2.5099, 2.4473, 2.3621, 2.3215]
  policy_loss : [2.6706, 2.5933, 2.5197, 2.4076, 2.3467, 2.2626, 2.2230]
  policy_policy_acc : [0.2384, 0.2533, 0.2673, 0.2894, 0.3028, 0.3187, 0.3279]
  policy_top10_acc : [0.7384, 0.7626, 0.7827, 0.8100, 0.8260, 0.8429, 0.8511]
  policy_top5_acc : [0.5657, 0.5907, 0.6125, 0.6447, 0.6610, 0.6849, 0.6949]
  val_loss : [2.7624, 2.7674, 2.7801, 2.7878, 2.8040, 2.8335, 2.8481]
  val_policy_loss : [2.6529, 2.6568, 2.6706, 2.6791, 2.6948, 2.7248, 2.7391]
  val_policy_policy_acc : [0.2375, 0.2424, 0.2403, 0.2423, 0.2433, 0.2458, 0.2425]
  val_policy_top10_acc : [0.7494, 0.7408, 0.7394, 0.7439, 0.7481, 0.7451, 0.7471]
  val_policy_top5_acc : [0.5707, 0.5708, 0.5625, 0.5724, 0.5727, 0.5734, 0.5694]
  val_value_loss : [0.2190, 0.2209, 0.2190, 0.2173, 0.2182, 0.2173, 0.2180]
  val_value_value_mse : [0.2190, 0.2210, 0.2190, 0.2174, 0.2183, 0.2173, 0.2181]
  value_loss : [0.2162, 0.2116, 0.2082, 0.2045, 0.2011, 0.1991, 0.1969]
  value_value_mse : [0.2162, 0.2116, 0.2082, 0.2045, 0.2011, 0.1991, 0.1969]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:00:30.728579Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game9001_game10500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.334939   (epoch 7)   mode=min
  policy_loss: 2.235303   (epoch 7)   mode=min
  policy_policy_acc: 0.325312   (epoch 7)   mode=max
  policy_top10_acc: 0.849306   (epoch 7)   mode=max
  policy_top5_acc: 0.693502   (epoch 7)   mode=max
  val_loss: 2.756637   (epoch 1)   mode=min
  val_policy_loss: 2.647094   (epoch 1)   mode=min
  val_policy_policy_acc: 0.246254   (epoch 7)   mode=max
  val_policy_top10_acc: 0.753247   (epoch 4)   mode=max
  val_policy_top5_acc: 0.578322   (epoch 4)   mode=max
  val_value_loss: 0.217780   (epoch 4)   mode=min
  val_value_value_mse: 0.217797   (epoch 4)   mode=min
  value_loss: 0.199220   (epoch 7)   mode=min
  value_value_mse: 0.199218   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7769, 2.7024, 2.6299, 2.5189, 2.4533, 2.3746, 2.3349]
  policy_loss : [2.6692, 2.5966, 2.5253, 2.4159, 2.3513, 2.2743, 2.2353]
  policy_policy_acc : [0.2377, 0.2516, 0.2651, 0.2888, 0.2999, 0.3171, 0.3253]
  policy_top10_acc : [0.7409, 0.7634, 0.7837, 0.8095, 0.8256, 0.8408, 0.8493]
  policy_top5_acc : [0.5676, 0.5894, 0.6123, 0.6432, 0.6623, 0.6812, 0.6935]
  val_loss : [2.7566, 2.7669, 2.7596, 2.7763, 2.7904, 2.8149, 2.8225]
  val_policy_loss : [2.6471, 2.6564, 2.6501, 2.6673, 2.6802, 2.7053, 2.7128]
  val_policy_policy_acc : [0.2446, 0.2428, 0.2446, 0.2436, 0.2438, 0.2442, 0.2463]
  val_policy_top10_acc : [0.7457, 0.7427, 0.7505, 0.7532, 0.7477, 0.7465, 0.7454]
  val_policy_top5_acc : [0.5743, 0.5672, 0.5714, 0.5783, 0.5686, 0.5724, 0.5715]
  val_value_loss : [0.2189, 0.2208, 0.2187, 0.2178, 0.2205, 0.2190, 0.2193]
  val_value_value_mse : [0.2190, 0.2208, 0.2188, 0.2178, 0.2205, 0.2190, 0.2193]
  value_loss : [0.2155, 0.2114, 0.2093, 0.2060, 0.2038, 0.2007, 0.1992]
  value_value_mse : [0.2155, 0.2114, 0.2093, 0.2060, 0.2038, 0.2007, 0.1992]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:09:53.275298Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game10501_game12000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.316935   (epoch 7)   mode=min
  policy_loss: 2.219268   (epoch 7)   mode=min
  policy_policy_acc: 0.329827   (epoch 7)   mode=max
  policy_top10_acc: 0.852820   (epoch 7)   mode=max
  policy_top5_acc: 0.697133   (epoch 7)   mode=max
  val_loss: 2.741114   (epoch 1)   mode=min
  val_policy_loss: 2.633449   (epoch 1)   mode=min
  val_policy_policy_acc: 0.245954   (epoch 1)   mode=max
  val_policy_top10_acc: 0.751149   (epoch 1)   mode=max
  val_policy_top5_acc: 0.576224   (epoch 6)   mode=max
  val_value_loss: 0.214383   (epoch 7)   mode=min
  val_value_value_mse: 0.214415   (epoch 7)   mode=min
  value_loss: 0.195047   (epoch 7)   mode=min
  value_value_mse: 0.195056   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7494, 2.6774, 2.6094, 2.4957, 2.4317, 2.3562, 2.3169]
  policy_loss : [2.6421, 2.5724, 2.5056, 2.3945, 2.3319, 2.2580, 2.2193]
  policy_policy_acc : [0.2433, 0.2565, 0.2696, 0.2913, 0.3031, 0.3183, 0.3298]
  policy_top10_acc : [0.7476, 0.7680, 0.7881, 0.8149, 0.8306, 0.8454, 0.8528]
  policy_top5_acc : [0.5753, 0.5977, 0.6183, 0.6498, 0.6674, 0.6876, 0.6971]
  val_loss : [2.7411, 2.7489, 2.7552, 2.7889, 2.7903, 2.8178, 2.8336]
  val_policy_loss : [2.6334, 2.6410, 2.6463, 2.6812, 2.6807, 2.7104, 2.7262]
  val_policy_policy_acc : [0.2460, 0.2456, 0.2457, 0.2435, 0.2420, 0.2398, 0.2391]
  val_policy_top10_acc : [0.7511, 0.7500, 0.7483, 0.7495, 0.7465, 0.7505, 0.7476]
  val_policy_top5_acc : [0.5755, 0.5738, 0.5709, 0.5757, 0.5737, 0.5762, 0.5761]
  val_value_loss : [0.2151, 0.2153, 0.2177, 0.2152, 0.2189, 0.2145, 0.2144]
  val_value_value_mse : [0.2151, 0.2154, 0.2177, 0.2152, 0.2189, 0.2145, 0.2144]
  value_loss : [0.2141, 0.2099, 0.2072, 0.2020, 0.1993, 0.1969, 0.1950]
  value_value_mse : [0.2142, 0.2099, 0.2072, 0.2020, 0.1993, 0.1969, 0.1951]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:18:06.591333Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game12001_game13500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.310119   (epoch 7)   mode=min
  policy_loss: 2.214224   (epoch 7)   mode=min
  policy_policy_acc: 0.327225   (epoch 7)   mode=max
  policy_top10_acc: 0.851987   (epoch 7)   mode=max
  policy_top5_acc: 0.699734   (epoch 7)   mode=max
  val_loss: 2.732416   (epoch 1)   mode=min
  val_policy_loss: 2.623142   (epoch 1)   mode=min
  val_policy_policy_acc: 0.248352   (epoch 7)   mode=max
  val_policy_top10_acc: 0.752947   (epoch 1)   mode=max
  val_policy_top5_acc: 0.582518   (epoch 1)   mode=max
  val_value_loss: 0.215622   (epoch 3)   mode=min
  val_value_value_mse: 0.215656   (epoch 3)   mode=min
  value_loss: 0.191793   (epoch 7)   mode=min
  value_value_mse: 0.191793   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7292, 2.6577, 2.5924, 2.4804, 2.4211, 2.3451, 2.3101]
  policy_loss : [2.6223, 2.5523, 2.4895, 2.3806, 2.3225, 2.2486, 2.2142]
  policy_policy_acc : [0.2481, 0.2609, 0.2712, 0.2931, 0.3033, 0.3195, 0.3272]
  policy_top10_acc : [0.7531, 0.7747, 0.7922, 0.8175, 0.8302, 0.8453, 0.8520]
  policy_top5_acc : [0.5811, 0.6029, 0.6224, 0.6545, 0.6697, 0.6899, 0.6997]
  val_loss : [2.7324, 2.7476, 2.7748, 2.7609, 2.8104, 2.7998, 2.8258]
  val_policy_loss : [2.6231, 2.6388, 2.6669, 2.6522, 2.7015, 2.6917, 2.7165]
  val_policy_policy_acc : [0.2480, 0.2439, 0.2415, 0.2456, 0.2425, 0.2462, 0.2484]
  val_policy_top10_acc : [0.7529, 0.7480, 0.7445, 0.7508, 0.7496, 0.7512, 0.7491]
  val_policy_top5_acc : [0.5825, 0.5804, 0.5752, 0.5811, 0.5789, 0.5810, 0.5821]
  val_value_loss : [0.2182, 0.2175, 0.2156, 0.2171, 0.2174, 0.2158, 0.2183]
  val_value_value_mse : [0.2183, 0.2175, 0.2157, 0.2171, 0.2175, 0.2158, 0.2183]
  value_loss : [0.2138, 0.2107, 0.2058, 0.1995, 0.1972, 0.1930, 0.1918]
  value_value_mse : [0.2138, 0.2107, 0.2058, 0.1995, 0.1972, 0.1930, 0.1918]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:26:28.888901Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game13501_game15000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.302027   (epoch 7)   mode=min
  policy_loss: 2.205946   (epoch 7)   mode=min
  policy_policy_acc: 0.331274   (epoch 7)   mode=max
  policy_top10_acc: 0.854890   (epoch 7)   mode=max
  policy_top5_acc: 0.699846   (epoch 7)   mode=max
  val_loss: 2.729031   (epoch 1)   mode=min
  val_policy_loss: 2.620343   (epoch 1)   mode=min
  val_policy_policy_acc: 0.251349   (epoch 6)   mode=max
  val_policy_top10_acc: 0.756543   (epoch 1)   mode=max
  val_policy_top5_acc: 0.584416   (epoch 5)   mode=max
  val_value_loss: 0.212326   (epoch 4)   mode=min
  val_value_value_mse: 0.212356   (epoch 4)   mode=min
  value_loss: 0.192190   (epoch 7)   mode=min
  value_value_mse: 0.192198   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7202, 2.6496, 2.5799, 2.4707, 2.4140, 2.3338, 2.3020]
  policy_loss : [2.6126, 2.5446, 2.4773, 2.3705, 2.3159, 2.2366, 2.2059]
  policy_policy_acc : [0.2511, 0.2643, 0.2761, 0.2969, 0.3092, 0.3258, 0.3313]
  policy_top10_acc : [0.7567, 0.7771, 0.7960, 0.8196, 0.8330, 0.8477, 0.8549]
  policy_top5_acc : [0.5861, 0.6056, 0.6254, 0.6569, 0.6712, 0.6927, 0.6998]
  val_loss : [2.7290, 2.7371, 2.7475, 2.7512, 2.7666, 2.8067, 2.8212]
  val_policy_loss : [2.6203, 2.6291, 2.6401, 2.6450, 2.6595, 2.6992, 2.7138]
  val_policy_policy_acc : [0.2500, 0.2461, 0.2448, 0.2506, 0.2474, 0.2513, 0.2506]
  val_policy_top10_acc : [0.7565, 0.7520, 0.7471, 0.7506, 0.7500, 0.7500, 0.7480]
  val_policy_top5_acc : [0.5828, 0.5810, 0.5746, 0.5843, 0.5844, 0.5828, 0.5790]
  val_value_loss : [0.2172, 0.2160, 0.2148, 0.2123, 0.2142, 0.2147, 0.2146]
  val_value_value_mse : [0.2172, 0.2160, 0.2149, 0.2124, 0.2142, 0.2147, 0.2146]
  value_loss : [0.2152, 0.2101, 0.2051, 0.2003, 0.1964, 0.1944, 0.1922]
  value_value_mse : [0.2152, 0.2101, 0.2051, 0.2003, 0.1964, 0.1944, 0.1922]

================================================================================

History file: model_versions/chess_elo_model_V2_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:35:42.707454Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game15001_game16500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V1
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.245776   (epoch 8)   mode=min
  policy_loss: 2.151521   (epoch 8)   mode=min
  policy_policy_acc: 0.343201   (epoch 8)   mode=max
  policy_top10_acc: 0.864182   (epoch 8)   mode=max
  policy_top5_acc: 0.714882   (epoch 8)   mode=max
  val_loss: 2.720978   (epoch 2)   mode=min
  val_policy_loss: 2.614416   (epoch 2)   mode=min
  val_policy_policy_acc: 0.248452   (epoch 4)   mode=max
  val_policy_top10_acc: 0.754545   (epoch 2)   mode=max
  val_policy_top5_acc: 0.583916   (epoch 1)   mode=max
  val_value_loss: 0.212915   (epoch 2)   mode=min
  val_value_value_mse: 0.212943   (epoch 2)   mode=min
  value_loss: 0.188103   (epoch 8)   mode=min
  value_value_mse: 0.188103   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7213, 2.6538, 2.5883, 2.5258, 2.4143, 2.3584, 2.2806, 2.2458]
  policy_loss : [2.6153, 2.5500, 2.4863, 2.4253, 2.3166, 2.2618, 2.1856, 2.1515]
  policy_policy_acc : [0.2496, 0.2622, 0.2737, 0.2858, 0.3071, 0.3176, 0.3352, 0.3432]
  policy_top10_acc : [0.7551, 0.7745, 0.7942, 0.8080, 0.8336, 0.8445, 0.8595, 0.8642]
  policy_top5_acc : [0.5852, 0.6051, 0.6250, 0.6434, 0.6727, 0.6878, 0.7062, 0.7149]
  val_loss : [2.7248, 2.7210, 2.7407, 2.7471, 2.7655, 2.7890, 2.8063, 2.8346]
  val_policy_loss : [2.6170, 2.6144, 2.6310, 2.6402, 2.6578, 2.6811, 2.6982, 2.7261]
  val_policy_policy_acc : [0.2479, 0.2458, 0.2428, 0.2485, 0.2443, 0.2416, 0.2411, 0.2442]
  val_policy_top10_acc : [0.7543, 0.7545, 0.7520, 0.7533, 0.7496, 0.7535, 0.7522, 0.7517]
  val_policy_top5_acc : [0.5839, 0.5818, 0.5772, 0.5811, 0.5783, 0.5833, 0.5762, 0.5745]
  val_value_loss : [0.2156, 0.2129, 0.2190, 0.2134, 0.2150, 0.2154, 0.2159, 0.2165]
  val_value_value_mse : [0.2156, 0.2129, 0.2190, 0.2135, 0.2151, 0.2154, 0.2159, 0.2165]
  value_loss : [0.2122, 0.2072, 0.2043, 0.2009, 0.1957, 0.1930, 0.1896, 0.1881]
  value_value_mse : [0.2122, 0.2072, 0.2043, 0.2009, 0.1957, 0.1929, 0.1896, 0.1881]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:43:58.849237Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game16501_game18000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.319896   (epoch 7)   mode=min
  policy_loss: 2.225265   (epoch 7)   mode=min
  policy_policy_acc: 0.327948   (epoch 7)   mode=max
  policy_top10_acc: 0.850082   (epoch 7)   mode=max
  policy_top5_acc: 0.696007   (epoch 7)   mode=max
  val_loss: 2.731224   (epoch 1)   mode=min
  val_policy_loss: 2.625705   (epoch 1)   mode=min
  val_policy_policy_acc: 0.249850   (epoch 4)   mode=max
  val_policy_top10_acc: 0.756543   (epoch 5)   mode=max
  val_policy_top5_acc: 0.587512   (epoch 4)   mode=max
  val_value_loss: 0.208486   (epoch 4)   mode=min
  val_value_value_mse: 0.208527   (epoch 4)   mode=min
  value_loss: 0.189368   (epoch 7)   mode=min
  value_value_mse: 0.189376   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7224, 2.6533, 2.5894, 2.4842, 2.4266, 2.3525, 2.3199]
  policy_loss : [2.6181, 2.5507, 2.4884, 2.3860, 2.3299, 2.2572, 2.2253]
  policy_policy_acc : [0.2497, 0.2618, 0.2743, 0.2952, 0.3058, 0.3214, 0.3279]
  policy_top10_acc : [0.7556, 0.7736, 0.7912, 0.8159, 0.8294, 0.8432, 0.8501]
  policy_top5_acc : [0.5858, 0.6060, 0.6236, 0.6544, 0.6684, 0.6880, 0.6960]
  val_loss : [2.7312, 2.7328, 2.7343, 2.7457, 2.7604, 2.7766, 2.7967]
  val_policy_loss : [2.6257, 2.6275, 2.6277, 2.6412, 2.6547, 2.6709, 2.6905]
  val_policy_policy_acc : [0.2460, 0.2440, 0.2455, 0.2499, 0.2482, 0.2457, 0.2460]
  val_policy_top10_acc : [0.7519, 0.7537, 0.7549, 0.7553, 0.7565, 0.7550, 0.7550]
  val_policy_top5_acc : [0.5804, 0.5749, 0.5818, 0.5875, 0.5872, 0.5805, 0.5808]
  val_value_loss : [0.2108, 0.2103, 0.2129, 0.2085, 0.2108, 0.2108, 0.2119]
  val_value_value_mse : [0.2108, 0.2103, 0.2129, 0.2085, 0.2109, 0.2108, 0.2119]
  value_loss : [0.2086, 0.2051, 0.2020, 0.1965, 0.1940, 0.1904, 0.1894]
  value_value_mse : [0.2086, 0.2052, 0.2020, 0.1965, 0.1940, 0.1904, 0.1894]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T17:52:28.422911Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game18001_game19500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.308032   (epoch 7)   mode=min
  policy_loss: 2.212678   (epoch 7)   mode=min
  policy_policy_acc: 0.329220   (epoch 7)   mode=max
  policy_top10_acc: 0.854698   (epoch 7)   mode=max
  policy_top5_acc: 0.702196   (epoch 7)   mode=max
  val_loss: 2.709459   (epoch 1)   mode=min
  val_policy_loss: 2.602906   (epoch 1)   mode=min
  val_policy_policy_acc: 0.253946   (epoch 4)   mode=max
  val_policy_top10_acc: 0.761638   (epoch 4)   mode=max
  val_policy_top5_acc: 0.591109   (epoch 4)   mode=max
  val_value_loss: 0.211626   (epoch 5)   mode=min
  val_value_value_mse: 0.211651   (epoch 5)   mode=min
  value_loss: 0.190671   (epoch 7)   mode=min
  value_value_mse: 0.190687   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7165, 2.6438, 2.5771, 2.4711, 2.4166, 2.3413, 2.3080]
  policy_loss : [2.6107, 2.5405, 2.4753, 2.3719, 2.3192, 2.2456, 2.2127]
  policy_policy_acc : [0.2494, 0.2640, 0.2761, 0.2963, 0.3083, 0.3220, 0.3292]
  policy_top10_acc : [0.7590, 0.7791, 0.7969, 0.8214, 0.8328, 0.8478, 0.8547]
  policy_top5_acc : [0.5896, 0.6104, 0.6316, 0.6598, 0.6746, 0.6942, 0.7022]
  val_loss : [2.7095, 2.7098, 2.7307, 2.7175, 2.7329, 2.7521, 2.7567]
  val_policy_loss : [2.6029, 2.6034, 2.6242, 2.6115, 2.6269, 2.6456, 2.6502]
  val_policy_policy_acc : [0.2500, 0.2495, 0.2456, 0.2539, 0.2508, 0.2507, 0.2534]
  val_policy_top10_acc : [0.7612, 0.7584, 0.7513, 0.7616, 0.7614, 0.7613, 0.7610]
  val_policy_top5_acc : [0.5895, 0.5780, 0.5788, 0.5911, 0.5896, 0.5870, 0.5887]
  val_value_loss : [0.2128, 0.2125, 0.2129, 0.2116, 0.2116, 0.2125, 0.2125]
  val_value_value_mse : [0.2129, 0.2125, 0.2129, 0.2117, 0.2117, 0.2125, 0.2125]
  value_loss : [0.2114, 0.2067, 0.2037, 0.1985, 0.1952, 0.1918, 0.1907]
  value_value_mse : [0.2114, 0.2067, 0.2037, 0.1985, 0.1952, 0.1918, 0.1907]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:01:29.325101Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game19501_game21000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.319952   (epoch 7)   mode=min
  policy_loss: 2.221695   (epoch 7)   mode=min
  policy_policy_acc: 0.326695   (epoch 7)   mode=max
  policy_top10_acc: 0.851765   (epoch 7)   mode=max
  policy_top5_acc: 0.697532   (epoch 7)   mode=max
  val_loss: 2.713324   (epoch 1)   mode=min
  val_policy_loss: 2.605759   (epoch 1)   mode=min
  val_policy_policy_acc: 0.258541   (epoch 5)   mode=max
  val_policy_top10_acc: 0.760939   (epoch 5)   mode=max
  val_policy_top5_acc: 0.590609   (epoch 5)   mode=max
  val_value_loss: 0.213882   (epoch 2)   mode=min
  val_value_value_mse: 0.213903   (epoch 2)   mode=min
  value_loss: 0.196566   (epoch 7)   mode=min
  value_value_mse: 0.196570   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7105, 2.6480, 2.5829, 2.4784, 2.4229, 2.3536, 2.3200]
  policy_loss : [2.6034, 2.5424, 2.4794, 2.3767, 2.3229, 2.2544, 2.2217]
  policy_policy_acc : [0.2540, 0.2659, 0.2756, 0.2958, 0.3079, 0.3209, 0.3267]
  policy_top10_acc : [0.7586, 0.7773, 0.7946, 0.8183, 0.8302, 0.8446, 0.8518]
  policy_top5_acc : [0.5890, 0.6070, 0.6273, 0.6559, 0.6700, 0.6886, 0.6975]
  val_loss : [2.7133, 2.7170, 2.7207, 2.7212, 2.7297, 2.7498, 2.7756]
  val_policy_loss : [2.6058, 2.6099, 2.6097, 2.6130, 2.6206, 2.6416, 2.6674]
  val_policy_policy_acc : [0.2513, 0.2566, 0.2531, 0.2547, 0.2585, 0.2517, 0.2503]
  val_policy_top10_acc : [0.7557, 0.7572, 0.7582, 0.7599, 0.7609, 0.7574, 0.7592]
  val_policy_top5_acc : [0.5854, 0.5858, 0.5890, 0.5867, 0.5906, 0.5888, 0.5868]
  val_value_loss : [0.2148, 0.2139, 0.2217, 0.2160, 0.2177, 0.2161, 0.2160]
  val_value_value_mse : [0.2148, 0.2139, 0.2218, 0.2161, 0.2178, 0.2161, 0.2160]
  value_loss : [0.2152, 0.2111, 0.2076, 0.2034, 0.2009, 0.1985, 0.1966]
  value_value_mse : [0.2152, 0.2111, 0.2076, 0.2034, 0.2009, 0.1985, 0.1966]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:11:39.810898Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game21001_game22500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.293349   (epoch 8)   mode=min
  policy_loss: 2.202847   (epoch 8)   mode=min
  policy_policy_acc: 0.331339   (epoch 8)   mode=max
  policy_top10_acc: 0.855447   (epoch 8)   mode=max
  policy_top5_acc: 0.705046   (epoch 8)   mode=max
  val_loss: 2.709078   (epoch 1)   mode=min
  val_policy_loss: 2.602088   (epoch 2)   mode=min
  val_policy_policy_acc: 0.257043   (epoch 6)   mode=max
  val_policy_top10_acc: 0.764236   (epoch 2)   mode=max
  val_policy_top5_acc: 0.592008   (epoch 6)   mode=max
  val_value_loss: 0.212626   (epoch 3)   mode=min
  val_value_value_mse: 0.212636   (epoch 3)   mode=min
  value_loss: 0.180957   (epoch 8)   mode=min
  value_value_mse: 0.180958   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.7195, 2.6525, 2.5890, 2.4859, 2.4334, 2.3634, 2.3329, 2.2933]
  policy_loss : [2.6178, 2.5528, 2.4908, 2.3905, 2.3400, 2.2713, 2.2415, 2.2028]
  policy_policy_acc : [0.2474, 0.2610, 0.2724, 0.2919, 0.3017, 0.3175, 0.3246, 0.3313]
  policy_top10_acc : [0.7561, 0.7747, 0.7924, 0.8154, 0.8273, 0.8426, 0.8478, 0.8554]
  policy_top5_acc : [0.5859, 0.6053, 0.6248, 0.6528, 0.6665, 0.6867, 0.6938, 0.7050]
  val_loss : [2.7091, 2.7105, 2.7207, 2.7216, 2.7431, 2.7412, 2.7606, 2.7657]
  val_policy_loss : [2.6022, 2.6021, 2.6143, 2.6150, 2.6350, 2.6332, 2.6518, 2.6574]
  val_policy_policy_acc : [0.2535, 0.2522, 0.2534, 0.2547, 0.2530, 0.2570, 0.2523, 0.2517]
  val_policy_top10_acc : [0.7611, 0.7642, 0.7585, 0.7587, 0.7568, 0.7600, 0.7574, 0.7583]
  val_policy_top5_acc : [0.5871, 0.5899, 0.5858, 0.5907, 0.5881, 0.5920, 0.5918, 0.5916]
  val_value_loss : [0.2136, 0.2166, 0.2126, 0.2130, 0.2161, 0.2158, 0.2174, 0.2164]
  val_value_value_mse : [0.2136, 0.2166, 0.2126, 0.2130, 0.2161, 0.2159, 0.2174, 0.2164]
  value_loss : [0.2035, 0.1994, 0.1963, 0.1909, 0.1869, 0.1844, 0.1828, 0.1810]
  value_value_mse : [0.2035, 0.1994, 0.1963, 0.1909, 0.1869, 0.1844, 0.1828, 0.1810]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:21:46.853531Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game22501_game24000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.270502   (epoch 8)   mode=min
  policy_loss: 2.180853   (epoch 8)   mode=min
  policy_policy_acc: 0.336088   (epoch 8)   mode=max
  policy_top10_acc: 0.857481   (epoch 8)   mode=max
  policy_top5_acc: 0.707875   (epoch 8)   mode=max
  val_loss: 2.708191   (epoch 2)   mode=min
  val_policy_loss: 2.601725   (epoch 2)   mode=min
  val_policy_policy_acc: 0.254346   (epoch 5)   mode=max
  val_policy_top10_acc: 0.759441   (epoch 2)   mode=max
  val_policy_top5_acc: 0.592208   (epoch 6)   mode=max
  val_value_loss: 0.212681   (epoch 2)   mode=min
  val_value_value_mse: 0.212716   (epoch 2)   mode=min
  value_loss: 0.179271   (epoch 8)   mode=min
  value_value_mse: 0.179273   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7207, 2.6511, 2.5883, 2.5279, 2.4247, 2.3736, 2.3062, 2.2705]
  policy_loss : [2.6181, 2.5509, 2.4900, 2.4312, 2.3310, 2.2812, 2.2158, 2.1809]
  policy_policy_acc : [0.2484, 0.2612, 0.2725, 0.2835, 0.3028, 0.3157, 0.3295, 0.3361]
  policy_top10_acc : [0.7538, 0.7743, 0.7925, 0.8066, 0.8283, 0.8383, 0.8515, 0.8575]
  policy_top5_acc : [0.5853, 0.6081, 0.6248, 0.6428, 0.6700, 0.6838, 0.7000, 0.7079]
  val_loss : [2.7172, 2.7082, 2.7361, 2.7445, 2.7590, 2.7632, 2.8035, 2.8047]
  val_policy_loss : [2.6097, 2.6017, 2.6265, 2.6346, 2.6494, 2.6529, 2.6941, 2.6946]
  val_policy_policy_acc : [0.2490, 0.2480, 0.2456, 0.2503, 0.2543, 0.2507, 0.2538, 0.2538]
  val_policy_top10_acc : [0.7582, 0.7594, 0.7483, 0.7559, 0.7572, 0.7561, 0.7554, 0.7546]
  val_policy_top5_acc : [0.5822, 0.5868, 0.5770, 0.5867, 0.5896, 0.5922, 0.5867, 0.5868]
  val_value_loss : [0.2150, 0.2127, 0.2190, 0.2196, 0.2189, 0.2205, 0.2186, 0.2199]
  val_value_value_mse : [0.2150, 0.2127, 0.2190, 0.2196, 0.2189, 0.2205, 0.2187, 0.2199]
  value_loss : [0.2054, 0.2004, 0.1967, 0.1934, 0.1872, 0.1850, 0.1809, 0.1793]
  value_value_mse : [0.2054, 0.2004, 0.1967, 0.1934, 0.1872, 0.1850, 0.1809, 0.1793]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:32:03.483773Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game24001_game25500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.268472   (epoch 8)   mode=min
  policy_loss: 2.180692   (epoch 8)   mode=min
  policy_policy_acc: 0.337732   (epoch 8)   mode=max
  policy_top10_acc: 0.859420   (epoch 8)   mode=max
  policy_top5_acc: 0.709304   (epoch 8)   mode=max
  val_loss: 2.692907   (epoch 1)   mode=min
  val_policy_loss: 2.588223   (epoch 2)   mode=min
  val_policy_policy_acc: 0.257343   (epoch 3)   mode=max
  val_policy_top10_acc: 0.765734   (epoch 1)   mode=max
  val_policy_top5_acc: 0.588312   (epoch 1)   mode=max
  val_value_loss: 0.208990   (epoch 1)   mode=min
  val_value_value_mse: 0.209023   (epoch 1)   mode=min
  value_loss: 0.175514   (epoch 8)   mode=min
  value_value_mse: 0.175514   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7093, 2.6410, 2.5774, 2.5214, 2.4157, 2.3650, 2.2996, 2.2685]
  policy_loss : [2.6068, 2.5408, 2.4803, 2.4262, 2.3228, 2.2742, 2.2108, 2.1807]
  policy_policy_acc : [0.2511, 0.2641, 0.2758, 0.2851, 0.3078, 0.3168, 0.3303, 0.3377]
  policy_top10_acc : [0.7600, 0.7795, 0.7960, 0.8093, 0.8314, 0.8416, 0.8536, 0.8594]
  policy_top5_acc : [0.5916, 0.6105, 0.6283, 0.6442, 0.6721, 0.6866, 0.7017, 0.7093]
  val_loss : [2.6929, 2.6954, 2.7120, 2.7526, 2.7419, 2.7721, 2.7784, 2.7991]
  val_policy_loss : [2.5883, 2.5882, 2.6058, 2.6426, 2.6360, 2.6658, 2.6725, 2.6929]
  val_policy_policy_acc : [0.2549, 0.2571, 0.2573, 0.2568, 0.2543, 0.2520, 0.2548, 0.2564]
  val_policy_top10_acc : [0.7657, 0.7643, 0.7553, 0.7552, 0.7576, 0.7572, 0.7585, 0.7529]
  val_policy_top5_acc : [0.5883, 0.5878, 0.5838, 0.5830, 0.5836, 0.5805, 0.5852, 0.5817]
  val_value_loss : [0.2090, 0.2143, 0.2121, 0.2198, 0.2116, 0.2122, 0.2115, 0.2123]
  val_value_value_mse : [0.2090, 0.2143, 0.2121, 0.2199, 0.2116, 0.2123, 0.2116, 0.2123]
  value_loss : [0.2052, 0.2002, 0.1943, 0.1903, 0.1858, 0.1819, 0.1777, 0.1755]
  value_value_mse : [0.2052, 0.2002, 0.1943, 0.1903, 0.1858, 0.1819, 0.1777, 0.1755]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:40:27.001390Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game25501_game27000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.326413   (epoch 7)   mode=min
  policy_loss: 2.238546   (epoch 7)   mode=min
  policy_policy_acc: 0.325458   (epoch 7)   mode=max
  policy_top10_acc: 0.845825   (epoch 7)   mode=max
  policy_top5_acc: 0.694314   (epoch 7)   mode=max
  val_loss: 2.677163   (epoch 1)   mode=min
  val_policy_loss: 2.572802   (epoch 1)   mode=min
  val_policy_policy_acc: 0.257942   (epoch 5)   mode=max
  val_policy_top10_acc: 0.763936   (epoch 5)   mode=max
  val_policy_top5_acc: 0.599201   (epoch 1)   mode=max
  val_value_loss: 0.208565   (epoch 1)   mode=min
  val_value_value_mse: 0.208594   (epoch 1)   mode=min
  value_loss: 0.176028   (epoch 7)   mode=min
  value_value_mse: 0.176016   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6985, 2.6320, 2.5705, 2.4683, 2.4189, 2.3543, 2.3264]
  policy_loss : [2.5975, 2.5332, 2.4743, 2.3756, 2.3281, 2.2651, 2.2385]
  policy_policy_acc : [0.2556, 0.2669, 0.2788, 0.2982, 0.3085, 0.3211, 0.3255]
  policy_top10_acc : [0.7605, 0.7785, 0.7958, 0.8181, 0.8275, 0.8398, 0.8458]
  policy_top5_acc : [0.5927, 0.6107, 0.6311, 0.6573, 0.6692, 0.6873, 0.6943]
  val_loss : [2.6772, 2.7045, 2.7096, 2.6999, 2.7087, 2.7295, 2.7376]
  val_policy_loss : [2.5728, 2.5996, 2.6042, 2.5947, 2.6025, 2.6227, 2.6314]
  val_policy_policy_acc : [0.2560, 0.2477, 0.2499, 0.2574, 0.2579, 0.2554, 0.2565]
  val_policy_top10_acc : [0.7638, 0.7530, 0.7585, 0.7618, 0.7639, 0.7626, 0.7589]
  val_policy_top5_acc : [0.5992, 0.5815, 0.5865, 0.5937, 0.5945, 0.5955, 0.5958]
  val_value_loss : [0.2086, 0.2097, 0.2107, 0.2104, 0.2124, 0.2136, 0.2125]
  val_value_value_mse : [0.2086, 0.2098, 0.2107, 0.2105, 0.2124, 0.2136, 0.2125]
  value_loss : [0.2026, 0.1972, 0.1930, 0.1862, 0.1816, 0.1781, 0.1760]
  value_value_mse : [0.2026, 0.1972, 0.1930, 0.1862, 0.1816, 0.1781, 0.1760]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T18:51:59.688404Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game27001_game28500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.236732   (epoch 10)   mode=min
  policy_loss: 2.149238   (epoch 10)   mode=min
  policy_policy_acc: 0.342429   (epoch 10)   mode=max
  policy_top10_acc: 0.863943   (epoch 10)   mode=max
  policy_top5_acc: 0.716435   (epoch 10)   mode=max
  val_loss: 2.687912   (epoch 4)   mode=min
  val_policy_loss: 2.584939   (epoch 4)   mode=min
  val_policy_policy_acc: 0.263037   (epoch 4)   mode=max
  val_policy_top10_acc: 0.768432   (epoch 7)   mode=max
  val_policy_top5_acc: 0.605095   (epoch 6)   mode=max
  val_value_loss: 0.204893   (epoch 2)   mode=min
  val_value_value_mse: 0.204904   (epoch 2)   mode=min
  value_loss: 0.175013   (epoch 10)   mode=min
  value_value_mse: 0.175013   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6968, 2.6293, 2.5705, 2.4708, 2.4226, 2.3819, 2.3198, 2.2890, 2.2503, 2.2367]
  policy_loss : [2.5945, 2.5293, 2.4732, 2.3765, 2.3298, 2.2906, 2.2300, 2.2001, 2.1623, 2.1492]
  policy_policy_acc : [0.2549, 0.2672, 0.2766, 0.2975, 0.3062, 0.3137, 0.3256, 0.3317, 0.3418, 0.3424]
  policy_top10_acc : [0.7639, 0.7823, 0.7983, 0.8189, 0.8299, 0.8375, 0.8501, 0.8549, 0.8616, 0.8639]
  policy_top5_acc : [0.5932, 0.6139, 0.6306, 0.6570, 0.6707, 0.6805, 0.6965, 0.7036, 0.7134, 0.7164]
  val_loss : [2.6892, 2.6969, 2.7055, 2.6879, 2.7147, 2.7254, 2.7428, 2.7522, 2.7653, 2.7737]
  val_policy_loss : [2.5865, 2.5943, 2.6019, 2.5849, 2.6113, 2.6217, 2.6386, 2.6472, 2.6603, 2.6684]
  val_policy_policy_acc : [0.2518, 0.2535, 0.2527, 0.2630, 0.2605, 0.2606, 0.2607, 0.2615, 0.2609, 0.2582]
  val_policy_top10_acc : [0.7597, 0.7626, 0.7601, 0.7678, 0.7665, 0.7650, 0.7684, 0.7638, 0.7647, 0.7639]
  val_policy_top5_acc : [0.5908, 0.5890, 0.5874, 0.6003, 0.5999, 0.6051, 0.5978, 0.5964, 0.6002, 0.5995]
  val_value_loss : [0.2052, 0.2049, 0.2072, 0.2056, 0.2065, 0.2072, 0.2083, 0.2099, 0.2100, 0.2104]
  val_value_value_mse : [0.2052, 0.2049, 0.2072, 0.2056, 0.2065, 0.2072, 0.2083, 0.2099, 0.2100, 0.2104]
  value_loss : [0.2046, 0.1997, 0.1946, 0.1886, 0.1854, 0.1826, 0.1795, 0.1777, 0.1760, 0.1750]
  value_value_mse : [0.2046, 0.1997, 0.1946, 0.1886, 0.1854, 0.1826, 0.1795, 0.1777, 0.1760, 0.1750]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:02:31.486968Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game28501_game30000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.257836   (epoch 10)   mode=min
  policy_loss: 2.169632   (epoch 10)   mode=min
  policy_policy_acc: 0.339036   (epoch 10)   mode=max
  policy_top10_acc: 0.859169   (epoch 10)   mode=max
  policy_top5_acc: 0.710545   (epoch 10)   mode=max
  val_loss: 2.692640   (epoch 4)   mode=min
  val_policy_loss: 2.590025   (epoch 4)   mode=min
  val_policy_policy_acc: 0.260739   (epoch 7)   mode=max
  val_policy_top10_acc: 0.763037   (epoch 4)   mode=max
  val_policy_top5_acc: 0.599101   (epoch 5)   mode=max
  val_value_loss: 0.204848   (epoch 1)   mode=min
  val_value_value_mse: 0.204869   (epoch 1)   mode=min
  value_loss: 0.176344   (epoch 10)   mode=min
  value_value_mse: 0.176328   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.7198, 2.6487, 2.5892, 2.4922, 2.4442, 2.4031, 2.3359, 2.3096, 2.2722, 2.2578]
  policy_loss : [2.6163, 2.5478, 2.4908, 2.3968, 2.3509, 2.3111, 2.2457, 2.2200, 2.1833, 2.1696]
  policy_policy_acc : [0.2508, 0.2636, 0.2740, 0.2939, 0.3025, 0.3106, 0.3228, 0.3279, 0.3368, 0.3390]
  policy_top10_acc : [0.7606, 0.7773, 0.7936, 0.8139, 0.8243, 0.8343, 0.8458, 0.8515, 0.8580, 0.8592]
  policy_top5_acc : [0.5865, 0.6078, 0.6236, 0.6509, 0.6635, 0.6734, 0.6912, 0.6969, 0.7073, 0.7105]
  val_loss : [2.6933, 2.6946, 2.7092, 2.6926, 2.7134, 2.7076, 2.7330, 2.7488, 2.7563, 2.7591]
  val_policy_loss : [2.5908, 2.5912, 2.6058, 2.5900, 2.6085, 2.6030, 2.6282, 2.6439, 2.6507, 2.6539]
  val_policy_policy_acc : [0.2558, 0.2588, 0.2535, 0.2597, 0.2599, 0.2587, 0.2607, 0.2581, 0.2574, 0.2594]
  val_policy_top10_acc : [0.7587, 0.7606, 0.7549, 0.7630, 0.7600, 0.7618, 0.7627, 0.7599, 0.7604, 0.7596]
  val_policy_top5_acc : [0.5928, 0.5988, 0.5870, 0.5990, 0.5991, 0.5989, 0.5951, 0.5947, 0.5939, 0.5952]
  val_value_loss : [0.2048, 0.2065, 0.2066, 0.2051, 0.2096, 0.2088, 0.2094, 0.2095, 0.2110, 0.2102]
  val_value_value_mse : [0.2049, 0.2065, 0.2067, 0.2052, 0.2096, 0.2089, 0.2094, 0.2095, 0.2110, 0.2103]
  value_loss : [0.2070, 0.2018, 0.1967, 0.1907, 0.1866, 0.1841, 0.1804, 0.1792, 0.1777, 0.1763]
  value_value_mse : [0.2070, 0.2018, 0.1967, 0.1907, 0.1866, 0.1840, 0.1804, 0.1792, 0.1777, 0.1763]

================================================================================

History file: model_versions/chess_elo_model_V3_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:09:46.947351Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game30001_game31500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V2
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.333130   (epoch 7)   mode=min
  policy_loss: 2.243116   (epoch 7)   mode=min
  policy_policy_acc: 0.322978   (epoch 7)   mode=max
  policy_top10_acc: 0.846617   (epoch 7)   mode=max
  policy_top5_acc: 0.692695   (epoch 7)   mode=max
  val_loss: 2.685603   (epoch 1)   mode=min
  val_policy_loss: 2.583047   (epoch 1)   mode=min
  val_policy_policy_acc: 0.259341   (epoch 6)   mode=max
  val_policy_top10_acc: 0.765934   (epoch 1)   mode=max
  val_policy_top5_acc: 0.602298   (epoch 7)   mode=max
  val_value_loss: 0.205070   (epoch 2)   mode=min
  val_value_value_mse: 0.205079   (epoch 2)   mode=min
  value_loss: 0.179872   (epoch 7)   mode=min
  value_value_mse: 0.179884   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.7068, 2.6344, 2.5750, 2.4750, 2.4284, 2.3644, 2.3331]
  policy_loss : [2.6051, 2.5355, 2.4779, 2.3809, 2.3362, 2.2735, 2.2431]
  policy_policy_acc : [0.2542, 0.2654, 0.2774, 0.2972, 0.3060, 0.3182, 0.3230]
  policy_top10_acc : [0.7635, 0.7819, 0.7976, 0.8187, 0.8298, 0.8425, 0.8466]
  policy_top5_acc : [0.5941, 0.6135, 0.6300, 0.6575, 0.6689, 0.6846, 0.6927]
  val_loss : [2.6856, 2.6906, 2.7152, 2.6974, 2.7137, 2.7222, 2.7333]
  val_policy_loss : [2.5830, 2.5879, 2.6110, 2.5935, 2.6104, 2.6188, 2.6301]
  val_policy_policy_acc : [0.2580, 0.2560, 0.2550, 0.2585, 0.2569, 0.2593, 0.2584]
  val_policy_top10_acc : [0.7659, 0.7642, 0.7585, 0.7659, 0.7600, 0.7592, 0.7574]
  val_policy_top5_acc : [0.5957, 0.5984, 0.5881, 0.6018, 0.5987, 0.5981, 0.6023]
  val_value_loss : [0.2051, 0.2051, 0.2083, 0.2077, 0.2065, 0.2066, 0.2063]
  val_value_value_mse : [0.2051, 0.2051, 0.2083, 0.2077, 0.2065, 0.2066, 0.2063]
  value_loss : [0.2036, 0.1981, 0.1940, 0.1882, 0.1847, 0.1818, 0.1799]
  value_value_mse : [0.2036, 0.1981, 0.1940, 0.1882, 0.1847, 0.1818, 0.1799]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:17:31.068942Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game31501_game33000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.338619   (epoch 7)   mode=min
  policy_loss: 2.246527   (epoch 7)   mode=min
  policy_policy_acc: 0.321800   (epoch 7)   mode=max
  policy_top10_acc: 0.844865   (epoch 7)   mode=max
  policy_top5_acc: 0.691432   (epoch 7)   mode=max
  val_loss: 2.664501   (epoch 1)   mode=min
  val_policy_loss: 2.560815   (epoch 1)   mode=min
  val_policy_policy_acc: 0.266034   (epoch 6)   mode=max
  val_policy_top10_acc: 0.770130   (epoch 1)   mode=max
  val_policy_top5_acc: 0.600699   (epoch 4)   mode=max
  val_value_loss: 0.206183   (epoch 2)   mode=min
  val_value_value_mse: 0.206213   (epoch 2)   mode=min
  value_loss: 0.184145   (epoch 7)   mode=min
  value_value_mse: 0.184149   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6972, 2.6300, 2.5729, 2.4756, 2.4273, 2.3672, 2.3386]
  policy_loss : [2.5935, 2.5284, 2.4738, 2.3793, 2.3332, 2.2745, 2.2465]
  policy_policy_acc : [0.2554, 0.2677, 0.2793, 0.2974, 0.3063, 0.3167, 0.3218]
  policy_top10_acc : [0.7635, 0.7820, 0.7972, 0.8171, 0.8265, 0.8379, 0.8449]
  policy_top5_acc : [0.5970, 0.6142, 0.6299, 0.6573, 0.6689, 0.6842, 0.6914]
  val_loss : [2.6645, 2.6899, 2.6918, 2.6750, 2.6901, 2.6932, 2.7044]
  val_policy_loss : [2.5608, 2.5867, 2.5867, 2.5708, 2.5859, 2.5892, 2.6000]
  val_policy_policy_acc : [0.2603, 0.2564, 0.2571, 0.2622, 0.2610, 0.2660, 0.2630]
  val_policy_top10_acc : [0.7701, 0.7603, 0.7673, 0.7676, 0.7666, 0.7683, 0.7682]
  val_policy_top5_acc : [0.5974, 0.5967, 0.5918, 0.6007, 0.5982, 0.5998, 0.5989]
  val_value_loss : [0.2073, 0.2062, 0.2098, 0.2081, 0.2081, 0.2076, 0.2085]
  val_value_value_mse : [0.2074, 0.2062, 0.2098, 0.2082, 0.2082, 0.2076, 0.2085]
  value_loss : [0.2071, 0.2030, 0.1980, 0.1924, 0.1883, 0.1854, 0.1841]
  value_value_mse : [0.2071, 0.2030, 0.1980, 0.1924, 0.1883, 0.1854, 0.1841]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:29:30.289560Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game33001_game34500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.291998   (epoch 8)   mode=min
  policy_loss: 2.204865   (epoch 8)   mode=min
  policy_policy_acc: 0.335346   (epoch 8)   mode=max
  policy_top10_acc: 0.851844   (epoch 8)   mode=max
  policy_top5_acc: 0.701547   (epoch 8)   mode=max
  val_loss: 2.672316   (epoch 2)   mode=min
  val_policy_loss: 2.569192   (epoch 2)   mode=min
  val_policy_policy_acc: 0.265435   (epoch 5)   mode=max
  val_policy_top10_acc: 0.768132   (epoch 7)   mode=max
  val_policy_top5_acc: 0.596903   (epoch 7)   mode=max
  val_value_loss: 0.206074   (epoch 2)   mode=min
  val_value_value_mse: 0.206091   (epoch 2)   mode=min
  value_loss: 0.174239   (epoch 8)   mode=min
  value_value_mse: 0.174240   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6902, 2.6232, 2.5668, 2.5180, 2.4238, 2.3797, 2.3200, 2.2920]
  policy_loss : [2.5900, 2.5257, 2.4714, 2.4237, 2.3325, 2.2902, 2.2317, 2.2049]
  policy_policy_acc : [0.2572, 0.2688, 0.2787, 0.2893, 0.3066, 0.3151, 0.3279, 0.3353]
  policy_top10_acc : [0.7643, 0.7809, 0.7963, 0.8074, 0.8268, 0.8358, 0.8461, 0.8518]
  policy_top5_acc : [0.5965, 0.6147, 0.6303, 0.6447, 0.6701, 0.6804, 0.6945, 0.7015]
  val_loss : [2.6772, 2.6723, 2.6814, 2.6978, 2.7006, 2.7242, 2.7304, 2.7366]
  val_policy_loss : [2.5724, 2.5692, 2.5758, 2.5916, 2.5967, 2.6185, 2.6242, 2.6310]
  val_policy_policy_acc : [0.2601, 0.2623, 0.2639, 0.2612, 0.2654, 0.2650, 0.2631, 0.2653]
  val_policy_top10_acc : [0.7645, 0.7637, 0.7662, 0.7593, 0.7635, 0.7619, 0.7681, 0.7669]
  val_policy_top5_acc : [0.5952, 0.5936, 0.5924, 0.5936, 0.5961, 0.5953, 0.5969, 0.5913]
  val_value_loss : [0.2093, 0.2061, 0.2110, 0.2123, 0.2076, 0.2112, 0.2123, 0.2110]
  val_value_value_mse : [0.2093, 0.2061, 0.2111, 0.2124, 0.2076, 0.2113, 0.2123, 0.2111]
  value_loss : [0.2003, 0.1951, 0.1908, 0.1886, 0.1825, 0.1789, 0.1766, 0.1742]
  value_value_mse : [0.2003, 0.1951, 0.1908, 0.1886, 0.1825, 0.1789, 0.1766, 0.1742]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:38:33.206033Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game34501_game36000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.328617   (epoch 7)   mode=min
  policy_loss: 2.237494   (epoch 7)   mode=min
  policy_policy_acc: 0.325926   (epoch 7)   mode=max
  policy_top10_acc: 0.847361   (epoch 7)   mode=max
  policy_top5_acc: 0.692600   (epoch 7)   mode=max
  val_loss: 2.664984   (epoch 1)   mode=min
  val_policy_loss: 2.559659   (epoch 1)   mode=min
  val_policy_policy_acc: 0.267333   (epoch 1)   mode=max
  val_policy_top10_acc: 0.771329   (epoch 4)   mode=max
  val_policy_top5_acc: 0.607393   (epoch 6)   mode=max
  val_value_loss: 0.206536   (epoch 6)   mode=min
  val_value_value_mse: 0.206569   (epoch 6)   mode=min
  value_loss: 0.182291   (epoch 7)   mode=min
  value_value_mse: 0.182287   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6768, 2.6114, 2.5522, 2.4596, 2.4148, 2.3541, 2.3286]
  policy_loss : [2.5739, 2.5102, 2.4534, 2.3634, 2.3209, 2.2623, 2.2375]
  policy_policy_acc : [0.2616, 0.2730, 0.2838, 0.3018, 0.3103, 0.3219, 0.3259]
  policy_top10_acc : [0.7716, 0.7883, 0.8021, 0.8220, 0.8301, 0.8420, 0.8474]
  policy_top5_acc : [0.6025, 0.6221, 0.6384, 0.6632, 0.6720, 0.6888, 0.6926]
  val_loss : [2.6650, 2.6780, 2.6904, 2.6698, 2.6916, 2.7102, 2.7043]
  val_policy_loss : [2.5597, 2.5726, 2.5838, 2.5664, 2.5871, 2.6068, 2.6003]
  val_policy_policy_acc : [0.2673, 0.2600, 0.2613, 0.2664, 0.2652, 0.2670, 0.2662]
  val_policy_top10_acc : [0.7693, 0.7694, 0.7614, 0.7713, 0.7673, 0.7661, 0.7673]
  val_policy_top5_acc : [0.6016, 0.5956, 0.5968, 0.6057, 0.5988, 0.6074, 0.6045]
  val_value_loss : [0.2106, 0.2108, 0.2130, 0.2065, 0.2087, 0.2065, 0.2077]
  val_value_value_mse : [0.2106, 0.2108, 0.2130, 0.2066, 0.2087, 0.2066, 0.2077]
  value_loss : [0.2056, 0.2023, 0.1975, 0.1924, 0.1877, 0.1836, 0.1823]
  value_value_mse : [0.2056, 0.2023, 0.1975, 0.1924, 0.1877, 0.1836, 0.1823]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:50:26.365238Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game36001_game37500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.296940   (epoch 8)   mode=min
  policy_loss: 2.209233   (epoch 8)   mode=min
  policy_policy_acc: 0.331690   (epoch 8)   mode=max
  policy_top10_acc: 0.850850   (epoch 8)   mode=max
  policy_top5_acc: 0.700191   (epoch 8)   mode=max
  val_loss: 2.670258   (epoch 2)   mode=min
  val_policy_loss: 2.567366   (epoch 2)   mode=min
  val_policy_policy_acc: 0.267333   (epoch 5)   mode=max
  val_policy_top10_acc: 0.766533   (epoch 5)   mode=max
  val_policy_top5_acc: 0.601998   (epoch 5)   mode=max
  val_value_loss: 0.205621   (epoch 2)   mode=min
  val_value_value_mse: 0.205632   (epoch 2)   mode=min
  value_loss: 0.175474   (epoch 8)   mode=min
  value_value_mse: 0.175475   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6894, 2.6275, 2.5709, 2.5218, 2.4248, 2.3857, 2.3240, 2.2969]
  policy_loss : [2.5869, 2.5271, 2.4730, 2.4263, 2.3327, 2.2955, 2.2350, 2.2092]
  policy_policy_acc : [0.2593, 0.2681, 0.2802, 0.2877, 0.3074, 0.3150, 0.3270, 0.3317]
  policy_top10_acc : [0.7662, 0.7835, 0.7971, 0.8085, 0.8277, 0.8348, 0.8463, 0.8508]
  policy_top5_acc : [0.5989, 0.6176, 0.6324, 0.6447, 0.6706, 0.6790, 0.6950, 0.7002]
  val_loss : [2.6906, 2.6703, 2.6926, 2.7052, 2.6865, 2.7066, 2.7246, 2.7330]
  val_policy_loss : [2.5830, 2.5674, 2.5872, 2.5989, 2.5812, 2.6002, 2.6187, 2.6258]
  val_policy_policy_acc : [0.2574, 0.2619, 0.2595, 0.2625, 0.2673, 0.2604, 0.2636, 0.2632]
  val_policy_top10_acc : [0.7610, 0.7624, 0.7608, 0.7645, 0.7665, 0.7607, 0.7658, 0.7647]
  val_policy_top5_acc : [0.5973, 0.5957, 0.5969, 0.5930, 0.6020, 0.5975, 0.5992, 0.5993]
  val_value_loss : [0.2151, 0.2056, 0.2106, 0.2123, 0.2104, 0.2126, 0.2116, 0.2141]
  val_value_value_mse : [0.2151, 0.2056, 0.2106, 0.2124, 0.2104, 0.2126, 0.2117, 0.2141]
  value_loss : [0.2050, 0.2006, 0.1959, 0.1910, 0.1844, 0.1806, 0.1779, 0.1755]
  value_value_mse : [0.2050, 0.2006, 0.1959, 0.1910, 0.1844, 0.1806, 0.1779, 0.1755]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T19:59:50.544798Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game37501_game39000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.289777   (epoch 8)   mode=min
  policy_loss: 2.202228   (epoch 8)   mode=min
  policy_policy_acc: 0.336587   (epoch 8)   mode=max
  policy_top10_acc: 0.851067   (epoch 8)   mode=max
  policy_top5_acc: 0.703647   (epoch 8)   mode=max
  val_loss: 2.669323   (epoch 2)   mode=min
  val_policy_loss: 2.566827   (epoch 2)   mode=min
  val_policy_policy_acc: 0.266334   (epoch 5)   mode=max
  val_policy_top10_acc: 0.770629   (epoch 6)   mode=max
  val_policy_top5_acc: 0.605095   (epoch 5)   mode=max
  val_value_loss: 0.204748   (epoch 2)   mode=min
  val_value_value_mse: 0.204755   (epoch 2)   mode=min
  value_loss: 0.175076   (epoch 8)   mode=min
  value_value_mse: 0.175077   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6829, 2.6176, 2.5584, 2.5079, 2.4152, 2.3751, 2.3147, 2.2898]
  policy_loss : [2.5813, 2.5190, 2.4622, 2.4138, 2.3238, 2.2856, 2.2261, 2.2022]
  policy_policy_acc : [0.2620, 0.2737, 0.2834, 0.2927, 0.3127, 0.3193, 0.3309, 0.3366]
  policy_top10_acc : [0.7667, 0.7820, 0.7964, 0.8088, 0.8277, 0.8353, 0.8470, 0.8511]
  policy_top5_acc : [0.6005, 0.6191, 0.6357, 0.6477, 0.6719, 0.6825, 0.6962, 0.7036]
  val_loss : [2.6822, 2.6693, 2.6873, 2.6864, 2.6972, 2.7094, 2.7210, 2.7311]
  val_policy_loss : [2.5775, 2.5668, 2.5814, 2.5820, 2.5901, 2.6057, 2.6165, 2.6256]
  val_policy_policy_acc : [0.2584, 0.2620, 0.2605, 0.2613, 0.2663, 0.2648, 0.2661, 0.2629]
  val_policy_top10_acc : [0.7614, 0.7689, 0.7639, 0.7653, 0.7705, 0.7706, 0.7691, 0.7685]
  val_policy_top5_acc : [0.5924, 0.6009, 0.5969, 0.5993, 0.6051, 0.6028, 0.6005, 0.5989]
  val_value_loss : [0.2092, 0.2047, 0.2115, 0.2086, 0.2139, 0.2070, 0.2085, 0.2107]
  val_value_value_mse : [0.2092, 0.2048, 0.2115, 0.2087, 0.2139, 0.2070, 0.2085, 0.2108]
  value_loss : [0.2034, 0.1973, 0.1924, 0.1882, 0.1827, 0.1791, 0.1771, 0.1751]
  value_value_mse : [0.2034, 0.1973, 0.1924, 0.1882, 0.1827, 0.1791, 0.1771, 0.1751]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:08:25.983025Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game39001_game40500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.355014   (epoch 7)   mode=min
  policy_loss: 2.263195   (epoch 7)   mode=min
  policy_policy_acc: 0.324375   (epoch 7)   mode=max
  policy_top10_acc: 0.840662   (epoch 7)   mode=max
  policy_top5_acc: 0.687171   (epoch 7)   mode=max
  val_loss: 2.668291   (epoch 1)   mode=min
  val_policy_loss: 2.558397   (epoch 1)   mode=min
  val_policy_policy_acc: 0.268731   (epoch 5)   mode=max
  val_policy_top10_acc: 0.767233   (epoch 7)   mode=max
  val_policy_top5_acc: 0.600699   (epoch 6)   mode=max
  val_value_loss: 0.205144   (epoch 2)   mode=min
  val_value_value_mse: 0.205149   (epoch 2)   mode=min
  value_loss: 0.183632   (epoch 7)   mode=min
  value_value_mse: 0.183636   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6876, 2.6242, 2.5706, 2.4780, 2.4368, 2.3792, 2.3550]
  policy_loss : [2.5850, 2.5239, 2.4717, 2.3825, 2.3426, 2.2865, 2.2632]
  policy_policy_acc : [0.2620, 0.2731, 0.2824, 0.3017, 0.3072, 0.3189, 0.3244]
  policy_top10_acc : [0.7666, 0.7820, 0.7960, 0.8148, 0.8235, 0.8351, 0.8407]
  policy_top5_acc : [0.5972, 0.6153, 0.6311, 0.6551, 0.6662, 0.6801, 0.6872]
  val_loss : [2.6683, 2.6724, 2.6828, 2.6808, 2.6849, 2.6929, 2.7016]
  val_policy_loss : [2.5584, 2.5696, 2.5780, 2.5756, 2.5780, 2.5883, 2.5962]
  val_policy_policy_acc : [0.2664, 0.2651, 0.2612, 0.2668, 0.2687, 0.2680, 0.2665]
  val_policy_top10_acc : [0.7637, 0.7638, 0.7652, 0.7655, 0.7613, 0.7657, 0.7672]
  val_policy_top5_acc : [0.6005, 0.6003, 0.5969, 0.5997, 0.5999, 0.6007, 0.6006]
  val_value_loss : [0.2196, 0.2051, 0.2093, 0.2102, 0.2134, 0.2088, 0.2104]
  val_value_value_mse : [0.2196, 0.2051, 0.2093, 0.2102, 0.2134, 0.2088, 0.2105]
  value_loss : [0.2053, 0.2005, 0.1978, 0.1911, 0.1885, 0.1855, 0.1836]
  value_value_mse : [0.2053, 0.2005, 0.1978, 0.1911, 0.1885, 0.1855, 0.1836]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:19:02.236109Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game40501_game42000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.261209   (epoch 9)   mode=min
  policy_loss: 2.174571   (epoch 9)   mode=min
  policy_policy_acc: 0.342764   (epoch 9)   mode=max
  policy_top10_acc: 0.856899   (epoch 9)   mode=max
  policy_top5_acc: 0.708469   (epoch 9)   mode=max
  val_loss: 2.664086   (epoch 2)   mode=min
  val_policy_loss: 2.561217   (epoch 3)   mode=min
  val_policy_policy_acc: 0.272827   (epoch 6)   mode=max
  val_policy_top10_acc: 0.768432   (epoch 1)   mode=max
  val_policy_top5_acc: 0.608891   (epoch 7)   mode=max
  val_value_loss: 0.205418   (epoch 2)   mode=min
  val_value_value_mse: 0.205433   (epoch 2)   mode=min
  value_loss: 0.173327   (epoch 9)   mode=min
  value_value_mse: 0.173331   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6695, 2.6108, 2.5524, 2.5077, 2.4147, 2.3733, 2.3158, 2.2922, 2.2612]
  policy_loss : [2.5692, 2.5134, 2.4569, 2.4143, 2.3236, 2.2839, 2.2277, 2.2050, 2.1746]
  policy_policy_acc : [0.2616, 0.2733, 0.2845, 0.2930, 0.3108, 0.3195, 0.3292, 0.3355, 0.3428]
  policy_top10_acc : [0.7717, 0.7858, 0.8000, 0.8109, 0.8278, 0.8373, 0.8474, 0.8520, 0.8569]
  policy_top5_acc : [0.6026, 0.6190, 0.6361, 0.6475, 0.6720, 0.6832, 0.6946, 0.7014, 0.7085]
  val_loss : [2.6660, 2.6641, 2.6669, 2.6911, 2.6797, 2.6834, 2.6997, 2.7232, 2.7161]
  val_policy_loss : [2.5630, 2.5613, 2.5612, 2.5871, 2.5768, 2.5797, 2.5953, 2.6192, 2.6116]
  val_policy_policy_acc : [0.2623, 0.2672, 0.2682, 0.2669, 0.2701, 0.2728, 0.2705, 0.2670, 0.2685]
  val_policy_top10_acc : [0.7684, 0.7667, 0.7664, 0.7622, 0.7659, 0.7650, 0.7668, 0.7629, 0.7674]
  val_policy_top5_acc : [0.6051, 0.6019, 0.6045, 0.5987, 0.6080, 0.6051, 0.6089, 0.6019, 0.6052]
  val_value_loss : [0.2059, 0.2054, 0.2112, 0.2077, 0.2055, 0.2072, 0.2085, 0.2076, 0.2086]
  val_value_value_mse : [0.2059, 0.2054, 0.2112, 0.2077, 0.2055, 0.2072, 0.2085, 0.2076, 0.2086]
  value_loss : [0.2005, 0.1948, 0.1911, 0.1868, 0.1821, 0.1788, 0.1764, 0.1745, 0.1733]
  value_value_mse : [0.2005, 0.1948, 0.1911, 0.1868, 0.1821, 0.1788, 0.1764, 0.1744, 0.1733]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:27:28.413407Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game42001_game43500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.355630   (epoch 7)   mode=min
  policy_loss: 2.263082   (epoch 7)   mode=min
  policy_policy_acc: 0.321056   (epoch 7)   mode=max
  policy_top10_acc: 0.840200   (epoch 7)   mode=max
  policy_top5_acc: 0.687289   (epoch 7)   mode=max
  val_loss: 2.650691   (epoch 1)   mode=min
  val_policy_loss: 2.545838   (epoch 1)   mode=min
  val_policy_policy_acc: 0.271429   (epoch 2)   mode=max
  val_policy_top10_acc: 0.770929   (epoch 6)   mode=max
  val_policy_top5_acc: 0.608691   (epoch 3)   mode=max
  val_value_loss: 0.204860   (epoch 3)   mode=min
  val_value_value_mse: 0.204879   (epoch 3)   mode=min
  value_loss: 0.185344   (epoch 7)   mode=min
  value_value_mse: 0.185332   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6860, 2.6189, 2.5684, 2.4794, 2.4379, 2.3837, 2.3556]
  policy_loss : [2.5829, 2.5183, 2.4693, 2.3832, 2.3430, 2.2902, 2.2631]
  policy_policy_acc : [0.2597, 0.2713, 0.2809, 0.3009, 0.3063, 0.3184, 0.3211]
  policy_top10_acc : [0.7665, 0.7833, 0.7966, 0.8154, 0.8239, 0.8336, 0.8402]
  policy_top5_acc : [0.5989, 0.6183, 0.6320, 0.6560, 0.6655, 0.6797, 0.6873]
  val_loss : [2.6507, 2.6551, 2.6706, 2.6653, 2.6781, 2.6721, 2.6895]
  val_policy_loss : [2.5458, 2.5520, 2.5681, 2.5620, 2.5754, 2.5696, 2.5856]
  val_policy_policy_acc : [0.2680, 0.2714, 0.2642, 0.2698, 0.2685, 0.2709, 0.2693]
  val_policy_top10_acc : [0.7692, 0.7684, 0.7676, 0.7666, 0.7675, 0.7709, 0.7700]
  val_policy_top5_acc : [0.6055, 0.6045, 0.6087, 0.6040, 0.6055, 0.6059, 0.6060]
  val_value_loss : [0.2095, 0.2061, 0.2049, 0.2066, 0.2055, 0.2050, 0.2077]
  val_value_value_mse : [0.2096, 0.2061, 0.2049, 0.2066, 0.2055, 0.2050, 0.2077]
  value_loss : [0.2063, 0.2010, 0.1980, 0.1925, 0.1901, 0.1869, 0.1853]
  value_value_mse : [0.2063, 0.2009, 0.1980, 0.1925, 0.1901, 0.1869, 0.1853]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:37:12.762927Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game43501_game45000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.299444   (epoch 8)   mode=min
  policy_loss: 2.209674   (epoch 8)   mode=min
  policy_policy_acc: 0.333773   (epoch 8)   mode=max
  policy_top10_acc: 0.852016   (epoch 8)   mode=max
  policy_top5_acc: 0.700585   (epoch 8)   mode=max
  val_loss: 2.655941   (epoch 1)   mode=min
  val_policy_loss: 2.551713   (epoch 2)   mode=min
  val_policy_policy_acc: 0.270430   (epoch 2)   mode=max
  val_policy_top10_acc: 0.772727   (epoch 5)   mode=max
  val_policy_top5_acc: 0.611289   (epoch 3)   mode=max
  val_value_loss: 0.206403   (epoch 1)   mode=min
  val_value_value_mse: 0.206415   (epoch 1)   mode=min
  value_loss: 0.179490   (epoch 8)   mode=min
  value_value_mse: 0.179482   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6622, 2.6028, 2.5511, 2.5051, 2.4160, 2.3763, 2.3202, 2.2994]
  policy_loss : [2.5597, 2.5027, 2.4534, 2.4087, 2.3225, 2.2842, 2.2296, 2.2097]
  policy_policy_acc : [0.2649, 0.2758, 0.2847, 0.2933, 0.3117, 0.3192, 0.3299, 0.3338]
  policy_top10_acc : [0.7741, 0.7892, 0.8023, 0.8117, 0.8292, 0.8374, 0.8476, 0.8520]
  policy_top5_acc : [0.6051, 0.6230, 0.6377, 0.6495, 0.6724, 0.6807, 0.6937, 0.7006]
  val_loss : [2.6559, 2.6574, 2.6581, 2.6754, 2.6665, 2.6791, 2.6931, 2.7083]
  val_policy_loss : [2.5528, 2.5517, 2.5530, 2.5713, 2.5620, 2.5742, 2.5895, 2.6041]
  val_policy_policy_acc : [0.2654, 0.2704, 0.2687, 0.2617, 0.2689, 0.2665, 0.2685, 0.2676]
  val_policy_top10_acc : [0.7701, 0.7706, 0.7696, 0.7686, 0.7727, 0.7687, 0.7693, 0.7691]
  val_policy_top5_acc : [0.6002, 0.6058, 0.6113, 0.6031, 0.6070, 0.6080, 0.6081, 0.6039]
  val_value_loss : [0.2064, 0.2112, 0.2101, 0.2082, 0.2087, 0.2095, 0.2070, 0.2082]
  val_value_value_mse : [0.2064, 0.2112, 0.2101, 0.2082, 0.2088, 0.2095, 0.2070, 0.2082]
  value_loss : [0.2048, 0.2001, 0.1954, 0.1928, 0.1869, 0.1841, 0.1810, 0.1795]
  value_value_mse : [0.2048, 0.2001, 0.1954, 0.1928, 0.1869, 0.1841, 0.1810, 0.1795]

================================================================================

History file: model_versions/chess_elo_model_V4_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:45:49.194474Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game45001_game46500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V3
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.309078   (epoch 8)   mode=min
  policy_loss: 2.221308   (epoch 8)   mode=min
  policy_policy_acc: 0.332159   (epoch 8)   mode=max
  policy_top10_acc: 0.847457   (epoch 8)   mode=max
  policy_top5_acc: 0.696945   (epoch 8)   mode=max
  val_loss: 2.643064   (epoch 2)   mode=min
  val_policy_loss: 2.540958   (epoch 2)   mode=min
  val_policy_policy_acc: 0.268432   (epoch 8)   mode=max
  val_policy_top10_acc: 0.770230   (epoch 2)   mode=max
  val_policy_top5_acc: 0.611289   (epoch 7)   mode=max
  val_value_loss: 0.203640   (epoch 1)   mode=min
  val_value_value_mse: 0.203650   (epoch 1)   mode=min
  value_loss: 0.175483   (epoch 8)   mode=min
  value_value_mse: 0.175466   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6676, 2.6080, 2.5571, 2.5118, 2.4246, 2.3868, 2.3289, 2.3091]
  policy_loss : [2.5682, 2.5109, 2.4620, 2.4180, 2.3335, 2.2973, 2.2404, 2.2213]
  policy_policy_acc : [0.2623, 0.2727, 0.2832, 0.2931, 0.3095, 0.3171, 0.3285, 0.3322]
  policy_top10_acc : [0.7709, 0.7869, 0.7984, 0.8079, 0.8251, 0.8346, 0.8432, 0.8475]
  policy_top5_acc : [0.6026, 0.6205, 0.6331, 0.6473, 0.6683, 0.6788, 0.6925, 0.6969]
  val_loss : [2.6527, 2.6431, 2.6559, 2.6688, 2.6762, 2.6922, 2.7000, 2.7024]
  val_policy_loss : [2.5509, 2.5410, 2.5533, 2.5648, 2.5730, 2.5883, 2.5964, 2.5986]
  val_policy_policy_acc : [0.2659, 0.2681, 0.2677, 0.2632, 0.2633, 0.2624, 0.2669, 0.2684]
  val_policy_top10_acc : [0.7667, 0.7702, 0.7689, 0.7690, 0.7682, 0.7668, 0.7687, 0.7680]
  val_policy_top5_acc : [0.6076, 0.6083, 0.6006, 0.6023, 0.6051, 0.6048, 0.6113, 0.6061]
  val_value_loss : [0.2036, 0.2040, 0.2049, 0.2078, 0.2062, 0.2075, 0.2069, 0.2075]
  val_value_value_mse : [0.2036, 0.2040, 0.2049, 0.2078, 0.2062, 0.2075, 0.2070, 0.2075]
  value_loss : [0.1988, 0.1941, 0.1904, 0.1875, 0.1822, 0.1789, 0.1772, 0.1755]
  value_value_mse : [0.1988, 0.1941, 0.1904, 0.1875, 0.1822, 0.1789, 0.1771, 0.1755]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T20:53:45.086246Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game46501_game48000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.317976   (epoch 8)   mode=min
  policy_loss: 2.226520   (epoch 8)   mode=min
  policy_policy_acc: 0.332490   (epoch 8)   mode=max
  policy_top10_acc: 0.846296   (epoch 8)   mode=max
  policy_top5_acc: 0.696647   (epoch 8)   mode=max
  val_loss: 2.648184   (epoch 2)   mode=min
  val_policy_loss: 2.545439   (epoch 2)   mode=min
  val_policy_policy_acc: 0.276523   (epoch 6)   mode=max
  val_policy_top10_acc: 0.771029   (epoch 2)   mode=max
  val_policy_top5_acc: 0.606593   (epoch 1)   mode=max
  val_value_loss: 0.205313   (epoch 2)   mode=min
  val_value_value_mse: 0.205330   (epoch 2)   mode=min
  value_loss: 0.182951   (epoch 8)   mode=min
  value_value_mse: 0.182921   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6738, 2.6187, 2.5674, 2.5207, 2.4341, 2.3935, 2.3400, 2.3180]
  policy_loss : [2.5718, 2.5180, 2.4679, 2.4234, 2.3393, 2.3006, 2.2477, 2.2265]
  policy_policy_acc : [0.2628, 0.2753, 0.2833, 0.2928, 0.3110, 0.3175, 0.3292, 0.3325]
  policy_top10_acc : [0.7682, 0.7816, 0.7970, 0.8067, 0.8236, 0.8331, 0.8415, 0.8463]
  policy_top5_acc : [0.6039, 0.6194, 0.6339, 0.6457, 0.6691, 0.6785, 0.6900, 0.6966]
  val_loss : [2.6542, 2.6482, 2.6759, 2.6686, 2.6656, 2.6752, 2.6837, 2.6929]
  val_policy_loss : [2.5507, 2.5454, 2.5713, 2.5655, 2.5619, 2.5713, 2.5802, 2.5890]
  val_policy_policy_acc : [0.2670, 0.2696, 0.2668, 0.2691, 0.2726, 0.2765, 0.2754, 0.2724]
  val_policy_top10_acc : [0.7704, 0.7710, 0.7616, 0.7690, 0.7709, 0.7693, 0.7689, 0.7706]
  val_policy_top5_acc : [0.6066, 0.6024, 0.5943, 0.6023, 0.5989, 0.6040, 0.6053, 0.6049]
  val_value_loss : [0.2067, 0.2053, 0.2089, 0.2061, 0.2072, 0.2075, 0.2067, 0.2074]
  val_value_value_mse : [0.2068, 0.2053, 0.2089, 0.2061, 0.2072, 0.2076, 0.2068, 0.2074]
  value_loss : [0.2049, 0.2021, 0.1986, 0.1943, 0.1898, 0.1865, 0.1846, 0.1830]
  value_value_mse : [0.2049, 0.2022, 0.1986, 0.1943, 0.1899, 0.1865, 0.1845, 0.1829]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:02:52.240931Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game48001_game49500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.267980   (epoch 9)   mode=min
  policy_loss: 2.174045   (epoch 9)   mode=min
  policy_policy_acc: 0.341298   (epoch 9)   mode=max
  policy_top10_acc: 0.857335   (epoch 9)   mode=max
  policy_top5_acc: 0.710065   (epoch 9)   mode=max
  val_loss: 2.648704   (epoch 3)   mode=min
  val_policy_loss: 2.543013   (epoch 3)   mode=min
  val_policy_policy_acc: 0.272328   (epoch 3)   mode=max
  val_policy_top10_acc: 0.774426   (epoch 3)   mode=max
  val_policy_top5_acc: 0.613387   (epoch 3)   mode=max
  val_value_loss: 0.210033   (epoch 4)   mode=min
  val_value_value_mse: 0.210045   (epoch 4)   mode=min
  value_loss: 0.187687   (epoch 9)   mode=min
  value_value_mse: 0.187687   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6609, 2.6034, 2.5519, 2.5079, 2.4684, 2.3815, 2.3411, 2.2885, 2.2680]
  policy_loss : [2.5550, 2.4994, 2.4494, 2.4071, 2.3692, 2.2845, 2.2455, 2.1942, 2.1740]
  policy_policy_acc : [0.2649, 0.2750, 0.2863, 0.2938, 0.3008, 0.3188, 0.3263, 0.3375, 0.3413]
  policy_top10_acc : [0.7738, 0.7871, 0.8014, 0.8106, 0.8196, 0.8364, 0.8444, 0.8525, 0.8573]
  policy_top5_acc : [0.6057, 0.6222, 0.6384, 0.6485, 0.6600, 0.6819, 0.6928, 0.7026, 0.7101]
  val_loss : [2.6568, 2.6600, 2.6487, 2.6870, 2.6857, 2.6860, 2.7052, 2.7111, 2.7189]
  val_policy_loss : [2.5511, 2.5487, 2.5430, 2.5818, 2.5794, 2.5808, 2.5999, 2.6052, 2.6135]
  val_policy_policy_acc : [0.2707, 0.2690, 0.2723, 0.2698, 0.2675, 0.2706, 0.2667, 0.2710, 0.2694]
  val_policy_top10_acc : [0.7738, 0.7628, 0.7744, 0.7651, 0.7646, 0.7686, 0.7678, 0.7696, 0.7675]
  val_policy_top5_acc : [0.6043, 0.6049, 0.6134, 0.6038, 0.6085, 0.6085, 0.6062, 0.6089, 0.6055]
  val_value_loss : [0.2112, 0.2225, 0.2113, 0.2100, 0.2126, 0.2102, 0.2103, 0.2115, 0.2105]
  val_value_value_mse : [0.2112, 0.2226, 0.2113, 0.2100, 0.2126, 0.2102, 0.2103, 0.2116, 0.2105]
  value_loss : [0.2117, 0.2082, 0.2051, 0.2016, 0.1985, 0.1939, 0.1912, 0.1887, 0.1877]
  value_value_mse : [0.2117, 0.2082, 0.2051, 0.2016, 0.1985, 0.1939, 0.1912, 0.1887, 0.1877]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:11:31.602945Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game49501_game51000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.331095   (epoch 8)   mode=min
  policy_loss: 2.238527   (epoch 8)   mode=min
  policy_policy_acc: 0.329448   (epoch 8)   mode=max
  policy_top10_acc: 0.842951   (epoch 8)   mode=max
  policy_top5_acc: 0.692661   (epoch 8)   mode=max
  val_loss: 2.651703   (epoch 2)   mode=min
  val_policy_loss: 2.548161   (epoch 2)   mode=min
  val_policy_policy_acc: 0.275425   (epoch 7)   mode=max
  val_policy_top10_acc: 0.773427   (epoch 8)   mode=max
  val_policy_top5_acc: 0.613686   (epoch 7)   mode=max
  val_value_loss: 0.205239   (epoch 5)   mode=min
  val_value_value_mse: 0.205261   (epoch 5)   mode=min
  value_loss: 0.185126   (epoch 8)   mode=min
  value_value_mse: 0.185128   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6825, 2.6226, 2.5696, 2.5275, 2.4422, 2.4061, 2.3519, 2.3311]
  policy_loss : [2.5779, 2.5206, 2.4694, 2.4293, 2.3464, 2.3117, 2.2585, 2.2385]
  policy_policy_acc : [0.2597, 0.2718, 0.2831, 0.2902, 0.3068, 0.3136, 0.3253, 0.3294]
  policy_top10_acc : [0.7670, 0.7831, 0.7950, 0.8041, 0.8219, 0.8294, 0.8402, 0.8430]
  policy_top5_acc : [0.6016, 0.6175, 0.6325, 0.6426, 0.6648, 0.6736, 0.6866, 0.6927]
  val_loss : [2.6626, 2.6517, 2.6686, 2.6812, 2.6593, 2.6884, 2.6760, 2.6880]
  val_policy_loss : [2.5567, 2.5482, 2.5647, 2.5737, 2.5566, 2.5831, 2.5709, 2.5830]
  val_policy_policy_acc : [0.2682, 0.2706, 0.2651, 0.2699, 0.2722, 0.2709, 0.2754, 0.2735]
  val_policy_top10_acc : [0.7635, 0.7683, 0.7651, 0.7682, 0.7730, 0.7704, 0.7732, 0.7734]
  val_policy_top5_acc : [0.6011, 0.6086, 0.6061, 0.6081, 0.6130, 0.6119, 0.6137, 0.6126]
  val_value_loss : [0.2116, 0.2070, 0.2077, 0.2151, 0.2052, 0.2105, 0.2100, 0.2098]
  val_value_value_mse : [0.2116, 0.2070, 0.2077, 0.2151, 0.2053, 0.2105, 0.2100, 0.2098]
  value_loss : [0.2091, 0.2040, 0.2005, 0.1963, 0.1917, 0.1887, 0.1868, 0.1851]
  value_value_mse : [0.2091, 0.2040, 0.2005, 0.1963, 0.1917, 0.1887, 0.1868, 0.1851]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:21:58.938803Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game51001_game52500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.320524   (epoch 10)   mode=min
  policy_loss: 2.227092   (epoch 10)   mode=min
  policy_policy_acc: 0.330862   (epoch 10)   mode=max
  policy_top10_acc: 0.844340   (epoch 10)   mode=max
  policy_top5_acc: 0.696065   (epoch 10)   mode=max
  val_loss: 2.640209   (epoch 4)   mode=min
  val_policy_loss: 2.537230   (epoch 4)   mode=min
  val_policy_policy_acc: 0.274825   (epoch 9)   mode=max
  val_policy_top10_acc: 0.777523   (epoch 4)   mode=max
  val_policy_top5_acc: 0.613187   (epoch 6)   mode=max
  val_value_loss: 0.205320   (epoch 2)   mode=min
  val_value_value_mse: 0.205339   (epoch 2)   mode=min
  value_loss: 0.186689   (epoch 9)   mode=min
  value_value_mse: 0.186672   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6942, 2.6360, 2.5880, 2.5043, 2.4663, 2.4343, 2.3860, 2.3584, 2.3298, 2.3205]
  policy_loss : [2.5882, 2.5324, 2.4862, 2.4051, 2.3688, 2.3380, 2.2909, 2.2645, 2.2364, 2.2271]
  policy_policy_acc : [0.2595, 0.2704, 0.2783, 0.2948, 0.3012, 0.3098, 0.3181, 0.3219, 0.3302, 0.3309]
  policy_top10_acc : [0.7653, 0.7786, 0.7906, 0.8084, 0.8165, 0.8234, 0.8322, 0.8370, 0.8414, 0.8443]
  policy_top5_acc : [0.5958, 0.6146, 0.6270, 0.6483, 0.6591, 0.6671, 0.6794, 0.6843, 0.6911, 0.6961]
  val_loss : [2.6438, 2.6468, 2.6460, 2.6402, 2.6493, 2.6527, 2.6552, 2.6624, 2.6699, 2.6789]
  val_policy_loss : [2.5398, 2.5441, 2.5422, 2.5372, 2.5439, 2.5485, 2.5508, 2.5591, 2.5662, 2.5736]
  val_policy_policy_acc : [0.2739, 0.2673, 0.2702, 0.2706, 0.2738, 0.2741, 0.2730, 0.2739, 0.2748, 0.2726]
  val_policy_top10_acc : [0.7723, 0.7730, 0.7721, 0.7775, 0.7740, 0.7763, 0.7758, 0.7762, 0.7741, 0.7751]
  val_policy_top5_acc : [0.6102, 0.6067, 0.6080, 0.6112, 0.6119, 0.6132, 0.6102, 0.6112, 0.6125, 0.6104]
  val_value_loss : [0.2079, 0.2053, 0.2074, 0.2057, 0.2104, 0.2082, 0.2084, 0.2064, 0.2071, 0.2102]
  val_value_value_mse : [0.2079, 0.2053, 0.2074, 0.2057, 0.2104, 0.2082, 0.2084, 0.2064, 0.2071, 0.2102]
  value_loss : [0.2121, 0.2074, 0.2036, 0.1984, 0.1951, 0.1927, 0.1903, 0.1880, 0.1867, 0.1871]
  value_value_mse : [0.2121, 0.2074, 0.2036, 0.1984, 0.1951, 0.1927, 0.1903, 0.1880, 0.1867, 0.1871]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:30:17.753099Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game52501_game54000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.339644   (epoch 8)   mode=min
  policy_loss: 2.245152   (epoch 8)   mode=min
  policy_policy_acc: 0.328516   (epoch 8)   mode=max
  policy_top10_acc: 0.842835   (epoch 8)   mode=max
  policy_top5_acc: 0.691155   (epoch 8)   mode=max
  val_loss: 2.639441   (epoch 2)   mode=min
  val_policy_loss: 2.537448   (epoch 2)   mode=min
  val_policy_policy_acc: 0.274625   (epoch 6)   mode=max
  val_policy_top10_acc: 0.774426   (epoch 2)   mode=max
  val_policy_top5_acc: 0.612587   (epoch 6)   mode=max
  val_value_loss: 0.203871   (epoch 5)   mode=min
  val_value_value_mse: 0.203893   (epoch 5)   mode=min
  value_loss: 0.189006   (epoch 8)   mode=min
  value_value_mse: 0.189006   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6867, 2.6261, 2.5757, 2.5332, 2.4504, 2.4126, 2.3617, 2.3396]
  policy_loss : [2.5803, 2.5223, 2.4737, 2.4326, 2.3524, 2.3161, 2.2662, 2.2452]
  policy_policy_acc : [0.2622, 0.2734, 0.2825, 0.2915, 0.3071, 0.3143, 0.3241, 0.3285]
  policy_top10_acc : [0.7664, 0.7825, 0.7953, 0.8058, 0.8218, 0.8300, 0.8392, 0.8428]
  policy_top5_acc : [0.6005, 0.6165, 0.6315, 0.6434, 0.6649, 0.6747, 0.6868, 0.6912]
  val_loss : [2.6573, 2.6394, 2.6752, 2.7027, 2.6567, 2.6675, 2.6806, 2.6824]
  val_policy_loss : [2.5538, 2.5374, 2.5718, 2.5962, 2.5547, 2.5651, 2.5778, 2.5796]
  val_policy_policy_acc : [0.2692, 0.2679, 0.2684, 0.2660, 0.2731, 0.2746, 0.2724, 0.2729]
  val_policy_top10_acc : [0.7701, 0.7744, 0.7707, 0.7701, 0.7744, 0.7742, 0.7738, 0.7730]
  val_policy_top5_acc : [0.6049, 0.6067, 0.6086, 0.5994, 0.6109, 0.6126, 0.6110, 0.6103]
  val_value_loss : [0.2069, 0.2040, 0.2068, 0.2128, 0.2039, 0.2048, 0.2054, 0.2056]
  val_value_value_mse : [0.2069, 0.2040, 0.2068, 0.2129, 0.2039, 0.2048, 0.2055, 0.2056]
  value_loss : [0.2128, 0.2076, 0.2039, 0.2012, 0.1959, 0.1929, 0.1910, 0.1890]
  value_value_mse : [0.2128, 0.2076, 0.2039, 0.2012, 0.1959, 0.1929, 0.1910, 0.1890]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:38:30.517906Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game54001_game55500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.364498   (epoch 8)   mode=min
  policy_loss: 2.269142   (epoch 8)   mode=min
  policy_policy_acc: 0.322592   (epoch 8)   mode=max
  policy_top10_acc: 0.838680   (epoch 8)   mode=max
  policy_top5_acc: 0.684586   (epoch 8)   mode=max
  val_loss: 2.640823   (epoch 2)   mode=min
  val_policy_loss: 2.538043   (epoch 2)   mode=min
  val_policy_policy_acc: 0.274026   (epoch 5)   mode=max
  val_policy_top10_acc: 0.773227   (epoch 1)   mode=max
  val_policy_top5_acc: 0.610290   (epoch 2)   mode=max
  val_value_loss: 0.202736   (epoch 1)   mode=min
  val_value_value_mse: 0.202748   (epoch 1)   mode=min
  value_loss: 0.190734   (epoch 8)   mode=min
  value_value_mse: 0.190733   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6957, 2.6393, 2.5911, 2.5474, 2.4701, 2.4343, 2.3825, 2.3645]
  policy_loss : [2.5898, 2.5355, 2.4893, 2.4468, 2.3716, 2.3370, 2.2863, 2.2691]
  policy_policy_acc : [0.2595, 0.2691, 0.2784, 0.2856, 0.3015, 0.3069, 0.3176, 0.3226]
  policy_top10_acc : [0.7640, 0.7802, 0.7901, 0.8008, 0.8180, 0.8252, 0.8347, 0.8387]
  policy_top5_acc : [0.5980, 0.6131, 0.6254, 0.6372, 0.6575, 0.6659, 0.6803, 0.6846]
  val_loss : [2.6408, 2.6408, 2.6580, 2.6770, 2.6575, 2.6686, 2.6771, 2.6899]
  val_policy_loss : [2.5394, 2.5380, 2.5562, 2.5748, 2.5559, 2.5668, 2.5749, 2.5881]
  val_policy_policy_acc : [0.2694, 0.2663, 0.2690, 0.2692, 0.2740, 0.2701, 0.2711, 0.2707]
  val_policy_top10_acc : [0.7732, 0.7732, 0.7705, 0.7672, 0.7717, 0.7706, 0.7685, 0.7661]
  val_policy_top5_acc : [0.6057, 0.6103, 0.6074, 0.6076, 0.6084, 0.6080, 0.6075, 0.6064]
  val_value_loss : [0.2027, 0.2055, 0.2034, 0.2043, 0.2029, 0.2032, 0.2042, 0.2034]
  val_value_value_mse : [0.2027, 0.2055, 0.2035, 0.2043, 0.2029, 0.2032, 0.2042, 0.2034]
  value_loss : [0.2117, 0.2075, 0.2037, 0.2013, 0.1970, 0.1946, 0.1924, 0.1907]
  value_value_mse : [0.2117, 0.2075, 0.2037, 0.2012, 0.1970, 0.1947, 0.1924, 0.1907]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:48:25.965513Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game55501_game57000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.311586   (epoch 10)   mode=min
  policy_loss: 2.224481   (epoch 10)   mode=min
  policy_policy_acc: 0.332580   (epoch 10)   mode=max
  policy_top10_acc: 0.845716   (epoch 10)   mode=max
  policy_top5_acc: 0.694437   (epoch 10)   mode=max
  val_loss: 2.648634   (epoch 4)   mode=min
  val_policy_loss: 2.547535   (epoch 4)   mode=min
  val_policy_policy_acc: 0.276523   (epoch 7)   mode=max
  val_policy_top10_acc: 0.778322   (epoch 10)   mode=max
  val_policy_top5_acc: 0.614685   (epoch 10)   mode=max
  val_value_loss: 0.201714   (epoch 3)   mode=min
  val_value_value_mse: 0.201734   (epoch 3)   mode=min
  value_loss: 0.174205   (epoch 10)   mode=min
  value_value_mse: 0.174201   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6640, 2.6132, 2.5639, 2.4832, 2.4495, 2.4169, 2.3703, 2.3500, 2.3243, 2.3116]
  policy_loss : [2.5655, 2.5168, 2.4695, 2.3914, 2.3585, 2.3273, 2.2816, 2.2619, 2.2367, 2.2245]
  policy_policy_acc : [0.2652, 0.2734, 0.2841, 0.2982, 0.3036, 0.3101, 0.3209, 0.3251, 0.3301, 0.3326]
  policy_top10_acc : [0.7712, 0.7848, 0.7956, 0.8127, 0.8207, 0.8276, 0.8350, 0.8390, 0.8444, 0.8457]
  policy_top5_acc : [0.6035, 0.6182, 0.6313, 0.6509, 0.6608, 0.6685, 0.6805, 0.6847, 0.6926, 0.6944]
  val_loss : [2.6562, 2.6621, 2.6600, 2.6486, 2.6573, 2.6612, 2.6652, 2.6694, 2.6733, 2.6766]
  val_policy_loss : [2.5546, 2.5606, 2.5591, 2.5475, 2.5553, 2.5594, 2.5627, 2.5669, 2.5699, 2.5733]
  val_policy_policy_acc : [0.2662, 0.2698, 0.2674, 0.2736, 0.2733, 0.2732, 0.2765, 0.2748, 0.2750, 0.2740]
  val_policy_top10_acc : [0.7666, 0.7673, 0.7708, 0.7756, 0.7734, 0.7743, 0.7766, 0.7760, 0.7773, 0.7783]
  val_policy_top5_acc : [0.6063, 0.6016, 0.6041, 0.6111, 0.6091, 0.6076, 0.6122, 0.6108, 0.6140, 0.6147]
  val_value_loss : [0.2031, 0.2030, 0.2017, 0.2021, 0.2039, 0.2033, 0.2049, 0.2047, 0.2066, 0.2064]
  val_value_value_mse : [0.2031, 0.2030, 0.2017, 0.2021, 0.2039, 0.2034, 0.2049, 0.2047, 0.2066, 0.2064]
  value_loss : [0.1969, 0.1929, 0.1889, 0.1835, 0.1818, 0.1791, 0.1774, 0.1762, 0.1752, 0.1742]
  value_value_mse : [0.1969, 0.1929, 0.1889, 0.1835, 0.1818, 0.1791, 0.1774, 0.1762, 0.1752, 0.1742]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T21:56:35.288107Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game57001_game58500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.336345   (epoch 8)   mode=min
  policy_loss: 2.249067   (epoch 8)   mode=min
  policy_policy_acc: 0.325604   (epoch 8)   mode=max
  policy_top10_acc: 0.840885   (epoch 8)   mode=max
  policy_top5_acc: 0.688285   (epoch 8)   mode=max
  val_loss: 2.643843   (epoch 2)   mode=min
  val_policy_loss: 2.543038   (epoch 2)   mode=min
  val_policy_policy_acc: 0.270829   (epoch 2)   mode=max
  val_policy_top10_acc: 0.776923   (epoch 7)   mode=max
  val_policy_top5_acc: 0.610190   (epoch 6)   mode=max
  val_value_loss: 0.201517   (epoch 2)   mode=min
  val_value_value_mse: 0.201542   (epoch 2)   mode=min
  value_loss: 0.174156   (epoch 8)   mode=min
  value_value_mse: 0.174147   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6671, 2.6106, 2.5620, 2.5192, 2.4424, 2.4085, 2.3589, 2.3363]
  policy_loss : [2.5692, 2.5146, 2.4675, 2.4269, 2.3521, 2.3197, 2.2709, 2.2491]
  policy_policy_acc : [0.2600, 0.2730, 0.2804, 0.2905, 0.3040, 0.3115, 0.3216, 0.3256]
  policy_top10_acc : [0.7716, 0.7838, 0.7968, 0.8074, 0.8220, 0.8289, 0.8390, 0.8409]
  policy_top5_acc : [0.6026, 0.6181, 0.6313, 0.6428, 0.6620, 0.6713, 0.6839, 0.6883]
  val_loss : [2.6496, 2.6438, 2.6614, 2.6617, 2.6632, 2.6761, 2.6753, 2.6824]
  val_policy_loss : [2.5461, 2.5430, 2.5595, 2.5595, 2.5619, 2.5742, 2.5736, 2.5804]
  val_policy_policy_acc : [0.2678, 0.2708, 0.2641, 0.2648, 0.2670, 0.2657, 0.2703, 0.2708]
  val_policy_top10_acc : [0.7699, 0.7726, 0.7703, 0.7663, 0.7746, 0.7724, 0.7769, 0.7735]
  val_policy_top5_acc : [0.6070, 0.6073, 0.5998, 0.5992, 0.6074, 0.6102, 0.6094, 0.6066]
  val_value_loss : [0.2068, 0.2015, 0.2036, 0.2044, 0.2025, 0.2036, 0.2035, 0.2039]
  val_value_value_mse : [0.2068, 0.2015, 0.2036, 0.2044, 0.2026, 0.2036, 0.2035, 0.2039]
  value_loss : [0.1956, 0.1921, 0.1887, 0.1848, 0.1806, 0.1776, 0.1756, 0.1742]
  value_value_mse : [0.1956, 0.1921, 0.1887, 0.1848, 0.1806, 0.1776, 0.1756, 0.1741]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:04:39.091164Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game58501_game60000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.397811   (epoch 7)   mode=min
  policy_loss: 2.302814   (epoch 7)   mode=min
  policy_policy_acc: 0.316803   (epoch 7)   mode=max
  policy_top10_acc: 0.827315   (epoch 7)   mode=max
  policy_top5_acc: 0.674426   (epoch 7)   mode=max
  val_loss: 2.642367   (epoch 2)   mode=min
  val_policy_loss: 2.537960   (epoch 1)   mode=min
  val_policy_policy_acc: 0.277922   (epoch 7)   mode=max
  val_policy_top10_acc: 0.778122   (epoch 1)   mode=max
  val_policy_top5_acc: 0.614885   (epoch 7)   mode=max
  val_value_loss: 0.203551   (epoch 2)   mode=min
  val_value_value_mse: 0.203571   (epoch 2)   mode=min
  value_loss: 0.190077   (epoch 7)   mode=min
  value_value_mse: 0.190070   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6799, 2.6211, 2.5804, 2.4991, 2.4625, 2.4159, 2.3978]
  policy_loss : [2.5761, 2.5189, 2.4798, 2.4014, 2.3656, 2.3203, 2.3028]
  policy_policy_acc : [0.2636, 0.2737, 0.2816, 0.2979, 0.3034, 0.3133, 0.3168]
  policy_top10_acc : [0.7666, 0.7815, 0.7910, 0.8072, 0.8163, 0.8260, 0.8273]
  policy_top5_acc : [0.6014, 0.6162, 0.6272, 0.6477, 0.6575, 0.6707, 0.6744]
  val_loss : [2.6434, 2.6424, 2.6542, 2.6448, 2.6512, 2.6509, 2.6590]
  val_policy_loss : [2.5380, 2.5405, 2.5504, 2.5388, 2.5480, 2.5482, 2.5566]
  val_policy_policy_acc : [0.2690, 0.2683, 0.2669, 0.2705, 0.2725, 0.2763, 0.2779]
  val_policy_top10_acc : [0.7781, 0.7706, 0.7731, 0.7766, 0.7773, 0.7762, 0.7767]
  val_policy_top5_acc : [0.6088, 0.6085, 0.6066, 0.6083, 0.6094, 0.6143, 0.6149]
  val_value_loss : [0.2107, 0.2036, 0.2072, 0.2118, 0.2061, 0.2052, 0.2048]
  val_value_value_mse : [0.2107, 0.2036, 0.2072, 0.2118, 0.2061, 0.2052, 0.2048]
  value_loss : [0.2078, 0.2042, 0.2012, 0.1964, 0.1939, 0.1914, 0.1901]
  value_value_mse : [0.2077, 0.2041, 0.2012, 0.1964, 0.1939, 0.1914, 0.1901]

================================================================================

History file: model_versions/chess_elo_model_V5_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:12:43.788258Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game60001_game61500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V4
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.351162   (epoch 8)   mode=min
  policy_loss: 2.262040   (epoch 8)   mode=min
  policy_policy_acc: 0.324918   (epoch 8)   mode=max
  policy_top10_acc: 0.838720   (epoch 8)   mode=max
  policy_top5_acc: 0.686566   (epoch 8)   mode=max
  val_loss: 2.633226   (epoch 2)   mode=min
  val_policy_loss: 2.531094   (epoch 2)   mode=min
  val_policy_policy_acc: 0.279121   (epoch 5)   mode=max
  val_policy_top10_acc: 0.777223   (epoch 7)   mode=max
  val_policy_top5_acc: 0.611688   (epoch 5)   mode=max
  val_value_loss: 0.202484   (epoch 1)   mode=min
  val_value_value_mse: 0.202500   (epoch 1)   mode=min
  value_loss: 0.177811   (epoch 8)   mode=min
  value_value_mse: 0.177805   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6621, 2.6096, 2.5660, 2.5269, 2.4543, 2.4194, 2.3729, 2.3512]
  policy_loss : [2.5617, 2.5118, 2.4700, 2.4317, 2.3618, 2.3283, 2.2831, 2.2620]
  policy_policy_acc : [0.2646, 0.2752, 0.2825, 0.2901, 0.3050, 0.3107, 0.3193, 0.3249]
  policy_top10_acc : [0.7742, 0.7860, 0.7958, 0.8044, 0.8198, 0.8257, 0.8335, 0.8387]
  policy_top5_acc : [0.6077, 0.6213, 0.6333, 0.6424, 0.6625, 0.6700, 0.6819, 0.6866]
  val_loss : [2.6378, 2.6332, 2.6635, 2.6520, 2.6454, 2.6546, 2.6611, 2.6611]
  val_policy_loss : [2.5365, 2.5311, 2.5606, 2.5489, 2.5434, 2.5514, 2.5584, 2.5591]
  val_policy_policy_acc : [0.2696, 0.2718, 0.2681, 0.2748, 0.2791, 0.2741, 0.2760, 0.2767]
  val_policy_top10_acc : [0.7714, 0.7750, 0.7677, 0.7684, 0.7759, 0.7743, 0.7772, 0.7738]
  val_policy_top5_acc : [0.6061, 0.6090, 0.6048, 0.6056, 0.6117, 0.6076, 0.6080, 0.6093]
  val_value_loss : [0.2025, 0.2041, 0.2057, 0.2060, 0.2038, 0.2061, 0.2054, 0.2039]
  val_value_value_mse : [0.2025, 0.2041, 0.2057, 0.2060, 0.2038, 0.2061, 0.2055, 0.2039]
  value_loss : [0.2008, 0.1958, 0.1923, 0.1903, 0.1850, 0.1820, 0.1794, 0.1778]
  value_value_mse : [0.2008, 0.1958, 0.1924, 0.1903, 0.1850, 0.1820, 0.1794, 0.1778]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:21:27.263546Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game61501_game63000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.347466   (epoch 8)   mode=min
  policy_loss: 2.258842   (epoch 8)   mode=min
  policy_policy_acc: 0.327147   (epoch 8)   mode=max
  policy_top10_acc: 0.838742   (epoch 8)   mode=max
  policy_top5_acc: 0.687338   (epoch 8)   mode=max
  val_loss: 2.633660   (epoch 2)   mode=min
  val_policy_loss: 2.532841   (epoch 2)   mode=min
  val_policy_policy_acc: 0.278322   (epoch 6)   mode=max
  val_policy_top10_acc: 0.781119   (epoch 2)   mode=max
  val_policy_top5_acc: 0.611089   (epoch 5)   mode=max
  val_value_loss: 0.201656   (epoch 2)   mode=min
  val_value_value_mse: 0.201662   (epoch 2)   mode=min
  value_loss: 0.176891   (epoch 8)   mode=min
  value_value_mse: 0.176880   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6637, 2.6065, 2.5634, 2.5232, 2.4489, 2.4169, 2.3686, 2.3475]
  policy_loss : [2.5648, 2.5101, 2.4683, 2.4296, 2.3581, 2.3271, 2.2801, 2.2588]
  policy_policy_acc : [0.2660, 0.2762, 0.2854, 0.2919, 0.3059, 0.3135, 0.3223, 0.3271]
  policy_top10_acc : [0.7709, 0.7851, 0.7970, 0.8049, 0.8203, 0.8255, 0.8351, 0.8387]
  policy_top5_acc : [0.6059, 0.6218, 0.6333, 0.6431, 0.6623, 0.6694, 0.6824, 0.6873]
  val_loss : [2.6534, 2.6337, 2.6551, 2.6595, 2.6521, 2.6548, 2.6605, 2.6693]
  val_policy_loss : [2.5502, 2.5328, 2.5527, 2.5577, 2.5492, 2.5524, 2.5577, 2.5652]
  val_policy_policy_acc : [0.2679, 0.2678, 0.2708, 0.2694, 0.2762, 0.2783, 0.2778, 0.2749]
  val_policy_top10_acc : [0.7703, 0.7811, 0.7700, 0.7734, 0.7738, 0.7759, 0.7806, 0.7746]
  val_policy_top5_acc : [0.6035, 0.6108, 0.5992, 0.6041, 0.6111, 0.6109, 0.6103, 0.6048]
  val_value_loss : [0.2064, 0.2017, 0.2046, 0.2036, 0.2057, 0.2046, 0.2055, 0.2081]
  val_value_value_mse : [0.2064, 0.2017, 0.2046, 0.2036, 0.2057, 0.2046, 0.2056, 0.2081]
  value_loss : [0.1980, 0.1936, 0.1904, 0.1872, 0.1823, 0.1800, 0.1778, 0.1769]
  value_value_mse : [0.1980, 0.1936, 0.1905, 0.1872, 0.1824, 0.1800, 0.1778, 0.1769]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:31:35.138980Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game63001_game64500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.313752   (epoch 10)   mode=min
  policy_loss: 2.223387   (epoch 10)   mode=min
  policy_policy_acc: 0.334664   (epoch 10)   mode=max
  policy_top10_acc: 0.842554   (epoch 10)   mode=max
  policy_top5_acc: 0.693487   (epoch 10)   mode=max
  val_loss: 2.640287   (epoch 5)   mode=min
  val_policy_loss: 2.537365   (epoch 5)   mode=min
  val_policy_policy_acc: 0.277023   (epoch 8)   mode=max
  val_policy_top10_acc: 0.778322   (epoch 4)   mode=max
  val_policy_top5_acc: 0.617183   (epoch 5)   mode=max
  val_value_loss: 0.202245   (epoch 2)   mode=min
  val_value_value_mse: 0.202252   (epoch 2)   mode=min
  value_loss: 0.180770   (epoch 10)   mode=min
  value_value_mse: 0.180773   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6627, 2.6097, 2.5662, 2.4919, 2.4585, 2.4307, 2.4015, 2.3566, 2.3442, 2.3138]
  policy_loss : [2.5607, 2.5098, 2.4680, 2.3964, 2.3642, 2.3375, 2.3091, 2.2655, 2.2536, 2.2234]
  policy_policy_acc : [0.2667, 0.2759, 0.2861, 0.2989, 0.3068, 0.3106, 0.3158, 0.3251, 0.3278, 0.3347]
  policy_top10_acc : [0.7692, 0.7832, 0.7913, 0.8085, 0.8150, 0.8202, 0.8270, 0.8345, 0.8380, 0.8426]
  policy_top5_acc : [0.6061, 0.6206, 0.6311, 0.6495, 0.6586, 0.6651, 0.6730, 0.6835, 0.6870, 0.6935]
  val_loss : [2.6430, 2.6453, 2.6713, 2.6550, 2.6403, 2.6569, 2.6681, 2.6807, 2.6868, 2.6839]
  val_policy_loss : [2.5404, 2.5442, 2.5690, 2.5522, 2.5374, 2.5539, 2.5646, 2.5768, 2.5823, 2.5795]
  val_policy_policy_acc : [0.2740, 0.2727, 0.2660, 0.2749, 0.2754, 0.2732, 0.2758, 0.2770, 0.2733, 0.2759]
  val_policy_top10_acc : [0.7754, 0.7740, 0.7701, 0.7783, 0.7776, 0.7740, 0.7737, 0.7757, 0.7740, 0.7746]
  val_policy_top5_acc : [0.6060, 0.6099, 0.6085, 0.6103, 0.6172, 0.6135, 0.6109, 0.6140, 0.6146, 0.6142]
  val_value_loss : [0.2052, 0.2022, 0.2045, 0.2055, 0.2056, 0.2057, 0.2068, 0.2075, 0.2086, 0.2085]
  val_value_value_mse : [0.2052, 0.2023, 0.2045, 0.2055, 0.2056, 0.2057, 0.2068, 0.2075, 0.2086, 0.2084]
  value_loss : [0.2040, 0.1998, 0.1963, 0.1909, 0.1886, 0.1862, 0.1847, 0.1821, 0.1811, 0.1808]
  value_value_mse : [0.2040, 0.1998, 0.1963, 0.1909, 0.1886, 0.1862, 0.1847, 0.1821, 0.1811, 0.1808]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:39:46.298516Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game64501_game66000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.360276   (epoch 8)   mode=min
  policy_loss: 2.266881   (epoch 8)   mode=min
  policy_policy_acc: 0.322308   (epoch 8)   mode=max
  policy_top10_acc: 0.837885   (epoch 8)   mode=max
  policy_top5_acc: 0.684874   (epoch 8)   mode=max
  val_loss: 2.641595   (epoch 2)   mode=min
  val_policy_loss: 2.539316   (epoch 2)   mode=min
  val_policy_policy_acc: 0.274326   (epoch 5)   mode=max
  val_policy_top10_acc: 0.774925   (epoch 6)   mode=max
  val_policy_top5_acc: 0.613487   (epoch 6)   mode=max
  val_value_loss: 0.201136   (epoch 5)   mode=min
  val_value_value_mse: 0.201140   (epoch 5)   mode=min
  value_loss: 0.186843   (epoch 8)   mode=min
  value_value_mse: 0.186848   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6787, 2.6224, 2.5788, 2.5393, 2.4584, 2.4300, 2.3799, 2.3603]
  policy_loss : [2.5753, 2.5209, 2.4790, 2.4402, 2.3621, 2.3344, 2.2857, 2.2669]
  policy_policy_acc : [0.2602, 0.2728, 0.2809, 0.2894, 0.3029, 0.3078, 0.3179, 0.3223]
  policy_top10_acc : [0.7704, 0.7827, 0.7940, 0.8015, 0.8184, 0.8256, 0.8340, 0.8379]
  policy_top5_acc : [0.6028, 0.6176, 0.6283, 0.6397, 0.6601, 0.6678, 0.6790, 0.6849]
  val_loss : [2.6544, 2.6416, 2.6568, 2.6602, 2.6601, 2.6630, 2.6542, 2.6758]
  val_policy_loss : [2.5533, 2.5393, 2.5538, 2.5586, 2.5594, 2.5610, 2.5525, 2.5741]
  val_policy_policy_acc : [0.2684, 0.2690, 0.2642, 0.2684, 0.2743, 0.2734, 0.2726, 0.2715]
  val_policy_top10_acc : [0.7702, 0.7719, 0.7680, 0.7697, 0.7701, 0.7749, 0.7705, 0.7730]
  val_policy_top5_acc : [0.6079, 0.6075, 0.6025, 0.6021, 0.6109, 0.6135, 0.6118, 0.6128]
  val_value_loss : [0.2020, 0.2045, 0.2057, 0.2030, 0.2011, 0.2040, 0.2032, 0.2033]
  val_value_value_mse : [0.2021, 0.2045, 0.2057, 0.2030, 0.2011, 0.2040, 0.2032, 0.2033]
  value_loss : [0.2067, 0.2030, 0.1997, 0.1981, 0.1926, 0.1912, 0.1883, 0.1868]
  value_value_mse : [0.2067, 0.2030, 0.1997, 0.1981, 0.1926, 0.1912, 0.1883, 0.1868]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:46:42.373252Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game66001_game67500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.411614   (epoch 7)   mode=min
  policy_loss: 2.316276   (epoch 7)   mode=min
  policy_policy_acc: 0.315416   (epoch 7)   mode=max
  policy_top10_acc: 0.827699   (epoch 7)   mode=max
  policy_top5_acc: 0.671106   (epoch 7)   mode=max
  val_loss: 2.637709   (epoch 1)   mode=min
  val_policy_loss: 2.536568   (epoch 1)   mode=min
  val_policy_policy_acc: 0.277622   (epoch 6)   mode=max
  val_policy_top10_acc: 0.776124   (epoch 6)   mode=max
  val_policy_top5_acc: 0.611389   (epoch 5)   mode=max
  val_value_loss: 0.202009   (epoch 1)   mode=min
  val_value_value_mse: 0.202044   (epoch 1)   mode=min
  value_loss: 0.190611   (epoch 7)   mode=min
  value_value_mse: 0.190638   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6837, 2.6329, 2.5908, 2.5103, 2.4765, 2.4333, 2.4116]
  policy_loss : [2.5785, 2.5301, 2.4895, 2.4116, 2.3796, 2.3371, 2.3163]
  policy_policy_acc : [0.2614, 0.2714, 0.2819, 0.2959, 0.3023, 0.3112, 0.3154]
  policy_top10_acc : [0.7680, 0.7812, 0.7910, 0.8077, 0.8148, 0.8230, 0.8277]
  policy_top5_acc : [0.6000, 0.6173, 0.6262, 0.6478, 0.6555, 0.6666, 0.6711]
  val_loss : [2.6377, 2.6500, 2.6597, 2.6707, 2.6545, 2.6478, 2.6586]
  val_policy_loss : [2.5366, 2.5460, 2.5567, 2.5694, 2.5521, 2.5464, 2.5570]
  val_policy_policy_acc : [0.2760, 0.2677, 0.2689, 0.2757, 0.2739, 0.2776, 0.2759]
  val_policy_top10_acc : [0.7682, 0.7729, 0.7678, 0.7726, 0.7730, 0.7761, 0.7751]
  val_policy_top5_acc : [0.6108, 0.6057, 0.6014, 0.6106, 0.6114, 0.6106, 0.6100]
  val_value_loss : [0.2020, 0.2078, 0.2060, 0.2023, 0.2046, 0.2025, 0.2030]
  val_value_value_mse : [0.2020, 0.2078, 0.2060, 0.2023, 0.2046, 0.2025, 0.2030]
  value_loss : [0.2100, 0.2051, 0.2022, 0.1975, 0.1941, 0.1923, 0.1906]
  value_value_mse : [0.2099, 0.2051, 0.2022, 0.1975, 0.1941, 0.1923, 0.1906]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T22:53:43.111664Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game67501_game69000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399763   (epoch 7)   mode=min
  policy_loss: 2.305825   (epoch 7)   mode=min
  policy_policy_acc: 0.318236   (epoch 7)   mode=max
  policy_top10_acc: 0.827209   (epoch 7)   mode=max
  policy_top5_acc: 0.676114   (epoch 7)   mode=max
  val_loss: 2.625132   (epoch 1)   mode=min
  val_policy_loss: 2.524178   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280819   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783117   (epoch 6)   mode=max
  val_policy_top5_acc: 0.612587   (epoch 7)   mode=max
  val_value_loss: 0.201459   (epoch 6)   mode=min
  val_value_value_mse: 0.201489   (epoch 6)   mode=min
  value_loss: 0.187811   (epoch 7)   mode=min
  value_value_mse: 0.187813   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6715, 2.6196, 2.5744, 2.4979, 2.4624, 2.4135, 2.3998]
  policy_loss : [2.5687, 2.5182, 2.4748, 2.4010, 2.3668, 2.3188, 2.3058]
  policy_policy_acc : [0.2650, 0.2751, 0.2846, 0.2996, 0.3055, 0.3145, 0.3182]
  policy_top10_acc : [0.7684, 0.7807, 0.7926, 0.8084, 0.8140, 0.8243, 0.8272]
  policy_top5_acc : [0.6049, 0.6197, 0.6312, 0.6511, 0.6599, 0.6708, 0.6761]
  val_loss : [2.6251, 2.6470, 2.6351, 2.6333, 2.6358, 2.6362, 2.6480]
  val_policy_loss : [2.5242, 2.5439, 2.5323, 2.5318, 2.5342, 2.5353, 2.5467]
  val_policy_policy_acc : [0.2713, 0.2674, 0.2735, 0.2752, 0.2781, 0.2805, 0.2808]
  val_policy_top10_acc : [0.7798, 0.7738, 0.7761, 0.7774, 0.7817, 0.7831, 0.7787]
  val_policy_top5_acc : [0.6111, 0.6045, 0.6081, 0.6117, 0.6117, 0.6105, 0.6126]
  val_value_loss : [0.2017, 0.2060, 0.2055, 0.2027, 0.2030, 0.2015, 0.2023]
  val_value_value_mse : [0.2018, 0.2060, 0.2055, 0.2027, 0.2030, 0.2015, 0.2023]
  value_loss : [0.2054, 0.2028, 0.1989, 0.1937, 0.1912, 0.1893, 0.1878]
  value_value_mse : [0.2054, 0.2028, 0.1989, 0.1937, 0.1912, 0.1893, 0.1878]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:03:34.162772Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game69001_game70500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.344439   (epoch 10)   mode=min
  policy_loss: 2.252607   (epoch 10)   mode=min
  policy_policy_acc: 0.328181   (epoch 10)   mode=max
  policy_top10_acc: 0.838096   (epoch 10)   mode=max
  policy_top5_acc: 0.688715   (epoch 10)   mode=max
  val_loss: 2.631225   (epoch 4)   mode=min
  val_policy_loss: 2.531021   (epoch 4)   mode=min
  val_policy_policy_acc: 0.277123   (epoch 4)   mode=max
  val_policy_top10_acc: 0.779221   (epoch 4)   mode=max
  val_policy_top5_acc: 0.613986   (epoch 4)   mode=max
  val_value_loss: 0.200351   (epoch 4)   mode=min
  val_value_value_mse: 0.200379   (epoch 4)   mode=min
  value_loss: 0.183564   (epoch 10)   mode=min
  value_value_mse: 0.183554   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6814, 2.6299, 2.5880, 2.5083, 2.4763, 2.4473, 2.4005, 2.3802, 2.3533, 2.3444]
  policy_loss : [2.5794, 2.5296, 2.4893, 2.4118, 2.3809, 2.3530, 2.3073, 2.2875, 2.2612, 2.2526]
  policy_policy_acc : [0.2648, 0.2743, 0.2810, 0.2964, 0.3023, 0.3088, 0.3171, 0.3205, 0.3274, 0.3282]
  policy_top10_acc : [0.7678, 0.7792, 0.7901, 0.8072, 0.8142, 0.8196, 0.8286, 0.8330, 0.8373, 0.8381]
  policy_top5_acc : [0.6046, 0.6187, 0.6290, 0.6497, 0.6582, 0.6645, 0.6747, 0.6814, 0.6866, 0.6887]
  val_loss : [2.6379, 2.6542, 2.6572, 2.6312, 2.6479, 2.6559, 2.6601, 2.6648, 2.6808, 2.6713]
  val_policy_loss : [2.5367, 2.5501, 2.5557, 2.5310, 2.5475, 2.5550, 2.5593, 2.5642, 2.5802, 2.5709]
  val_policy_policy_acc : [0.2723, 0.2674, 0.2652, 0.2771, 0.2738, 0.2723, 0.2719, 0.2692, 0.2706, 0.2731]
  val_policy_top10_acc : [0.7711, 0.7713, 0.7737, 0.7792, 0.7771, 0.7766, 0.7746, 0.7760, 0.7766, 0.7749]
  val_policy_top5_acc : [0.6117, 0.6082, 0.6063, 0.6140, 0.6127, 0.6119, 0.6114, 0.6132, 0.6132, 0.6140]
  val_value_loss : [0.2024, 0.2083, 0.2032, 0.2004, 0.2008, 0.2016, 0.2016, 0.2011, 0.2009, 0.2007]
  val_value_value_mse : [0.2024, 0.2083, 0.2032, 0.2004, 0.2008, 0.2016, 0.2016, 0.2012, 0.2009, 0.2008]
  value_loss : [0.2041, 0.2005, 0.1975, 0.1929, 0.1908, 0.1889, 0.1865, 0.1854, 0.1840, 0.1836]
  value_value_mse : [0.2041, 0.2005, 0.1975, 0.1929, 0.1908, 0.1889, 0.1865, 0.1854, 0.1840, 0.1836]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:10:23.959127Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game70501_game72000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.390417   (epoch 7)   mode=min
  policy_loss: 2.298435   (epoch 7)   mode=min
  policy_policy_acc: 0.320462   (epoch 7)   mode=max
  policy_top10_acc: 0.828613   (epoch 7)   mode=max
  policy_top5_acc: 0.677181   (epoch 7)   mode=max
  val_loss: 2.649069   (epoch 1)   mode=min
  val_policy_loss: 2.545668   (epoch 1)   mode=min
  val_policy_policy_acc: 0.278821   (epoch 4)   mode=max
  val_policy_top10_acc: 0.779321   (epoch 4)   mode=max
  val_policy_top5_acc: 0.619580   (epoch 6)   mode=max
  val_value_loss: 0.199845   (epoch 2)   mode=min
  val_value_value_mse: 0.199872   (epoch 2)   mode=min
  value_loss: 0.183632   (epoch 7)   mode=min
  value_value_mse: 0.183623   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6715, 2.6146, 2.5697, 2.4956, 2.4614, 2.4124, 2.3904]
  policy_loss : [2.5701, 2.5152, 2.4722, 2.4003, 2.3676, 2.3198, 2.2984]
  policy_policy_acc : [0.2647, 0.2773, 0.2855, 0.2994, 0.3054, 0.3149, 0.3205]
  policy_top10_acc : [0.7691, 0.7828, 0.7926, 0.8068, 0.8139, 0.8250, 0.8286]
  policy_top5_acc : [0.6055, 0.6209, 0.6328, 0.6515, 0.6603, 0.6721, 0.6772]
  val_loss : [2.6491, 2.6628, 2.8163, 2.9320, 2.9486, 2.6907, 3.0794]
  val_policy_loss : [2.5457, 2.5628, 2.7152, 2.8304, 2.8473, 2.5907, 2.9789]
  val_policy_policy_acc : [0.2720, 0.2722, 0.2711, 0.2788, 0.2759, 0.2786, 0.2787]
  val_policy_top10_acc : [0.7716, 0.7699, 0.7733, 0.7793, 0.7782, 0.7787, 0.7762]
  val_policy_top5_acc : [0.6065, 0.6067, 0.6055, 0.6123, 0.6134, 0.6196, 0.6139]
  val_value_loss : [0.2067, 0.1998, 0.2021, 0.2027, 0.2021, 0.1999, 0.2004]
  val_value_value_mse : [0.2068, 0.1999, 0.2021, 0.2027, 0.2021, 0.1999, 0.2004]
  value_loss : [0.2027, 0.1984, 0.1950, 0.1906, 0.1877, 0.1854, 0.1836]
  value_value_mse : [0.2027, 0.1984, 0.1950, 0.1906, 0.1877, 0.1854, 0.1836]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:20:01.721901Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game72001_game73500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.327108   (epoch 10)   mode=min
  policy_loss: 2.231920   (epoch 10)   mode=min
  policy_policy_acc: 0.332001   (epoch 10)   mode=max
  policy_top10_acc: 0.841939   (epoch 10)   mode=max
  policy_top5_acc: 0.691751   (epoch 10)   mode=max
  val_loss: 2.633027   (epoch 4)   mode=min
  val_policy_loss: 2.532269   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279421   (epoch 9)   mode=max
  val_policy_top10_acc: 0.774426   (epoch 8)   mode=max
  val_policy_top5_acc: 0.614685   (epoch 5)   mode=max
  val_value_loss: 0.201317   (epoch 7)   mode=min
  val_value_value_mse: 0.201345   (epoch 7)   mode=min
  value_loss: 0.190353   (epoch 10)   mode=min
  value_value_mse: 0.190358   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6543, 2.6043, 2.5614, 2.4856, 2.4512, 2.4235, 2.3795, 2.3607, 2.3344, 2.3271]
  policy_loss : [2.5487, 2.5006, 2.4589, 2.3857, 2.3523, 2.3256, 2.2826, 2.2644, 2.2389, 2.2319]
  policy_policy_acc : [0.2709, 0.2806, 0.2879, 0.3021, 0.3078, 0.3135, 0.3229, 0.3255, 0.3316, 0.3320]
  policy_top10_acc : [0.7746, 0.7854, 0.7948, 0.8116, 0.8183, 0.8235, 0.8331, 0.8368, 0.8405, 0.8419]
  policy_top5_acc : [0.6101, 0.6218, 0.6331, 0.6536, 0.6610, 0.6686, 0.6785, 0.6841, 0.6904, 0.6918]
  val_loss : [2.6398, 2.6428, 2.6674, 2.6330, 2.6411, 2.6564, 2.6561, 2.6546, 2.6666, 2.6655]
  val_policy_loss : [2.5377, 2.5412, 2.5650, 2.5323, 2.5400, 2.5547, 2.5554, 2.5536, 2.5651, 2.5642]
  val_policy_policy_acc : [0.2692, 0.2754, 0.2708, 0.2758, 0.2747, 0.2745, 0.2764, 0.2777, 0.2794, 0.2775]
  val_policy_top10_acc : [0.7721, 0.7722, 0.7697, 0.7733, 0.7731, 0.7726, 0.7721, 0.7744, 0.7733, 0.7728]
  val_policy_top5_acc : [0.6083, 0.6075, 0.6083, 0.6116, 0.6147, 0.6105, 0.6109, 0.6118, 0.6136, 0.6134]
  val_value_loss : [0.2041, 0.2031, 0.2048, 0.2014, 0.2019, 0.2032, 0.2013, 0.2019, 0.2030, 0.2025]
  val_value_value_mse : [0.2041, 0.2031, 0.2048, 0.2014, 0.2019, 0.2032, 0.2013, 0.2019, 0.2030, 0.2025]
  value_loss : [0.2111, 0.2074, 0.2050, 0.1996, 0.1978, 0.1958, 0.1938, 0.1926, 0.1911, 0.1904]
  value_value_mse : [0.2111, 0.2074, 0.2050, 0.1996, 0.1978, 0.1958, 0.1938, 0.1926, 0.1911, 0.1904]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:27:14.735151Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game73501_game75000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.386464   (epoch 7)   mode=min
  policy_loss: 2.292069   (epoch 7)   mode=min
  policy_policy_acc: 0.321794   (epoch 7)   mode=max
  policy_top10_acc: 0.830853   (epoch 7)   mode=max
  policy_top5_acc: 0.677217   (epoch 7)   mode=max
  val_loss: 2.642473   (epoch 1)   mode=min
  val_policy_loss: 2.542053   (epoch 1)   mode=min
  val_policy_policy_acc: 0.275125   (epoch 6)   mode=max
  val_policy_top10_acc: 0.777223   (epoch 5)   mode=max
  val_policy_top5_acc: 0.613487   (epoch 4)   mode=max
  val_value_loss: 0.200266   (epoch 4)   mode=min
  val_value_value_mse: 0.200288   (epoch 4)   mode=min
  value_loss: 0.188782   (epoch 7)   mode=min
  value_value_mse: 0.188783   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6573, 2.6057, 2.5601, 2.4850, 2.4515, 2.4026, 2.3865]
  policy_loss : [2.5542, 2.5046, 2.4604, 2.3872, 2.3551, 2.3077, 2.2921]
  policy_policy_acc : [0.2699, 0.2786, 0.2878, 0.3021, 0.3081, 0.3178, 0.3218]
  policy_top10_acc : [0.7727, 0.7856, 0.7961, 0.8119, 0.8188, 0.8276, 0.8309]
  policy_top5_acc : [0.6077, 0.6205, 0.6328, 0.6529, 0.6606, 0.6728, 0.6772]
  val_loss : [2.6425, 2.8274, 2.6578, 2.6644, 2.6461, 2.7019, 2.6572]
  val_policy_loss : [2.5421, 2.7243, 2.5567, 2.5643, 2.5448, 2.6014, 2.5562]
  val_policy_policy_acc : [0.2736, 0.2693, 0.2653, 0.2726, 0.2711, 0.2751, 0.2748]
  val_policy_top10_acc : [0.7700, 0.7708, 0.7702, 0.7758, 0.7772, 0.7736, 0.7752]
  val_policy_top5_acc : [0.6090, 0.6064, 0.6036, 0.6135, 0.6100, 0.6083, 0.6108]
  val_value_loss : [0.2008, 0.2058, 0.2021, 0.2003, 0.2026, 0.2008, 0.2021]
  val_value_value_mse : [0.2008, 0.2059, 0.2022, 0.2003, 0.2026, 0.2008, 0.2021]
  value_loss : [0.2062, 0.2021, 0.1994, 0.1955, 0.1930, 0.1897, 0.1888]
  value_value_mse : [0.2062, 0.2021, 0.1994, 0.1955, 0.1930, 0.1897, 0.1888]

================================================================================

History file: model_versions/chess_elo_model_V6_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:37:26.846703Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game75001_game76500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V5
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.331489   (epoch 10)   mode=min
  policy_loss: 2.239433   (epoch 10)   mode=min
  policy_policy_acc: 0.330475   (epoch 10)   mode=max
  policy_top10_acc: 0.840377   (epoch 10)   mode=max
  policy_top5_acc: 0.690046   (epoch 10)   mode=max
  val_loss: 2.635792   (epoch 4)   mode=min
  val_policy_loss: 2.535057   (epoch 4)   mode=min
  val_policy_policy_acc: 0.273526   (epoch 7)   mode=max
  val_policy_top10_acc: 0.778222   (epoch 7)   mode=max
  val_policy_top5_acc: 0.615584   (epoch 7)   mode=max
  val_value_loss: 0.200736   (epoch 1)   mode=min
  val_value_value_mse: 0.200755   (epoch 1)   mode=min
  value_loss: 0.184065   (epoch 10)   mode=min
  value_value_mse: 0.184065   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6582, 2.6076, 2.5617, 2.4884, 2.4572, 2.4318, 2.3829, 2.3683, 2.3433, 2.3315]
  policy_loss : [2.5558, 2.5066, 2.4624, 2.3917, 2.3616, 2.3368, 2.2895, 2.2753, 2.2507, 2.2394]
  policy_policy_acc : [0.2686, 0.2779, 0.2868, 0.3002, 0.3080, 0.3123, 0.3207, 0.3241, 0.3286, 0.3305]
  policy_top10_acc : [0.7734, 0.7838, 0.7952, 0.8098, 0.8185, 0.8222, 0.8318, 0.8340, 0.8389, 0.8404]
  policy_top5_acc : [0.6102, 0.6227, 0.6344, 0.6520, 0.6596, 0.6668, 0.6792, 0.6817, 0.6876, 0.6900]
  val_loss : [2.6416, 2.6482, 2.6612, 2.6358, 2.6696, 2.6533, 2.6746, 2.6599, 2.6776, 2.6745]
  val_policy_loss : [2.5411, 2.5476, 2.5583, 2.5351, 2.5681, 2.5524, 2.5733, 2.5588, 2.5765, 2.5732]
  val_policy_policy_acc : [0.2714, 0.2714, 0.2702, 0.2732, 0.2732, 0.2711, 0.2735, 0.2717, 0.2732, 0.2735]
  val_policy_top10_acc : [0.7741, 0.7770, 0.7731, 0.7744, 0.7762, 0.7752, 0.7782, 0.7747, 0.7752, 0.7737]
  val_policy_top5_acc : [0.6064, 0.6114, 0.6072, 0.6111, 0.6135, 0.6103, 0.6156, 0.6110, 0.6130, 0.6131]
  val_value_loss : [0.2007, 0.2010, 0.2057, 0.2014, 0.2028, 0.2017, 0.2024, 0.2020, 0.2019, 0.2023]
  val_value_value_mse : [0.2008, 0.2010, 0.2057, 0.2014, 0.2028, 0.2017, 0.2024, 0.2020, 0.2019, 0.2023]
  value_loss : [0.2048, 0.2017, 0.1985, 0.1934, 0.1914, 0.1900, 0.1868, 0.1861, 0.1852, 0.1841]
  value_value_mse : [0.2048, 0.2017, 0.1985, 0.1934, 0.1914, 0.1900, 0.1868, 0.1861, 0.1852, 0.1841]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:44:24.437864Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game76501_game78000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.386696   (epoch 7)   mode=min
  policy_loss: 2.289980   (epoch 7)   mode=min
  policy_policy_acc: 0.323015   (epoch 7)   mode=max
  policy_top10_acc: 0.831448   (epoch 7)   mode=max
  policy_top5_acc: 0.680180   (epoch 7)   mode=max
  val_loss: 2.643605   (epoch 1)   mode=min
  val_policy_loss: 2.539068   (epoch 1)   mode=min
  val_policy_policy_acc: 0.273327   (epoch 2)   mode=max
  val_policy_top10_acc: 0.779121   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618781   (epoch 6)   mode=max
  val_value_loss: 0.202169   (epoch 6)   mode=min
  val_value_value_mse: 0.202193   (epoch 6)   mode=min
  value_loss: 0.193325   (epoch 7)   mode=min
  value_value_mse: 0.193322   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6569, 2.6012, 2.5607, 2.4858, 2.4498, 2.4039, 2.3867]
  policy_loss : [2.5522, 2.4982, 2.4592, 2.3865, 2.3514, 2.3064, 2.2900]
  policy_policy_acc : [0.2707, 0.2821, 0.2898, 0.3018, 0.3102, 0.3196, 0.3230]
  policy_top10_acc : [0.7734, 0.7877, 0.7983, 0.8117, 0.8202, 0.8282, 0.8314]
  policy_top5_acc : [0.6106, 0.6258, 0.6367, 0.6556, 0.6662, 0.6753, 0.6802]
  val_loss : [2.6436, 2.6466, 2.7064, 2.7617, 2.8034, 2.7689, 2.8826]
  val_policy_loss : [2.5391, 2.5447, 2.6033, 2.6590, 2.7020, 2.6676, 2.7812]
  val_policy_policy_acc : [0.2692, 0.2733, 0.2695, 0.2721, 0.2708, 0.2716, 0.2725]
  val_policy_top10_acc : [0.7740, 0.7720, 0.7732, 0.7790, 0.7761, 0.7787, 0.7791]
  val_policy_top5_acc : [0.6082, 0.6061, 0.6090, 0.6164, 0.6147, 0.6188, 0.6140]
  val_value_loss : [0.2090, 0.2036, 0.2059, 0.2052, 0.2023, 0.2022, 0.2022]
  val_value_value_mse : [0.2090, 0.2036, 0.2060, 0.2052, 0.2023, 0.2022, 0.2022]
  value_loss : [0.2093, 0.2059, 0.2030, 0.1987, 0.1970, 0.1952, 0.1933]
  value_value_mse : [0.2094, 0.2059, 0.2030, 0.1988, 0.1970, 0.1952, 0.1933]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-15T23:51:20.436405Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game78001_game79500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.421750   (epoch 7)   mode=min
  policy_loss: 2.328086   (epoch 7)   mode=min
  policy_policy_acc: 0.315340   (epoch 7)   mode=max
  policy_top10_acc: 0.823602   (epoch 7)   mode=max
  policy_top5_acc: 0.668176   (epoch 7)   mode=max
  val_loss: 2.633440   (epoch 1)   mode=min
  val_policy_loss: 2.529465   (epoch 1)   mode=min
  val_policy_policy_acc: 0.273027   (epoch 7)   mode=max
  val_policy_top10_acc: 0.779021   (epoch 7)   mode=max
  val_policy_top5_acc: 0.610589   (epoch 7)   mode=max
  val_value_loss: 0.203074   (epoch 5)   mode=min
  val_value_value_mse: 0.203106   (epoch 5)   mode=min
  value_loss: 0.187352   (epoch 7)   mode=min
  value_value_mse: 0.187357   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6862, 2.6332, 2.5893, 2.5167, 2.4856, 2.4400, 2.4217]
  policy_loss : [2.5831, 2.5316, 2.4896, 2.4196, 2.3897, 2.3456, 2.3281]
  policy_policy_acc : [0.2621, 0.2739, 0.2797, 0.2952, 0.3026, 0.3096, 0.3153]
  policy_top10_acc : [0.7669, 0.7793, 0.7899, 0.8046, 0.8109, 0.8198, 0.8236]
  policy_top5_acc : [0.6008, 0.6146, 0.6261, 0.6443, 0.6511, 0.6622, 0.6682]
  val_loss : [2.6334, 2.6545, 2.8005, 2.8371, 2.6476, 2.7842, 2.6630]
  val_policy_loss : [2.5295, 2.5513, 2.6987, 2.7344, 2.5459, 2.6808, 2.5611]
  val_policy_policy_acc : [0.2723, 0.2712, 0.2665, 0.2702, 0.2725, 0.2729, 0.2730]
  val_policy_top10_acc : [0.7764, 0.7743, 0.7714, 0.7783, 0.7787, 0.7783, 0.7790]
  val_policy_top5_acc : [0.6097, 0.6102, 0.6057, 0.6101, 0.6093, 0.6104, 0.6106]
  val_value_loss : [0.2078, 0.2062, 0.2032, 0.2049, 0.2031, 0.2064, 0.2035]
  val_value_value_mse : [0.2078, 0.2062, 0.2032, 0.2049, 0.2031, 0.2064, 0.2036]
  value_loss : [0.2062, 0.2031, 0.1993, 0.1940, 0.1917, 0.1889, 0.1874]
  value_value_mse : [0.2062, 0.2031, 0.1993, 0.1940, 0.1917, 0.1889, 0.1874]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:01:09.000862Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game79501_game81000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.320518   (epoch 10)   mode=min
  policy_loss: 2.230776   (epoch 10)   mode=min
  policy_policy_acc: 0.331780   (epoch 10)   mode=max
  policy_top10_acc: 0.842091   (epoch 10)   mode=max
  policy_top5_acc: 0.693570   (epoch 10)   mode=max
  val_loss: 2.641312   (epoch 1)   mode=min
  val_policy_loss: 2.538821   (epoch 5)   mode=min
  val_policy_policy_acc: 0.279620   (epoch 4)   mode=max
  val_policy_top10_acc: 0.781119   (epoch 10)   mode=max
  val_policy_top5_acc: 0.617083   (epoch 5)   mode=max
  val_value_loss: 0.202416   (epoch 8)   mode=min
  val_value_value_mse: 0.202446   (epoch 8)   mode=min
  value_loss: 0.179975   (epoch 10)   mode=min
  value_value_mse: 0.179990   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6543, 2.6048, 2.5614, 2.4875, 2.4574, 2.4325, 2.4082, 2.3636, 2.3459, 2.3205]
  policy_loss : [2.5539, 2.5057, 2.4640, 2.3922, 2.3637, 2.3395, 2.3165, 2.2724, 2.2552, 2.2308]
  policy_policy_acc : [0.2684, 0.2770, 0.2847, 0.3004, 0.3056, 0.3100, 0.3146, 0.3250, 0.3285, 0.3318]
  policy_top10_acc : [0.7733, 0.7831, 0.7942, 0.8103, 0.8164, 0.8216, 0.8272, 0.8346, 0.8371, 0.8421]
  policy_top5_acc : [0.6105, 0.6209, 0.6334, 0.6523, 0.6604, 0.6664, 0.6735, 0.6829, 0.6877, 0.6936]
  val_loss : [2.6413, 2.6516, 2.6580, 2.6548, 2.6419, 2.6452, 2.6558, 2.6553, 2.6602, 2.6665]
  val_policy_loss : [2.5391, 2.5489, 2.5523, 2.5527, 2.5388, 2.5426, 2.5537, 2.5540, 2.5584, 2.5645]
  val_policy_policy_acc : [0.2695, 0.2698, 0.2705, 0.2796, 0.2767, 0.2750, 0.2717, 0.2770, 0.2750, 0.2752]
  val_policy_top10_acc : [0.7736, 0.7672, 0.7755, 0.7807, 0.7810, 0.7773, 0.7795, 0.7806, 0.7781, 0.7811]
  val_policy_top5_acc : [0.6107, 0.6052, 0.6092, 0.6156, 0.6171, 0.6155, 0.6115, 0.6170, 0.6121, 0.6147]
  val_value_loss : [0.2043, 0.2053, 0.2113, 0.2041, 0.2060, 0.2052, 0.2042, 0.2024, 0.2035, 0.2039]
  val_value_value_mse : [0.2043, 0.2053, 0.2113, 0.2041, 0.2060, 0.2052, 0.2042, 0.2024, 0.2035, 0.2040]
  value_loss : [0.2017, 0.1979, 0.1945, 0.1902, 0.1876, 0.1859, 0.1846, 0.1823, 0.1812, 0.1800]
  value_value_mse : [0.2017, 0.1978, 0.1945, 0.1902, 0.1876, 0.1859, 0.1846, 0.1823, 0.1812, 0.1800]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:09:12.544661Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game81001_game82500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.368431   (epoch 8)   mode=min
  policy_loss: 2.280273   (epoch 8)   mode=min
  policy_policy_acc: 0.321463   (epoch 8)   mode=max
  policy_top10_acc: 0.833557   (epoch 8)   mode=max
  policy_top5_acc: 0.680363   (epoch 8)   mode=max
  val_loss: 2.625955   (epoch 2)   mode=min
  val_policy_loss: 2.525437   (epoch 2)   mode=min
  val_policy_policy_acc: 0.276224   (epoch 2)   mode=max
  val_policy_top10_acc: 0.781918   (epoch 6)   mode=max
  val_policy_top5_acc: 0.617083   (epoch 8)   mode=max
  val_value_loss: 0.198988   (epoch 1)   mode=min
  val_value_value_mse: 0.199018   (epoch 1)   mode=min
  value_loss: 0.176228   (epoch 8)   mode=min
  value_value_mse: 0.176235   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6720, 2.6180, 2.5738, 2.5352, 2.4623, 2.4298, 2.3862, 2.3684]
  policy_loss : [2.5727, 2.5205, 2.4778, 2.4412, 2.3712, 2.3395, 2.2976, 2.2803]
  policy_policy_acc : [0.2634, 0.2731, 0.2807, 0.2882, 0.3022, 0.3088, 0.3171, 0.3215]
  policy_top10_acc : [0.7680, 0.7809, 0.7912, 0.8003, 0.8137, 0.8208, 0.8297, 0.8336]
  policy_top5_acc : [0.6017, 0.6179, 0.6297, 0.6402, 0.6551, 0.6656, 0.6751, 0.6804]
  val_loss : [2.6302, 2.6260, 2.6399, 2.6606, 2.6467, 2.6595, 2.6644, 2.6674]
  val_policy_loss : [2.5307, 2.5254, 2.5388, 2.5600, 2.5458, 2.5584, 2.5633, 2.5659]
  val_policy_policy_acc : [0.2699, 0.2762, 0.2703, 0.2685, 0.2748, 0.2756, 0.2751, 0.2743]
  val_policy_top10_acc : [0.7761, 0.7799, 0.7781, 0.7778, 0.7781, 0.7819, 0.7802, 0.7786]
  val_policy_top5_acc : [0.6117, 0.6143, 0.6119, 0.6140, 0.6122, 0.6133, 0.6166, 0.6171]
  val_value_loss : [0.1990, 0.2008, 0.2022, 0.2009, 0.2017, 0.2019, 0.2019, 0.2027]
  val_value_value_mse : [0.1990, 0.2009, 0.2022, 0.2009, 0.2017, 0.2019, 0.2019, 0.2027]
  value_loss : [0.1985, 0.1950, 0.1917, 0.1882, 0.1821, 0.1809, 0.1773, 0.1762]
  value_value_mse : [0.1985, 0.1950, 0.1917, 0.1882, 0.1821, 0.1809, 0.1773, 0.1762]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:19:08.114636Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game82501_game84000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.361166   (epoch 10)   mode=min
  policy_loss: 2.272118   (epoch 10)   mode=min
  policy_policy_acc: 0.322621   (epoch 10)   mode=max
  policy_top10_acc: 0.834475   (epoch 10)   mode=max
  policy_top5_acc: 0.681056   (epoch 10)   mode=max
  val_loss: 2.623956   (epoch 4)   mode=min
  val_policy_loss: 2.523745   (epoch 4)   mode=min
  val_policy_policy_acc: 0.277123   (epoch 5)   mode=max
  val_policy_top10_acc: 0.782418   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618881   (epoch 9)   mode=max
  val_value_loss: 0.200323   (epoch 4)   mode=min
  val_value_value_mse: 0.200361   (epoch 4)   mode=min
  value_loss: 0.179538   (epoch 10)   mode=min
  value_value_mse: 0.179500   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6695, 2.6199, 2.5814, 2.5070, 2.4772, 2.4522, 2.4102, 2.3927, 2.3714, 2.3612]
  policy_loss : [2.5703, 2.5222, 2.4859, 2.4130, 2.3834, 2.3594, 2.3185, 2.3017, 2.2812, 2.2721]
  policy_policy_acc : [0.2636, 0.2729, 0.2792, 0.2958, 0.3018, 0.3038, 0.3132, 0.3169, 0.3207, 0.3226]
  policy_top10_acc : [0.7689, 0.7800, 0.7893, 0.8051, 0.8109, 0.8162, 0.8246, 0.8288, 0.8325, 0.8345]
  policy_top5_acc : [0.6010, 0.6148, 0.6254, 0.6440, 0.6519, 0.6586, 0.6701, 0.6731, 0.6806, 0.6811]
  val_loss : [2.6421, 2.6445, 2.6430, 2.6240, 2.6447, 2.6699, 2.6483, 2.6452, 2.6442, 2.6514]
  val_policy_loss : [2.5411, 2.5442, 2.5415, 2.5237, 2.5424, 2.5687, 2.5473, 2.5440, 2.5431, 2.5503]
  val_policy_policy_acc : [0.2731, 0.2688, 0.2748, 0.2755, 0.2771, 0.2754, 0.2761, 0.2746, 0.2769, 0.2764]
  val_policy_top10_acc : [0.7734, 0.7738, 0.7767, 0.7811, 0.7795, 0.7795, 0.7824, 0.7800, 0.7803, 0.7796]
  val_policy_top5_acc : [0.6079, 0.6070, 0.6100, 0.6174, 0.6134, 0.6153, 0.6165, 0.6145, 0.6189, 0.6165]
  val_value_loss : [0.2018, 0.2004, 0.2030, 0.2003, 0.2044, 0.2022, 0.2020, 0.2022, 0.2021, 0.2021]
  val_value_value_mse : [0.2018, 0.2005, 0.2030, 0.2004, 0.2044, 0.2023, 0.2020, 0.2022, 0.2021, 0.2021]
  value_loss : [0.2008, 0.1960, 0.1930, 0.1889, 0.1864, 0.1848, 0.1828, 0.1817, 0.1810, 0.1795]
  value_value_mse : [0.2008, 0.1960, 0.1930, 0.1888, 0.1864, 0.1848, 0.1828, 0.1817, 0.1810, 0.1795]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:29:07.778381Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game84001_game85500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.312195   (epoch 10)   mode=min
  policy_loss: 2.226198   (epoch 10)   mode=min
  policy_policy_acc: 0.331169   (epoch 10)   mode=max
  policy_top10_acc: 0.843062   (epoch 10)   mode=max
  policy_top5_acc: 0.692171   (epoch 10)   mode=max
  val_loss: 2.661959   (epoch 4)   mode=min
  val_policy_loss: 2.561246   (epoch 4)   mode=min
  val_policy_policy_acc: 0.274725   (epoch 9)   mode=max
  val_policy_top10_acc: 0.775624   (epoch 2)   mode=max
  val_policy_top5_acc: 0.609391   (epoch 3)   mode=max
  val_value_loss: 0.201505   (epoch 4)   mode=min
  val_value_value_mse: 0.201528   (epoch 4)   mode=min
  value_loss: 0.171955   (epoch 10)   mode=min
  value_value_mse: 0.171942   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6707, 2.6189, 2.5775, 2.5425, 2.5094, 2.4809, 2.4008, 2.3711, 2.3295, 2.3122]
  policy_loss : [2.5726, 2.5219, 2.4827, 2.4491, 2.4170, 2.3895, 2.3122, 2.2835, 2.2427, 2.2262]
  policy_policy_acc : [0.2627, 0.2739, 0.2804, 0.2880, 0.2935, 0.2975, 0.3139, 0.3194, 0.3291, 0.3312]
  policy_top10_acc : [0.7683, 0.7823, 0.7907, 0.7984, 0.8073, 0.8128, 0.8276, 0.8329, 0.8407, 0.8431]
  policy_top5_acc : [0.6031, 0.6171, 0.6268, 0.6371, 0.6462, 0.6536, 0.6716, 0.6776, 0.6898, 0.6922]
  val_loss : [2.7067, 2.7102, 2.6752, 2.6620, 2.6721, 2.8901, 2.9813, 2.9260, 2.7828, 2.7545]
  val_policy_loss : [2.6033, 2.6091, 2.5742, 2.5612, 2.5693, 2.7875, 2.8798, 2.8229, 2.6812, 2.6524]
  val_policy_policy_acc : [0.2677, 0.2696, 0.2681, 0.2709, 0.2692, 0.2666, 0.2699, 0.2707, 0.2747, 0.2707]
  val_policy_top10_acc : [0.7747, 0.7756, 0.7680, 0.7711, 0.7697, 0.7717, 0.7696, 0.7678, 0.7710, 0.7685]
  val_policy_top5_acc : [0.6082, 0.6080, 0.6094, 0.6057, 0.6072, 0.6054, 0.6044, 0.6060, 0.6067, 0.6089]
  val_value_loss : [0.2066, 0.2020, 0.2020, 0.2015, 0.2056, 0.2048, 0.2028, 0.2059, 0.2030, 0.2043]
  val_value_value_mse : [0.2067, 0.2020, 0.2021, 0.2015, 0.2057, 0.2048, 0.2028, 0.2060, 0.2030, 0.2043]
  value_loss : [0.1963, 0.1940, 0.1895, 0.1868, 0.1847, 0.1830, 0.1770, 0.1752, 0.1736, 0.1720]
  value_value_mse : [0.1963, 0.1940, 0.1895, 0.1868, 0.1847, 0.1830, 0.1770, 0.1752, 0.1736, 0.1719]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:38:09.486552Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game85501_game87000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.356701   (epoch 9)   mode=min
  policy_loss: 2.264992   (epoch 9)   mode=min
  policy_policy_acc: 0.327336   (epoch 9)   mode=max
  policy_top10_acc: 0.833519   (epoch 9)   mode=max
  policy_top5_acc: 0.680785   (epoch 9)   mode=max
  val_loss: 2.640140   (epoch 1)   mode=min
  val_policy_loss: 2.537851   (epoch 3)   mode=min
  val_policy_policy_acc: 0.278222   (epoch 8)   mode=max
  val_policy_top10_acc: 0.776324   (epoch 3)   mode=max
  val_policy_top5_acc: 0.616883   (epoch 8)   mode=max
  val_value_loss: 0.199324   (epoch 1)   mode=min
  val_value_value_mse: 0.199357   (epoch 1)   mode=min
  value_loss: 0.183827   (epoch 9)   mode=min
  value_value_mse: 0.183803   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6803, 2.6285, 2.5850, 2.5514, 2.5153, 2.4460, 2.4160, 2.3714, 2.3567]
  policy_loss : [2.5789, 2.5289, 2.4871, 2.4540, 2.4188, 2.3516, 2.3229, 2.2792, 2.2650]
  policy_policy_acc : [0.2643, 0.2753, 0.2840, 0.2888, 0.2972, 0.3089, 0.3156, 0.3242, 0.3273]
  policy_top10_acc : [0.7655, 0.7769, 0.7864, 0.7964, 0.8031, 0.8168, 0.8224, 0.8319, 0.8335]
  policy_top5_acc : [0.6002, 0.6158, 0.6262, 0.6346, 0.6442, 0.6604, 0.6675, 0.6793, 0.6808]
  val_loss : [2.6401, 2.6471, 2.6403, 2.6682, 2.6530, 2.6480, 2.6734, 2.6687, 2.6699]
  val_policy_loss : [2.5404, 2.5443, 2.5379, 2.5630, 2.5515, 2.5470, 2.5727, 2.5682, 2.5688]
  val_policy_policy_acc : [0.2674, 0.2702, 0.2712, 0.2682, 0.2672, 0.2743, 0.2746, 0.2782, 0.2752]
  val_policy_top10_acc : [0.7718, 0.7728, 0.7763, 0.7693, 0.7671, 0.7728, 0.7742, 0.7758, 0.7754]
  val_policy_top5_acc : [0.5993, 0.6064, 0.6105, 0.6067, 0.6075, 0.6095, 0.6115, 0.6169, 0.6152]
  val_value_loss : [0.1993, 0.2056, 0.2048, 0.2103, 0.2029, 0.2019, 0.2014, 0.2009, 0.2022]
  val_value_value_mse : [0.1994, 0.2056, 0.2049, 0.2104, 0.2030, 0.2020, 0.2014, 0.2010, 0.2023]
  value_loss : [0.2039, 0.2000, 0.1977, 0.1956, 0.1929, 0.1887, 0.1866, 0.1849, 0.1838]
  value_value_mse : [0.2039, 0.2000, 0.1978, 0.1956, 0.1929, 0.1888, 0.1867, 0.1848, 0.1838]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:46:05.740237Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game87001_game88500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.396652   (epoch 8)   mode=min
  policy_loss: 2.305389   (epoch 8)   mode=min
  policy_policy_acc: 0.317981   (epoch 8)   mode=max
  policy_top10_acc: 0.827951   (epoch 8)   mode=max
  policy_top5_acc: 0.673178   (epoch 8)   mode=max
  val_loss: 2.639678   (epoch 2)   mode=min
  val_policy_loss: 2.536743   (epoch 2)   mode=min
  val_policy_policy_acc: 0.276823   (epoch 6)   mode=max
  val_policy_top10_acc: 0.776623   (epoch 8)   mode=max
  val_policy_top5_acc: 0.614186   (epoch 5)   mode=max
  val_value_loss: 0.200853   (epoch 1)   mode=min
  val_value_value_mse: 0.200876   (epoch 1)   mode=min
  value_loss: 0.182643   (epoch 8)   mode=min
  value_value_mse: 0.182650   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6821, 2.6330, 2.5916, 2.5597, 2.4849, 2.4550, 2.4130, 2.3967]
  policy_loss : [2.5812, 2.5341, 2.4940, 2.4631, 2.3912, 2.3621, 2.3211, 2.3054]
  policy_policy_acc : [0.2625, 0.2718, 0.2790, 0.2846, 0.3003, 0.3057, 0.3131, 0.3180]
  policy_top10_acc : [0.7668, 0.7794, 0.7879, 0.7955, 0.8111, 0.8168, 0.8245, 0.8280]
  policy_top5_acc : [0.6007, 0.6121, 0.6227, 0.6322, 0.6520, 0.6591, 0.6698, 0.6732]
  val_loss : [2.6462, 2.6397, 2.6483, 2.6530, 2.6554, 2.6522, 2.6677, 2.6632]
  val_policy_loss : [2.5458, 2.5367, 2.5468, 2.5497, 2.5534, 2.5506, 2.5659, 2.5613]
  val_policy_policy_acc : [0.2697, 0.2699, 0.2654, 0.2682, 0.2740, 0.2768, 0.2760, 0.2765]
  val_policy_top10_acc : [0.7741, 0.7739, 0.7730, 0.7759, 0.7746, 0.7752, 0.7763, 0.7766]
  val_policy_top5_acc : [0.6130, 0.6091, 0.6061, 0.6026, 0.6142, 0.6122, 0.6100, 0.6125]
  val_value_loss : [0.2009, 0.2059, 0.2029, 0.2065, 0.2040, 0.2032, 0.2037, 0.2039]
  val_value_value_mse : [0.2009, 0.2059, 0.2030, 0.2065, 0.2041, 0.2032, 0.2037, 0.2039]
  value_loss : [0.2019, 0.1980, 0.1950, 0.1931, 0.1872, 0.1855, 0.1836, 0.1826]
  value_value_mse : [0.2019, 0.1980, 0.1950, 0.1931, 0.1872, 0.1855, 0.1836, 0.1826]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T00:55:25.817232Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game88501_game90000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.367658   (epoch 9)   mode=min
  policy_loss: 2.276873   (epoch 9)   mode=min
  policy_policy_acc: 0.320269   (epoch 9)   mode=max
  policy_top10_acc: 0.833866   (epoch 9)   mode=max
  policy_top5_acc: 0.680844   (epoch 9)   mode=max
  val_loss: 2.645187   (epoch 3)   mode=min
  val_policy_loss: 2.544438   (epoch 3)   mode=min
  val_policy_policy_acc: 0.273626   (epoch 3)   mode=max
  val_policy_top10_acc: 0.778022   (epoch 3)   mode=max
  val_policy_top5_acc: 0.612587   (epoch 6)   mode=max
  val_value_loss: 0.201446   (epoch 3)   mode=min
  val_value_value_mse: 0.201468   (epoch 3)   mode=min
  value_loss: 0.181591   (epoch 9)   mode=min
  value_value_mse: 0.181591   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6775, 2.6302, 2.5908, 2.5576, 2.5256, 2.4536, 2.4281, 2.3851, 2.3677]
  policy_loss : [2.5766, 2.5310, 2.4927, 2.4613, 2.4302, 2.3600, 2.3355, 2.2938, 2.2769]
  policy_policy_acc : [0.2630, 0.2714, 0.2780, 0.2824, 0.2899, 0.3031, 0.3085, 0.3183, 0.3203]
  policy_top10_acc : [0.7665, 0.7790, 0.7884, 0.7967, 0.8037, 0.8177, 0.8223, 0.8306, 0.8339]
  policy_top5_acc : [0.6008, 0.6137, 0.6234, 0.6332, 0.6404, 0.6604, 0.6668, 0.6773, 0.6808]
  val_loss : [2.6740, 2.6485, 2.6452, 2.6743, 2.6595, 2.6592, 3.1541, 3.0081, 2.8453]
  val_policy_loss : [2.5730, 2.5478, 2.5444, 2.5731, 2.5574, 2.5577, 3.0525, 2.9066, 2.7437]
  val_policy_policy_acc : [0.2632, 0.2666, 0.2736, 0.2646, 0.2711, 0.2715, 0.2699, 0.2716, 0.2732]
  val_policy_top10_acc : [0.7632, 0.7730, 0.7780, 0.7667, 0.7688, 0.7768, 0.7758, 0.7746, 0.7777]
  val_policy_top5_acc : [0.5968, 0.6058, 0.6115, 0.5980, 0.6005, 0.6126, 0.6116, 0.6095, 0.6114]
  val_value_loss : [0.2019, 0.2015, 0.2014, 0.2026, 0.2043, 0.2031, 0.2025, 0.2026, 0.2029]
  val_value_value_mse : [0.2020, 0.2015, 0.2015, 0.2026, 0.2043, 0.2031, 0.2025, 0.2027, 0.2029]
  value_loss : [0.2020, 0.1983, 0.1963, 0.1927, 0.1908, 0.1870, 0.1853, 0.1825, 0.1816]
  value_value_mse : [0.2020, 0.1983, 0.1963, 0.1927, 0.1908, 0.1870, 0.1853, 0.1825, 0.1816]

================================================================================

History file: model_versions/chess_elo_model_V7_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:04:40.782795Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game90001_game91500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V6
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.362048   (epoch 9)   mode=min
  policy_loss: 2.268316   (epoch 9)   mode=min
  policy_policy_acc: 0.322781   (epoch 9)   mode=max
  policy_top10_acc: 0.836403   (epoch 9)   mode=max
  policy_top5_acc: 0.682801   (epoch 9)   mode=max
  val_loss: 2.653831   (epoch 3)   mode=min
  val_policy_loss: 2.552818   (epoch 3)   mode=min
  val_policy_policy_acc: 0.279321   (epoch 9)   mode=max
  val_policy_top10_acc: 0.776623   (epoch 6)   mode=max
  val_policy_top5_acc: 0.617782   (epoch 8)   mode=max
  val_value_loss: 0.201790   (epoch 3)   mode=min
  val_value_value_mse: 0.201810   (epoch 3)   mode=min
  value_loss: 0.187284   (epoch 9)   mode=min
  value_value_mse: 0.187280   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6788, 2.6267, 2.5906, 2.5529, 2.5204, 2.4538, 2.4236, 2.3829, 2.3620]
  policy_loss : [2.5747, 2.5250, 2.4902, 2.4538, 2.4221, 2.3575, 2.3285, 2.2887, 2.2683]
  policy_policy_acc : [0.2612, 0.2723, 0.2780, 0.2862, 0.2914, 0.3047, 0.3110, 0.3187, 0.3228]
  policy_top10_acc : [0.7685, 0.7805, 0.7891, 0.7978, 0.8044, 0.8182, 0.8254, 0.8330, 0.8364]
  policy_top5_acc : [0.6002, 0.6145, 0.6256, 0.6353, 0.6448, 0.6620, 0.6685, 0.6785, 0.6828]
  val_loss : [3.1322, 2.6609, 2.6538, 2.6950, 2.8034, 3.1099, 3.0102, 3.2210, 3.3502]
  val_policy_loss : [3.0288, 2.5555, 2.5528, 2.5922, 2.6991, 3.0058, 2.9051, 3.1165, 3.2463]
  val_policy_policy_acc : [0.2700, 0.2771, 0.2705, 0.2708, 0.2668, 0.2725, 0.2756, 0.2774, 0.2793]
  val_policy_top10_acc : [0.7713, 0.7719, 0.7765, 0.7757, 0.7718, 0.7766, 0.7736, 0.7750, 0.7739]
  val_policy_top5_acc : [0.6061, 0.6138, 0.6114, 0.6151, 0.6084, 0.6130, 0.6129, 0.6178, 0.6143]
  val_value_loss : [0.2062, 0.2107, 0.2018, 0.2055, 0.2083, 0.2074, 0.2096, 0.2082, 0.2069]
  val_value_value_mse : [0.2063, 0.2108, 0.2018, 0.2056, 0.2083, 0.2074, 0.2096, 0.2082, 0.2069]
  value_loss : [0.2083, 0.2036, 0.2009, 0.1982, 0.1970, 0.1923, 0.1904, 0.1886, 0.1873]
  value_value_mse : [0.2083, 0.2036, 0.2009, 0.1982, 0.1970, 0.1923, 0.1904, 0.1885, 0.1873]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:12:34.165807Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game91501_game93000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.392896   (epoch 8)   mode=min
  policy_loss: 2.296603   (epoch 8)   mode=min
  policy_policy_acc: 0.316283   (epoch 7)   mode=max
  policy_top10_acc: 0.828651   (epoch 8)   mode=max
  policy_top5_acc: 0.676767   (epoch 8)   mode=max
  val_loss: 2.635540   (epoch 2)   mode=min
  val_policy_loss: 2.533057   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281419   (epoch 7)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 5)   mode=max
  val_policy_top5_acc: 0.617882   (epoch 5)   mode=max
  val_value_loss: 0.204083   (epoch 5)   mode=min
  val_value_value_mse: 0.204107   (epoch 5)   mode=min
  value_loss: 0.192583   (epoch 8)   mode=min
  value_value_mse: 0.192584   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6671, 2.6218, 2.5792, 2.5456, 2.4760, 2.4481, 2.4073, 2.3929]
  policy_loss : [2.5621, 2.5183, 2.4772, 2.4451, 2.3773, 2.3502, 2.3107, 2.2966]
  policy_policy_acc : [0.2652, 0.2728, 0.2810, 0.2886, 0.3014, 0.3064, 0.3163, 0.3162]
  policy_top10_acc : [0.7699, 0.7804, 0.7912, 0.7987, 0.8108, 0.8191, 0.8261, 0.8287]
  policy_top5_acc : [0.6049, 0.6168, 0.6286, 0.6381, 0.6545, 0.6630, 0.6725, 0.6768]
  val_loss : [2.7499, 2.6355, 2.6394, 2.8294, 2.7187, 2.8095, 2.7654, 2.8453]
  val_policy_loss : [2.6473, 2.5331, 2.5355, 2.7246, 2.6165, 2.7070, 2.6630, 2.7426]
  val_policy_policy_acc : [0.2672, 0.2691, 0.2704, 0.2718, 0.2787, 0.2782, 0.2814, 0.2797]
  val_policy_top10_acc : [0.7776, 0.7770, 0.7720, 0.7728, 0.7814, 0.7790, 0.7774, 0.7803]
  val_policy_top5_acc : [0.6162, 0.6115, 0.6082, 0.6079, 0.6179, 0.6139, 0.6177, 0.6155]
  val_value_loss : [0.2049, 0.2048, 0.2077, 0.2094, 0.2041, 0.2048, 0.2044, 0.2049]
  val_value_value_mse : [0.2049, 0.2048, 0.2077, 0.2094, 0.2041, 0.2048, 0.2044, 0.2050]
  value_loss : [0.2100, 0.2069, 0.2040, 0.2011, 0.1975, 0.1958, 0.1931, 0.1926]
  value_value_mse : [0.2100, 0.2069, 0.2040, 0.2011, 0.1975, 0.1958, 0.1931, 0.1926]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:22:28.877783Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game93001_game94500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.324352   (epoch 10)   mode=min
  policy_loss: 2.230305   (epoch 10)   mode=min
  policy_policy_acc: 0.330618   (epoch 10)   mode=max
  policy_top10_acc: 0.842182   (epoch 10)   mode=max
  policy_top5_acc: 0.693303   (epoch 10)   mode=max
  val_loss: 2.631404   (epoch 4)   mode=min
  val_policy_loss: 2.531205   (epoch 4)   mode=min
  val_policy_policy_acc: 0.275624   (epoch 7)   mode=max
  val_policy_top10_acc: 0.775924   (epoch 10)   mode=max
  val_policy_top5_acc: 0.615884   (epoch 9)   mode=max
  val_value_loss: 0.200296   (epoch 4)   mode=min
  val_value_value_mse: 0.200316   (epoch 4)   mode=min
  value_loss: 0.187871   (epoch 9)   mode=min
  value_value_mse: 0.187867   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6536, 2.6090, 2.5694, 2.5405, 2.5096, 2.4788, 2.4132, 2.3811, 2.3426, 2.3244]
  policy_loss : [2.5492, 2.5063, 2.4675, 2.4399, 2.4101, 2.3804, 2.3166, 2.2856, 2.2486, 2.2303]
  policy_policy_acc : [0.2664, 0.2746, 0.2819, 0.2884, 0.2937, 0.3007, 0.3134, 0.3191, 0.3281, 0.3306]
  policy_top10_acc : [0.7725, 0.7842, 0.7933, 0.7992, 0.8065, 0.8126, 0.8250, 0.8316, 0.8387, 0.8422]
  policy_top5_acc : [0.6085, 0.6216, 0.6320, 0.6403, 0.6482, 0.6552, 0.6722, 0.6784, 0.6880, 0.6933]
  val_loss : [2.7354, 2.8272, 2.6416, 2.6314, 2.6472, 2.6640, 2.6571, 2.6646, 2.6849, 2.6848]
  val_policy_loss : [2.6319, 2.7256, 2.5392, 2.5312, 2.5454, 2.5617, 2.5542, 2.5630, 2.5835, 2.5839]
  val_policy_policy_acc : [0.2667, 0.2698, 0.2684, 0.2729, 0.2741, 0.2675, 0.2756, 0.2745, 0.2740, 0.2747]
  val_policy_top10_acc : [0.7706, 0.7741, 0.7674, 0.7713, 0.7721, 0.7714, 0.7753, 0.7723, 0.7738, 0.7759]
  val_policy_top5_acc : [0.6074, 0.6103, 0.6103, 0.6143, 0.6124, 0.6096, 0.6139, 0.6155, 0.6159, 0.6116]
  val_value_loss : [0.2067, 0.2028, 0.2048, 0.2003, 0.2033, 0.2042, 0.2056, 0.2029, 0.2026, 0.2015]
  val_value_value_mse : [0.2067, 0.2028, 0.2048, 0.2003, 0.2033, 0.2042, 0.2056, 0.2029, 0.2026, 0.2016]
  value_loss : [0.2087, 0.2055, 0.2038, 0.2014, 0.1990, 0.1967, 0.1931, 0.1911, 0.1879, 0.1880]
  value_value_mse : [0.2086, 0.2055, 0.2038, 0.2014, 0.1990, 0.1967, 0.1931, 0.1911, 0.1879, 0.1880]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:29:30.456792Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game94501_game96000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.440436   (epoch 7)   mode=min
  policy_loss: 2.342933   (epoch 7)   mode=min
  policy_policy_acc: 0.309715   (epoch 7)   mode=max
  policy_top10_acc: 0.820888   (epoch 7)   mode=max
  policy_top5_acc: 0.663129   (epoch 7)   mode=max
  val_loss: 2.623655   (epoch 1)   mode=min
  val_policy_loss: 2.516567   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280420   (epoch 7)   mode=max
  val_policy_top10_acc: 0.779021   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620180   (epoch 7)   mode=max
  val_value_loss: 0.204320   (epoch 7)   mode=min
  val_value_value_mse: 0.204314   (epoch 7)   mode=min
  value_loss: 0.195167   (epoch 7)   mode=min
  value_value_mse: 0.195187   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6777, 2.6291, 2.5942, 2.5225, 2.4967, 2.4586, 2.4404]
  policy_loss : [2.5733, 2.5263, 2.4925, 2.4224, 2.3974, 2.3602, 2.3429]
  policy_policy_acc : [0.2651, 0.2736, 0.2803, 0.2946, 0.2986, 0.3068, 0.3097]
  policy_top10_acc : [0.7692, 0.7808, 0.7894, 0.8024, 0.8100, 0.8167, 0.8209]
  policy_top5_acc : [0.6027, 0.6144, 0.6252, 0.6432, 0.6509, 0.6606, 0.6631]
  val_loss : [2.6237, 2.6341, 2.6464, 2.6242, 2.6654, 2.6423, 2.6536]
  val_policy_loss : [2.5166, 2.5308, 2.5421, 2.5213, 2.5627, 2.5397, 2.5514]
  val_policy_policy_acc : [0.2735, 0.2728, 0.2713, 0.2769, 0.2737, 0.2797, 0.2804]
  val_policy_top10_acc : [0.7778, 0.7750, 0.7722, 0.7778, 0.7772, 0.7766, 0.7790]
  val_policy_top5_acc : [0.6164, 0.6147, 0.6104, 0.6174, 0.6173, 0.6193, 0.6202]
  val_value_loss : [0.2141, 0.2064, 0.2085, 0.2057, 0.2053, 0.2051, 0.2043]
  val_value_value_mse : [0.2141, 0.2064, 0.2085, 0.2057, 0.2053, 0.2050, 0.2043]
  value_loss : [0.2089, 0.2057, 0.2036, 0.2000, 0.1985, 0.1968, 0.1952]
  value_value_mse : [0.2089, 0.2057, 0.2036, 0.2000, 0.1985, 0.1968, 0.1952]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:37:28.748539Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game96001_game97500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.412015   (epoch 8)   mode=min
  policy_loss: 2.315985   (epoch 8)   mode=min
  policy_policy_acc: 0.315957   (epoch 8)   mode=max
  policy_top10_acc: 0.825444   (epoch 8)   mode=max
  policy_top5_acc: 0.670730   (epoch 8)   mode=max
  val_loss: 2.624533   (epoch 2)   mode=min
  val_policy_loss: 2.524081   (epoch 2)   mode=min
  val_policy_policy_acc: 0.280819   (epoch 8)   mode=max
  val_policy_top10_acc: 0.777323   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620080   (epoch 8)   mode=max
  val_value_loss: 0.200695   (epoch 2)   mode=min
  val_value_value_mse: 0.200712   (epoch 2)   mode=min
  value_loss: 0.192069   (epoch 8)   mode=min
  value_value_mse: 0.192069   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6786, 2.6315, 2.5944, 2.5596, 2.4936, 2.4664, 2.4304, 2.4120]
  policy_loss : [2.5734, 2.5283, 2.4925, 2.4591, 2.3950, 2.3686, 2.3337, 2.3160]
  policy_policy_acc : [0.2643, 0.2727, 0.2812, 0.2866, 0.3006, 0.3054, 0.3121, 0.3160]
  policy_top10_acc : [0.7683, 0.7798, 0.7869, 0.7962, 0.8094, 0.8147, 0.8216, 0.8254]
  policy_top5_acc : [0.6025, 0.6158, 0.6262, 0.6331, 0.6521, 0.6581, 0.6658, 0.6707]
  val_loss : [2.6337, 2.6245, 2.6282, 2.6987, 2.6298, 2.6371, 2.6333, 2.6426]
  val_policy_loss : [2.5301, 2.5241, 2.5263, 2.5965, 2.5280, 2.5355, 2.5320, 2.5415]
  val_policy_policy_acc : [0.2695, 0.2731, 0.2689, 0.2705, 0.2775, 0.2775, 0.2785, 0.2808]
  val_policy_top10_acc : [0.7750, 0.7755, 0.7739, 0.7757, 0.7762, 0.7772, 0.7773, 0.7753]
  val_policy_top5_acc : [0.6135, 0.6119, 0.6135, 0.6143, 0.6164, 0.6137, 0.6175, 0.6201]
  val_value_loss : [0.2072, 0.2007, 0.2037, 0.2041, 0.2034, 0.2031, 0.2023, 0.2022]
  val_value_value_mse : [0.2072, 0.2007, 0.2037, 0.2041, 0.2035, 0.2032, 0.2023, 0.2023]
  value_loss : [0.2105, 0.2064, 0.2036, 0.2010, 0.1972, 0.1957, 0.1934, 0.1921]
  value_value_mse : [0.2105, 0.2064, 0.2036, 0.2010, 0.1972, 0.1957, 0.1934, 0.1921]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:44:37.356094Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game97501_game99000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.443300   (epoch 7)   mode=min
  policy_loss: 2.351700   (epoch 7)   mode=min
  policy_policy_acc: 0.308790   (epoch 7)   mode=max
  policy_top10_acc: 0.816803   (epoch 7)   mode=max
  policy_top5_acc: 0.660988   (epoch 7)   mode=max
  val_loss: 2.615994   (epoch 1)   mode=min
  val_policy_loss: 2.513416   (epoch 1)   mode=min
  val_policy_policy_acc: 0.279021   (epoch 7)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 7)   mode=max
  val_policy_top5_acc: 0.624476   (epoch 4)   mode=max
  val_value_loss: 0.201135   (epoch 4)   mode=min
  val_value_value_mse: 0.201156   (epoch 4)   mode=min
  value_loss: 0.183172   (epoch 7)   mode=min
  value_value_mse: 0.183171   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6801, 2.6312, 2.5940, 2.5284, 2.4990, 2.4576, 2.4433]
  policy_loss : [2.5786, 2.5316, 2.4961, 2.4333, 2.4051, 2.3651, 2.3517]
  policy_policy_acc : [0.2621, 0.2729, 0.2781, 0.2915, 0.2966, 0.3056, 0.3088]
  policy_top10_acc : [0.7677, 0.7778, 0.7876, 0.7997, 0.8074, 0.8143, 0.8168]
  policy_top5_acc : [0.6004, 0.6132, 0.6216, 0.6374, 0.6462, 0.6558, 0.6610]
  val_loss : [2.6160, 2.7963, 2.8539, 3.0056, 2.8724, 2.8404, 2.9457]
  val_policy_loss : [2.5134, 2.6916, 2.7523, 2.9047, 2.7703, 2.7387, 2.8445]
  val_policy_policy_acc : [0.2748, 0.2666, 0.2677, 0.2785, 0.2778, 0.2786, 0.2790]
  val_policy_top10_acc : [0.7807, 0.7743, 0.7745, 0.7828, 0.7798, 0.7843, 0.7851]
  val_policy_top5_acc : [0.6190, 0.6086, 0.6061, 0.6245, 0.6197, 0.6195, 0.6202]
  val_value_loss : [0.2050, 0.2090, 0.2026, 0.2011, 0.2038, 0.2029, 0.2020]
  val_value_value_mse : [0.2050, 0.2090, 0.2026, 0.2012, 0.2038, 0.2030, 0.2020]
  value_loss : [0.2030, 0.1992, 0.1958, 0.1903, 0.1878, 0.1849, 0.1832]
  value_value_mse : [0.2030, 0.1992, 0.1958, 0.1903, 0.1878, 0.1849, 0.1832]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T01:53:00.829256Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game99001_game100500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399648   (epoch 8)   mode=min
  policy_loss: 2.310140   (epoch 8)   mode=min
  policy_policy_acc: 0.316848   (epoch 8)   mode=max
  policy_top10_acc: 0.826767   (epoch 8)   mode=max
  policy_top5_acc: 0.673123   (epoch 8)   mode=max
  val_loss: 2.637400   (epoch 2)   mode=min
  val_policy_loss: 2.534804   (epoch 2)   mode=min
  val_policy_policy_acc: 0.280619   (epoch 8)   mode=max
  val_policy_top10_acc: 0.779920   (epoch 5)   mode=max
  val_policy_top5_acc: 0.619181   (epoch 7)   mode=max
  val_value_loss: 0.202107   (epoch 7)   mode=min
  val_value_value_mse: 0.202120   (epoch 7)   mode=min
  value_loss: 0.180086   (epoch 8)   mode=min
  value_value_mse: 0.180070   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6623, 2.6185, 2.5793, 2.5502, 2.4835, 2.4568, 2.4137, 2.3996]
  policy_loss : [2.5640, 2.5211, 2.4831, 2.4555, 2.3904, 2.3652, 2.3230, 2.3101]
  policy_policy_acc : [0.2660, 0.2743, 0.2803, 0.2864, 0.3004, 0.3045, 0.3133, 0.3168]
  policy_top10_acc : [0.7712, 0.7812, 0.7913, 0.7964, 0.8104, 0.8166, 0.8246, 0.8268]
  policy_top5_acc : [0.6044, 0.6166, 0.6274, 0.6346, 0.6522, 0.6580, 0.6690, 0.6731]
  val_loss : [2.6393, 2.6374, 3.0214, 2.6848, 2.6970, 2.7926, 2.7202, 2.6634]
  val_policy_loss : [2.5374, 2.5348, 2.9186, 2.5813, 2.5936, 2.6896, 2.6191, 2.5618]
  val_policy_policy_acc : [0.2711, 0.2716, 0.2705, 0.2711, 0.2751, 0.2751, 0.2750, 0.2806]
  val_policy_top10_acc : [0.7768, 0.7721, 0.7764, 0.7767, 0.7799, 0.7771, 0.7770, 0.7797]
  val_policy_top5_acc : [0.6121, 0.6050, 0.6102, 0.6070, 0.6176, 0.6138, 0.6192, 0.6147]
  val_value_loss : [0.2035, 0.2050, 0.2051, 0.2067, 0.2065, 0.2056, 0.2021, 0.2030]
  val_value_value_mse : [0.2035, 0.2050, 0.2051, 0.2067, 0.2065, 0.2057, 0.2021, 0.2030]
  value_loss : [0.1972, 0.1944, 0.1921, 0.1892, 0.1852, 0.1830, 0.1814, 0.1801]
  value_value_mse : [0.1972, 0.1944, 0.1921, 0.1892, 0.1853, 0.1831, 0.1814, 0.1801]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:03:11.551593Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game100501_game102000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.365098   (epoch 10)   mode=min
  policy_loss: 2.271874   (epoch 10)   mode=min
  policy_policy_acc: 0.325807   (epoch 10)   mode=max
  policy_top10_acc: 0.834549   (epoch 10)   mode=max
  policy_top5_acc: 0.681759   (epoch 10)   mode=max
  val_loss: 2.635466   (epoch 1)   mode=min
  val_policy_loss: 2.532239   (epoch 4)   mode=min
  val_policy_policy_acc: 0.278721   (epoch 9)   mode=max
  val_policy_top10_acc: 0.780519   (epoch 9)   mode=max
  val_policy_top5_acc: 0.622178   (epoch 10)   mode=max
  val_value_loss: 0.202554   (epoch 10)   mode=min
  val_value_value_mse: 0.202596   (epoch 10)   mode=min
  value_loss: 0.186432   (epoch 10)   mode=min
  value_value_mse: 0.186436   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6448, 2.5992, 2.5625, 2.4972, 2.4690, 2.4471, 2.4067, 2.3912, 2.3725, 2.3651]
  policy_loss : [2.5418, 2.4984, 2.4625, 2.3998, 2.3728, 2.3517, 2.3125, 2.2972, 2.2787, 2.2719]
  policy_policy_acc : [0.2718, 0.2795, 0.2856, 0.2974, 0.3038, 0.3098, 0.3156, 0.3211, 0.3239, 0.3258]
  policy_top10_acc : [0.7770, 0.7877, 0.7944, 0.8081, 0.8142, 0.8190, 0.8259, 0.8283, 0.8325, 0.8345]
  policy_top5_acc : [0.6129, 0.6249, 0.6355, 0.6511, 0.6578, 0.6623, 0.6735, 0.6773, 0.6814, 0.6818]
  val_loss : [2.6355, 2.6384, 2.6984, 2.6357, 2.6447, 2.7037, 2.9033, 2.8812, 2.7073, 2.6662]
  val_policy_loss : [2.5337, 2.5349, 2.5939, 2.5322, 2.5422, 2.6013, 2.8015, 2.7782, 2.6057, 2.5648]
  val_policy_policy_acc : [0.2718, 0.2740, 0.2734, 0.2782, 0.2747, 0.2755, 0.2782, 0.2775, 0.2787, 0.2784]
  val_policy_top10_acc : [0.7691, 0.7737, 0.7763, 0.7751, 0.7790, 0.7774, 0.7780, 0.7790, 0.7805, 0.7777]
  val_policy_top5_acc : [0.6108, 0.6131, 0.6108, 0.6183, 0.6168, 0.6164, 0.6183, 0.6154, 0.6193, 0.6222]
  val_value_loss : [0.2034, 0.2069, 0.2089, 0.2068, 0.2049, 0.2047, 0.2033, 0.2056, 0.2031, 0.2026]
  val_value_value_mse : [0.2034, 0.2069, 0.2089, 0.2068, 0.2049, 0.2047, 0.2033, 0.2056, 0.2031, 0.2026]
  value_loss : [0.2057, 0.2018, 0.2001, 0.1949, 0.1924, 0.1905, 0.1886, 0.1879, 0.1876, 0.1864]
  value_value_mse : [0.2057, 0.2018, 0.2001, 0.1949, 0.1924, 0.1905, 0.1886, 0.1879, 0.1876, 0.1864]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:12:13.583111Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game102001_game103500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.369014   (epoch 9)   mode=min
  policy_loss: 2.279744   (epoch 9)   mode=min
  policy_policy_acc: 0.322959   (epoch 9)   mode=max
  policy_top10_acc: 0.831271   (epoch 9)   mode=max
  policy_top5_acc: 0.679643   (epoch 9)   mode=max
  val_loss: 2.629452   (epoch 3)   mode=min
  val_policy_loss: 2.527129   (epoch 3)   mode=min
  val_policy_policy_acc: 0.272627   (epoch 2)   mode=max
  val_policy_top10_acc: 0.779121   (epoch 3)   mode=max
  val_policy_top5_acc: 0.616184   (epoch 3)   mode=max
  val_value_loss: 0.202139   (epoch 4)   mode=min
  val_value_value_mse: 0.202158   (epoch 4)   mode=min
  value_loss: 0.178486   (epoch 9)   mode=min
  value_value_mse: 0.178488   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6712, 2.6220, 2.5835, 2.5516, 2.5168, 2.4537, 2.4254, 2.3853, 2.3690]
  policy_loss : [2.5719, 2.5245, 2.4871, 2.4561, 2.4222, 2.3614, 2.3342, 2.2955, 2.2797]
  policy_policy_acc : [0.2652, 0.2743, 0.2809, 0.2872, 0.2935, 0.3081, 0.3138, 0.3205, 0.3230]
  policy_top10_acc : [0.7677, 0.7797, 0.7886, 0.7969, 0.8037, 0.8158, 0.8215, 0.8299, 0.8313]
  policy_top5_acc : [0.6035, 0.6162, 0.6278, 0.6366, 0.6448, 0.6607, 0.6666, 0.6775, 0.6796]
  val_loss : [2.6430, 2.6370, 2.6295, 2.6431, 2.7211, 2.6587, 2.7410, 2.7311, 2.7126]
  val_policy_loss : [2.5418, 2.5358, 2.5271, 2.5421, 2.6195, 2.5570, 2.6390, 2.6291, 2.6104]
  val_policy_policy_acc : [0.2688, 0.2726, 0.2726, 0.2705, 0.2683, 0.2691, 0.2667, 0.2694, 0.2707]
  val_policy_top10_acc : [0.7711, 0.7738, 0.7791, 0.7756, 0.7759, 0.7764, 0.7763, 0.7776, 0.7760]
  val_policy_top5_acc : [0.6047, 0.6111, 0.6162, 0.6097, 0.6118, 0.6135, 0.6161, 0.6124, 0.6132]
  val_value_loss : [0.2023, 0.2023, 0.2045, 0.2021, 0.2030, 0.2035, 0.2039, 0.2038, 0.2043]
  val_value_value_mse : [0.2023, 0.2024, 0.2045, 0.2022, 0.2030, 0.2035, 0.2039, 0.2038, 0.2043]
  value_loss : [0.1986, 0.1949, 0.1929, 0.1910, 0.1892, 0.1846, 0.1824, 0.1798, 0.1785]
  value_value_mse : [0.1986, 0.1949, 0.1929, 0.1910, 0.1893, 0.1846, 0.1825, 0.1798, 0.1785]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:18:54.881782Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game103501_game105000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451398   (epoch 7)   mode=min
  policy_loss: 2.355455   (epoch 7)   mode=min
  policy_policy_acc: 0.312511   (epoch 7)   mode=max
  policy_top10_acc: 0.816799   (epoch 7)   mode=max
  policy_top5_acc: 0.663608   (epoch 7)   mode=max
  val_loss: 2.640680   (epoch 1)   mode=min
  val_policy_loss: 2.534267   (epoch 1)   mode=min
  val_policy_policy_acc: 0.276424   (epoch 6)   mode=max
  val_policy_top10_acc: 0.781119   (epoch 6)   mode=max
  val_policy_top5_acc: 0.622378   (epoch 6)   mode=max
  val_value_loss: 0.204734   (epoch 7)   mode=min
  val_value_value_mse: 0.204750   (epoch 7)   mode=min
  value_loss: 0.191998   (epoch 7)   mode=min
  value_value_mse: 0.191993   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6872, 2.6395, 2.6009, 2.5344, 2.5083, 2.4689, 2.4514]
  policy_loss : [2.5835, 2.5372, 2.4999, 2.4355, 2.4108, 2.3724, 2.3555]
  policy_policy_acc : [0.2642, 0.2739, 0.2815, 0.2928, 0.2979, 0.3068, 0.3125]
  policy_top10_acc : [0.7668, 0.7791, 0.7874, 0.8007, 0.8058, 0.8139, 0.8168]
  policy_top5_acc : [0.6043, 0.6159, 0.6266, 0.6419, 0.6489, 0.6579, 0.6636]
  val_loss : [2.6407, 2.7365, 2.8909, 2.8187, 2.7441, 3.0518, 2.8304]
  val_policy_loss : [2.5343, 2.6306, 2.7882, 2.7156, 2.6409, 2.9487, 2.7279]
  val_policy_policy_acc : [0.2688, 0.2658, 0.2714, 0.2740, 0.2730, 0.2764, 0.2749]
  val_policy_top10_acc : [0.7770, 0.7690, 0.7741, 0.7795, 0.7756, 0.7811, 0.7797]
  val_policy_top5_acc : [0.6134, 0.6047, 0.6111, 0.6217, 0.6174, 0.6224, 0.6204]
  val_value_loss : [0.2129, 0.2116, 0.2051, 0.2059, 0.2062, 0.2058, 0.2047]
  val_value_value_mse : [0.2129, 0.2116, 0.2051, 0.2059, 0.2062, 0.2058, 0.2047]
  value_loss : [0.2074, 0.2045, 0.2019, 0.1976, 0.1950, 0.1930, 0.1920]
  value_value_mse : [0.2074, 0.2045, 0.2019, 0.1976, 0.1950, 0.1930, 0.1920]

================================================================================

History file: model_versions/chess_elo_model_V8_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:28:46.113428Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game105001_game106500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V7
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.369524   (epoch 10)   mode=min
  policy_loss: 2.275046   (epoch 10)   mode=min
  policy_policy_acc: 0.324654   (epoch 10)   mode=max
  policy_top10_acc: 0.831886   (epoch 10)   mode=max
  policy_top5_acc: 0.681691   (epoch 10)   mode=max
  val_loss: 2.624574   (epoch 5)   mode=min
  val_policy_loss: 2.523377   (epoch 5)   mode=min
  val_policy_policy_acc: 0.280320   (epoch 7)   mode=max
  val_policy_top10_acc: 0.778621   (epoch 1)   mode=max
  val_policy_top5_acc: 0.621578   (epoch 10)   mode=max
  val_value_loss: 0.200916   (epoch 10)   mode=min
  val_value_value_mse: 0.200910   (epoch 10)   mode=min
  value_loss: 0.188650   (epoch 10)   mode=min
  value_value_mse: 0.188659   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6662, 2.6165, 2.5808, 2.5141, 2.4864, 2.4656, 2.4469, 2.4041, 2.3934, 2.3695]
  policy_loss : [2.5619, 2.5147, 2.4800, 2.4158, 2.3892, 2.3688, 2.3505, 2.3094, 2.2990, 2.2750]
  policy_policy_acc : [0.2668, 0.2763, 0.2832, 0.2959, 0.3017, 0.3056, 0.3100, 0.3184, 0.3213, 0.3247]
  policy_top10_acc : [0.7701, 0.7811, 0.7902, 0.8018, 0.8081, 0.8133, 0.8165, 0.8246, 0.8266, 0.8319]
  policy_top5_acc : [0.6061, 0.6193, 0.6303, 0.6452, 0.6515, 0.6587, 0.6614, 0.6726, 0.6759, 0.6817]
  val_loss : [2.6251, 2.6280, 2.8818, 2.7592, 2.6246, 2.6575, 2.6298, 2.7349, 2.6598, 2.6761]
  val_policy_loss : [2.5236, 2.5250, 2.7789, 2.6575, 2.5234, 2.5563, 2.5280, 2.6334, 2.5588, 2.5755]
  val_policy_policy_acc : [0.2684, 0.2703, 0.2704, 0.2753, 0.2784, 0.2795, 0.2803, 0.2786, 0.2789, 0.2789]
  val_policy_top10_acc : [0.7786, 0.7727, 0.7734, 0.7754, 0.7773, 0.7759, 0.7777, 0.7780, 0.7741, 0.7770]
  val_policy_top5_acc : [0.6097, 0.6094, 0.6139, 0.6145, 0.6174, 0.6184, 0.6191, 0.6202, 0.6192, 0.6216]
  val_value_loss : [0.2029, 0.2058, 0.2054, 0.2029, 0.2022, 0.2021, 0.2033, 0.2026, 0.2019, 0.2009]
  val_value_value_mse : [0.2029, 0.2058, 0.2054, 0.2029, 0.2022, 0.2021, 0.2033, 0.2027, 0.2019, 0.2009]
  value_loss : [0.2083, 0.2038, 0.2018, 0.1966, 0.1951, 0.1939, 0.1928, 0.1907, 0.1895, 0.1886]
  value_value_mse : [0.2083, 0.2039, 0.2018, 0.1966, 0.1951, 0.1939, 0.1928, 0.1907, 0.1895, 0.1887]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:37:35.417897Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game106501_game108000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.368368   (epoch 9)   mode=min
  policy_loss: 2.273129   (epoch 9)   mode=min
  policy_policy_acc: 0.326372   (epoch 9)   mode=max
  policy_top10_acc: 0.831288   (epoch 9)   mode=max
  policy_top5_acc: 0.679475   (epoch 9)   mode=max
  val_loss: 2.637405   (epoch 1)   mode=min
  val_policy_loss: 2.533652   (epoch 3)   mode=min
  val_policy_policy_acc: 0.278022   (epoch 8)   mode=max
  val_policy_top10_acc: 0.778521   (epoch 2)   mode=max
  val_policy_top5_acc: 0.617483   (epoch 9)   mode=max
  val_value_loss: 0.200903   (epoch 6)   mode=min
  val_value_value_mse: 0.200915   (epoch 6)   mode=min
  value_loss: 0.190465   (epoch 9)   mode=min
  value_value_mse: 0.190465   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6523, 2.6092, 2.5721, 2.5392, 2.5114, 2.4492, 2.4237, 2.3839, 2.3684]
  policy_loss : [2.5486, 2.5068, 2.4707, 2.4386, 2.4117, 2.3514, 2.3271, 2.2882, 2.2731]
  policy_policy_acc : [0.2701, 0.2786, 0.2862, 0.2914, 0.2967, 0.3095, 0.3146, 0.3241, 0.3264]
  policy_top10_acc : [0.7700, 0.7815, 0.7899, 0.7974, 0.8032, 0.8160, 0.8207, 0.8273, 0.8313]
  policy_top5_acc : [0.6080, 0.6205, 0.6295, 0.6379, 0.6459, 0.6605, 0.6665, 0.6764, 0.6795]
  val_loss : [2.6374, 2.6430, 2.6394, 2.6718, 2.6829, 2.6642, 2.7297, 2.8050, 2.8759]
  val_policy_loss : [2.5354, 2.5409, 2.5337, 2.5691, 2.5801, 2.5637, 2.6277, 2.7035, 2.7745]
  val_policy_policy_acc : [0.2715, 0.2685, 0.2692, 0.2759, 0.2650, 0.2760, 0.2755, 0.2780, 0.2771]
  val_policy_top10_acc : [0.7766, 0.7785, 0.7770, 0.7756, 0.7632, 0.7757, 0.7756, 0.7761, 0.7760]
  val_policy_top5_acc : [0.6109, 0.6121, 0.6123, 0.6091, 0.6001, 0.6159, 0.6149, 0.6138, 0.6175]
  val_value_loss : [0.2039, 0.2039, 0.2115, 0.2053, 0.2055, 0.2009, 0.2038, 0.2026, 0.2024]
  val_value_value_mse : [0.2039, 0.2039, 0.2115, 0.2053, 0.2055, 0.2009, 0.2038, 0.2026, 0.2024]
  value_loss : [0.2074, 0.2048, 0.2029, 0.2013, 0.1994, 0.1956, 0.1931, 0.1915, 0.1905]
  value_value_mse : [0.2074, 0.2048, 0.2029, 0.2013, 0.1994, 0.1956, 0.1931, 0.1915, 0.1905]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:45:21.197375Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game108001_game109500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.408039   (epoch 8)   mode=min
  policy_loss: 2.311363   (epoch 8)   mode=min
  policy_policy_acc: 0.316289   (epoch 8)   mode=max
  policy_top10_acc: 0.828131   (epoch 8)   mode=max
  policy_top5_acc: 0.673917   (epoch 8)   mode=max
  val_loss: 2.636981   (epoch 2)   mode=min
  val_policy_loss: 2.533387   (epoch 2)   mode=min
  val_policy_policy_acc: 0.278322   (epoch 7)   mode=max
  val_policy_top10_acc: 0.780420   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618382   (epoch 8)   mode=max
  val_value_loss: 0.202767   (epoch 3)   mode=min
  val_value_value_mse: 0.202794   (epoch 3)   mode=min
  value_loss: 0.193351   (epoch 8)   mode=min
  value_value_mse: 0.193352   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6689, 2.6212, 2.5835, 2.5519, 2.4863, 2.4599, 2.4225, 2.4080]
  policy_loss : [2.5636, 2.5178, 2.4811, 2.4507, 2.3876, 2.3618, 2.3251, 2.3114]
  policy_policy_acc : [0.2653, 0.2738, 0.2809, 0.2868, 0.2994, 0.3043, 0.3124, 0.3163]
  policy_top10_acc : [0.7729, 0.7833, 0.7924, 0.7984, 0.8126, 0.8187, 0.8254, 0.8281]
  policy_top5_acc : [0.6071, 0.6203, 0.6305, 0.6399, 0.6561, 0.6614, 0.6702, 0.6739]
  val_loss : [2.6794, 2.6370, 2.7957, 2.6611, 2.7952, 2.9343, 2.9385, 2.8541]
  val_policy_loss : [2.5751, 2.5334, 2.6941, 2.5559, 2.6923, 2.8315, 2.8362, 2.7518]
  val_policy_policy_acc : [0.2731, 0.2724, 0.2722, 0.2725, 0.2766, 0.2766, 0.2783, 0.2773]
  val_policy_top10_acc : [0.7742, 0.7723, 0.7759, 0.7703, 0.7771, 0.7752, 0.7804, 0.7795]
  val_policy_top5_acc : [0.6107, 0.6078, 0.6082, 0.6087, 0.6169, 0.6149, 0.6178, 0.6184]
  val_value_loss : [0.2083, 0.2071, 0.2028, 0.2103, 0.2053, 0.2049, 0.2040, 0.2041]
  val_value_value_mse : [0.2083, 0.2071, 0.2028, 0.2103, 0.2053, 0.2049, 0.2040, 0.2041]
  value_loss : [0.2103, 0.2065, 0.2051, 0.2024, 0.1976, 0.1964, 0.1947, 0.1934]
  value_value_mse : [0.2103, 0.2065, 0.2051, 0.2024, 0.1976, 0.1964, 0.1947, 0.1934]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T02:53:19.790004Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game109501_game111000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.401926   (epoch 8)   mode=min
  policy_loss: 2.305887   (epoch 8)   mode=min
  policy_policy_acc: 0.319851   (epoch 8)   mode=max
  policy_top10_acc: 0.828242   (epoch 8)   mode=max
  policy_top5_acc: 0.673405   (epoch 8)   mode=max
  val_loss: 2.625700   (epoch 1)   mode=min
  val_policy_loss: 2.523924   (epoch 2)   mode=min
  val_policy_policy_acc: 0.280420   (epoch 6)   mode=max
  val_policy_top10_acc: 0.786014   (epoch 8)   mode=max
  val_policy_top5_acc: 0.623676   (epoch 5)   mode=max
  val_value_loss: 0.202856   (epoch 5)   mode=min
  val_value_value_mse: 0.202864   (epoch 5)   mode=min
  value_loss: 0.192034   (epoch 8)   mode=min
  value_value_mse: 0.192034   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6577, 2.6163, 2.5803, 2.5538, 2.4829, 2.4576, 2.4165, 2.4019]
  policy_loss : [2.5549, 2.5144, 2.4794, 2.4536, 2.3847, 2.3604, 2.3200, 2.3059]
  policy_policy_acc : [0.2691, 0.2776, 0.2822, 0.2899, 0.3009, 0.3074, 0.3153, 0.3199]
  policy_top10_acc : [0.7736, 0.7837, 0.7927, 0.7981, 0.8126, 0.8165, 0.8258, 0.8282]
  policy_top5_acc : [0.6076, 0.6195, 0.6287, 0.6365, 0.6533, 0.6592, 0.6682, 0.6734]
  val_loss : [2.6257, 2.6294, 2.6385, 2.6377, 2.6365, 2.6475, 2.6667, 2.6416]
  val_policy_loss : [2.5241, 2.5239, 2.5343, 2.5345, 2.5350, 2.5456, 2.5649, 2.5399]
  val_policy_policy_acc : [0.2739, 0.2696, 0.2700, 0.2706, 0.2794, 0.2804, 0.2779, 0.2793]
  val_policy_top10_acc : [0.7774, 0.7790, 0.7782, 0.7774, 0.7849, 0.7814, 0.7841, 0.7860]
  val_policy_top5_acc : [0.6103, 0.6140, 0.6090, 0.6161, 0.6237, 0.6151, 0.6199, 0.6216]
  val_value_loss : [0.2031, 0.2108, 0.2083, 0.2063, 0.2029, 0.2036, 0.2033, 0.2031]
  val_value_value_mse : [0.2031, 0.2108, 0.2083, 0.2063, 0.2029, 0.2036, 0.2034, 0.2031]
  value_loss : [0.2056, 0.2037, 0.2017, 0.2001, 0.1964, 0.1944, 0.1929, 0.1920]
  value_value_mse : [0.2056, 0.2037, 0.2017, 0.2001, 0.1964, 0.1944, 0.1929, 0.1920]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:00:28.592162Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game111001_game112500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.435600   (epoch 7)   mode=min
  policy_loss: 2.340438   (epoch 7)   mode=min
  policy_policy_acc: 0.311225   (epoch 7)   mode=max
  policy_top10_acc: 0.820175   (epoch 7)   mode=max
  policy_top5_acc: 0.664015   (epoch 7)   mode=max
  val_loss: 2.627845   (epoch 1)   mode=min
  val_policy_loss: 2.524575   (epoch 1)   mode=min
  val_policy_policy_acc: 0.282517   (epoch 6)   mode=max
  val_policy_top10_acc: 0.783117   (epoch 4)   mode=max
  val_policy_top5_acc: 0.619081   (epoch 4)   mode=max
  val_value_loss: 0.199297   (epoch 6)   mode=min
  val_value_value_mse: 0.199317   (epoch 6)   mode=min
  value_loss: 0.190054   (epoch 7)   mode=min
  value_value_mse: 0.190047   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6611, 2.6129, 2.5782, 2.5121, 2.4880, 2.4502, 2.4356]
  policy_loss : [2.5583, 2.5119, 2.4786, 2.4144, 2.3913, 2.3545, 2.3404]
  policy_policy_acc : [0.2660, 0.2764, 0.2840, 0.2960, 0.2997, 0.3084, 0.3112]
  policy_top10_acc : [0.7716, 0.7830, 0.7913, 0.8055, 0.8109, 0.8174, 0.8202]
  policy_top5_acc : [0.6059, 0.6194, 0.6286, 0.6450, 0.6510, 0.6600, 0.6640]
  val_loss : [2.6278, 2.6369, 2.6418, 2.6307, 2.6792, 2.6387, 2.6601]
  val_policy_loss : [2.5246, 2.5352, 2.5396, 2.5305, 2.5790, 2.5390, 2.5603]
  val_policy_policy_acc : [0.2738, 0.2755, 0.2751, 0.2785, 0.2793, 0.2825, 0.2801]
  val_policy_top10_acc : [0.7767, 0.7776, 0.7720, 0.7831, 0.7802, 0.7810, 0.7799]
  val_policy_top5_acc : [0.6084, 0.6104, 0.6025, 0.6191, 0.6128, 0.6154, 0.6171]
  val_value_loss : [0.2064, 0.2034, 0.2044, 0.2004, 0.2003, 0.1993, 0.1995]
  val_value_value_mse : [0.2064, 0.2035, 0.2044, 0.2004, 0.2004, 0.1993, 0.1995]
  value_loss : [0.2056, 0.2022, 0.1992, 0.1953, 0.1933, 0.1914, 0.1901]
  value_value_mse : [0.2056, 0.2022, 0.1992, 0.1953, 0.1933, 0.1914, 0.1900]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:09:03.047069Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game112501_game114000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.369588   (epoch 9)   mode=min
  policy_loss: 2.278523   (epoch 9)   mode=min
  policy_policy_acc: 0.322213   (epoch 9)   mode=max
  policy_top10_acc: 0.834453   (epoch 9)   mode=max
  policy_top5_acc: 0.681243   (epoch 9)   mode=max
  val_loss: 2.627151   (epoch 3)   mode=min
  val_policy_loss: 2.521399   (epoch 3)   mode=min
  val_policy_policy_acc: 0.277423   (epoch 8)   mode=max
  val_policy_top10_acc: 0.779820   (epoch 8)   mode=max
  val_policy_top5_acc: 0.620779   (epoch 9)   mode=max
  val_value_loss: 0.203804   (epoch 6)   mode=min
  val_value_value_mse: 0.203827   (epoch 6)   mode=min
  value_loss: 0.180681   (epoch 9)   mode=min
  value_value_mse: 0.180690   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6576, 2.6241, 2.5783, 2.5482, 2.5201, 2.4541, 2.4298, 2.3888, 2.3696]
  policy_loss : [2.5607, 2.5247, 2.4812, 2.4532, 2.4265, 2.3611, 2.3381, 2.2973, 2.2785]
  policy_policy_acc : [0.2686, 0.2765, 0.2833, 0.2885, 0.2930, 0.3065, 0.3106, 0.3180, 0.3222]
  policy_top10_acc : [0.7745, 0.7818, 0.7934, 0.8004, 0.8059, 0.8191, 0.8239, 0.8307, 0.8345]
  policy_top5_acc : [0.6086, 0.6173, 0.6310, 0.6393, 0.6434, 0.6587, 0.6674, 0.6758, 0.6812]
  val_loss : [2.6507, 2.6289, 2.6272, 2.6570, 2.7304, 2.7961, 2.6835, 2.7289, 2.7477]
  val_policy_loss : [2.5480, 2.5260, 2.5214, 2.5544, 2.6265, 2.6940, 2.5811, 2.6262, 2.6446]
  val_policy_policy_acc : [0.2686, 0.2729, 0.2773, 0.2726, 0.2708, 0.2740, 0.2736, 0.2774, 0.2768]
  val_policy_top10_acc : [0.7725, 0.7786, 0.7782, 0.7704, 0.7737, 0.7788, 0.7789, 0.7798, 0.7795]
  val_policy_top5_acc : [0.6112, 0.6182, 0.6142, 0.6083, 0.6108, 0.6137, 0.6198, 0.6205, 0.6208]
  val_value_loss : [0.2054, 0.2057, 0.2112, 0.2049, 0.2074, 0.2038, 0.2046, 0.2050, 0.2059]
  val_value_value_mse : [0.2054, 0.2057, 0.2112, 0.2049, 0.2074, 0.2038, 0.2046, 0.2051, 0.2059]
  value_loss : [0.2014, 0.1985, 0.1957, 0.1927, 0.1917, 0.1870, 0.1845, 0.1816, 0.1807]
  value_value_mse : [0.2014, 0.1985, 0.1957, 0.1927, 0.1917, 0.1870, 0.1845, 0.1816, 0.1807]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:18:29.753209Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game114001_game115500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.378739   (epoch 10)   mode=min
  policy_loss: 2.284507   (epoch 10)   mode=min
  policy_policy_acc: 0.320980   (epoch 9)   mode=max
  policy_top10_acc: 0.831537   (epoch 9)   mode=max
  policy_top5_acc: 0.679288   (epoch 10)   mode=max
  val_loss: 2.610153   (epoch 4)   mode=min
  val_policy_loss: 2.509114   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279820   (epoch 8)   mode=max
  val_policy_top10_acc: 0.784216   (epoch 8)   mode=max
  val_policy_top5_acc: 0.619580   (epoch 7)   mode=max
  val_value_loss: 0.199779   (epoch 9)   mode=min
  val_value_value_mse: 0.199796   (epoch 9)   mode=min
  value_loss: 0.188495   (epoch 10)   mode=min
  value_value_mse: 0.188490   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6535, 2.6081, 2.5733, 2.5117, 2.4813, 2.4599, 2.4201, 2.4088, 2.3846, 2.3787]
  policy_loss : [2.5503, 2.5063, 2.4726, 2.4132, 2.3843, 2.3634, 2.3247, 2.3139, 2.2901, 2.2845]
  policy_policy_acc : [0.2694, 0.2793, 0.2848, 0.2966, 0.3044, 0.3056, 0.3133, 0.3161, 0.3210, 0.3209]
  policy_top10_acc : [0.7743, 0.7860, 0.7939, 0.8061, 0.8123, 0.8165, 0.8238, 0.8263, 0.8315, 0.8315]
  policy_top5_acc : [0.6083, 0.6237, 0.6318, 0.6475, 0.6539, 0.6584, 0.6700, 0.6723, 0.6788, 0.6793]
  val_loss : [2.6193, 2.6286, 2.6360, 2.6102, 2.6246, 2.6161, 2.6242, 2.6365, 2.6186, 2.7696]
  val_policy_loss : [2.5171, 2.5253, 2.5301, 2.5091, 2.5239, 2.5158, 2.5239, 2.5365, 2.5187, 2.6690]
  val_policy_policy_acc : [0.2736, 0.2692, 0.2709, 0.2763, 0.2736, 0.2763, 0.2776, 0.2798, 0.2785, 0.2784]
  val_policy_top10_acc : [0.7812, 0.7768, 0.7769, 0.7811, 0.7801, 0.7826, 0.7823, 0.7842, 0.7839, 0.7822]
  val_policy_top5_acc : [0.6150, 0.6117, 0.6116, 0.6156, 0.6172, 0.6178, 0.6196, 0.6195, 0.6190, 0.6180]
  val_value_loss : [0.2043, 0.2064, 0.2118, 0.2018, 0.2013, 0.2002, 0.2004, 0.1999, 0.1998, 0.2008]
  val_value_value_mse : [0.2043, 0.2064, 0.2118, 0.2019, 0.2013, 0.2002, 0.2004, 0.1999, 0.1998, 0.2008]
  value_loss : [0.2062, 0.2038, 0.2015, 0.1969, 0.1943, 0.1931, 0.1907, 0.1897, 0.1889, 0.1885]
  value_value_mse : [0.2062, 0.2038, 0.2015, 0.1969, 0.1943, 0.1931, 0.1907, 0.1897, 0.1889, 0.1885]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:26:30.563385Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game115501_game117000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.412574   (epoch 8)   mode=min
  policy_loss: 2.316921   (epoch 8)   mode=min
  policy_policy_acc: 0.315079   (epoch 8)   mode=max
  policy_top10_acc: 0.825865   (epoch 8)   mode=max
  policy_top5_acc: 0.671866   (epoch 8)   mode=max
  val_loss: 2.623021   (epoch 2)   mode=min
  val_policy_loss: 2.521203   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282817   (epoch 8)   mode=max
  val_policy_top10_acc: 0.780320   (epoch 5)   mode=max
  val_policy_top5_acc: 0.619381   (epoch 6)   mode=max
  val_value_loss: 0.201570   (epoch 7)   mode=min
  val_value_value_mse: 0.201576   (epoch 7)   mode=min
  value_loss: 0.191539   (epoch 8)   mode=min
  value_value_mse: 0.191538   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6700, 2.6290, 2.5899, 2.5611, 2.4952, 2.4676, 2.4283, 2.4126]
  policy_loss : [2.5681, 2.5272, 2.4896, 2.4612, 2.3975, 2.3706, 2.3319, 2.3169]
  policy_policy_acc : [0.2646, 0.2738, 0.2799, 0.2856, 0.2993, 0.3032, 0.3118, 0.3151]
  policy_top10_acc : [0.7693, 0.7805, 0.7892, 0.7952, 0.8083, 0.8144, 0.8226, 0.8259]
  policy_top5_acc : [0.6036, 0.6176, 0.6256, 0.6338, 0.6495, 0.6580, 0.6671, 0.6719]
  val_loss : [2.6244, 2.6230, 2.7649, 2.6318, 2.6368, 2.6659, 2.6558, 2.6429]
  val_policy_loss : [2.5228, 2.5212, 2.6569, 2.5308, 2.5357, 2.5641, 2.5549, 2.5419]
  val_policy_policy_acc : [0.2719, 0.2751, 0.2724, 0.2686, 0.2783, 0.2779, 0.2819, 0.2828]
  val_policy_top10_acc : [0.7740, 0.7753, 0.7705, 0.7725, 0.7803, 0.7774, 0.7749, 0.7787]
  val_policy_top5_acc : [0.6095, 0.6102, 0.6098, 0.6116, 0.6184, 0.6194, 0.6158, 0.6187]
  val_value_loss : [0.2029, 0.2034, 0.2156, 0.2019, 0.2019, 0.2033, 0.2016, 0.2017]
  val_value_value_mse : [0.2029, 0.2034, 0.2156, 0.2019, 0.2019, 0.2034, 0.2016, 0.2017]
  value_loss : [0.2037, 0.2036, 0.2007, 0.1997, 0.1954, 0.1939, 0.1926, 0.1915]
  value_value_mse : [0.2037, 0.2036, 0.2007, 0.1997, 0.1954, 0.1939, 0.1926, 0.1915]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:36:27.986300Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game117001_game118500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.394641   (epoch 10)   mode=min
  policy_loss: 2.300367   (epoch 10)   mode=min
  policy_policy_acc: 0.318145   (epoch 10)   mode=max
  policy_top10_acc: 0.829351   (epoch 10)   mode=max
  policy_top5_acc: 0.674213   (epoch 10)   mode=max
  val_loss: 2.622957   (epoch 5)   mode=min
  val_policy_loss: 2.519759   (epoch 4)   mode=min
  val_policy_policy_acc: 0.280819   (epoch 4)   mode=max
  val_policy_top10_acc: 0.783417   (epoch 9)   mode=max
  val_policy_top5_acc: 0.621478   (epoch 8)   mode=max
  val_value_loss: 0.203630   (epoch 10)   mode=min
  val_value_value_mse: 0.203638   (epoch 10)   mode=min
  value_loss: 0.188104   (epoch 10)   mode=min
  value_value_mse: 0.188147   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6678, 2.6239, 2.5910, 2.5251, 2.4979, 2.4769, 2.4370, 2.4236, 2.4015, 2.3946]
  policy_loss : [2.5655, 2.5227, 2.4911, 2.4266, 2.4012, 2.3807, 2.3420, 2.3291, 2.3074, 2.3004]
  policy_policy_acc : [0.2676, 0.2750, 0.2812, 0.2943, 0.2991, 0.3026, 0.3095, 0.3124, 0.3167, 0.3181]
  policy_top10_acc : [0.7714, 0.7820, 0.7893, 0.8033, 0.8091, 0.8140, 0.8207, 0.8239, 0.8270, 0.8294]
  policy_top5_acc : [0.6052, 0.6173, 0.6239, 0.6416, 0.6479, 0.6541, 0.6645, 0.6675, 0.6722, 0.6742]
  val_loss : [2.6406, 2.6642, 2.7206, 2.6230, 2.6230, 2.6689, 2.6909, 2.6870, 3.3058, 2.7373]
  val_policy_loss : [2.5346, 2.5594, 2.6149, 2.5198, 2.5204, 2.5653, 2.5887, 2.5836, 3.2031, 2.6353]
  val_policy_policy_acc : [0.2713, 0.2714, 0.2725, 0.2808, 0.2770, 0.2769, 0.2771, 0.2770, 0.2789, 0.2798]
  val_policy_top10_acc : [0.7747, 0.7725, 0.7747, 0.7804, 0.7800, 0.7796, 0.7799, 0.7826, 0.7834, 0.7825]
  val_policy_top5_acc : [0.6109, 0.6081, 0.6073, 0.6184, 0.6166, 0.6171, 0.6168, 0.6215, 0.6162, 0.6174]
  val_value_loss : [0.2118, 0.2093, 0.2109, 0.2063, 0.2050, 0.2070, 0.2040, 0.2066, 0.2044, 0.2036]
  val_value_value_mse : [0.2118, 0.2093, 0.2110, 0.2063, 0.2050, 0.2070, 0.2040, 0.2066, 0.2044, 0.2036]
  value_loss : [0.2048, 0.2026, 0.2001, 0.1965, 0.1935, 0.1922, 0.1899, 0.1889, 0.1888, 0.1881]
  value_value_mse : [0.2048, 0.2026, 0.2001, 0.1964, 0.1935, 0.1923, 0.1899, 0.1889, 0.1888, 0.1881]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:44:08.374131Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game118501_game120000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.415922   (epoch 8)   mode=min
  policy_loss: 2.319077   (epoch 8)   mode=min
  policy_policy_acc: 0.314087   (epoch 8)   mode=max
  policy_top10_acc: 0.825592   (epoch 8)   mode=max
  policy_top5_acc: 0.668455   (epoch 8)   mode=max
  val_loss: 2.628457   (epoch 2)   mode=min
  val_policy_loss: 2.525176   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281019   (epoch 7)   mode=max
  val_policy_top10_acc: 0.778621   (epoch 6)   mode=max
  val_policy_top5_acc: 0.616284   (epoch 7)   mode=max
  val_value_loss: 0.202944   (epoch 6)   mode=min
  val_value_value_mse: 0.202955   (epoch 6)   mode=min
  value_loss: 0.193702   (epoch 8)   mode=min
  value_value_mse: 0.193702   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6780, 2.6329, 2.5929, 2.5620, 2.4979, 2.4703, 2.4309, 2.4159]
  policy_loss : [2.5729, 2.5291, 2.4905, 2.4608, 2.3985, 2.3719, 2.3335, 2.3191]
  policy_policy_acc : [0.2635, 0.2722, 0.2803, 0.2864, 0.2985, 0.3029, 0.3110, 0.3141]
  policy_top10_acc : [0.7692, 0.7798, 0.7896, 0.7952, 0.8090, 0.8151, 0.8208, 0.8256]
  policy_top5_acc : [0.6029, 0.6139, 0.6233, 0.6331, 0.6503, 0.6568, 0.6657, 0.6685]
  val_loss : [2.6351, 2.6285, 2.6383, 2.6498, 2.6447, 2.6666, 2.6363, 2.6553]
  val_policy_loss : [2.5316, 2.5252, 2.5340, 2.5470, 2.5428, 2.5650, 2.5347, 2.5532]
  val_policy_policy_acc : [0.2726, 0.2723, 0.2766, 0.2734, 0.2763, 0.2800, 0.2810, 0.2802]
  val_policy_top10_acc : [0.7762, 0.7777, 0.7767, 0.7713, 0.7778, 0.7786, 0.7766, 0.7763]
  val_policy_top5_acc : [0.6135, 0.6137, 0.6125, 0.6036, 0.6146, 0.6127, 0.6163, 0.6104]
  val_value_loss : [0.2069, 0.2064, 0.2085, 0.2057, 0.2036, 0.2029, 0.2030, 0.2041]
  val_value_value_mse : [0.2069, 0.2065, 0.2085, 0.2057, 0.2036, 0.2030, 0.2030, 0.2041]
  value_loss : [0.2103, 0.2077, 0.2048, 0.2023, 0.1988, 0.1968, 0.1949, 0.1937]
  value_value_mse : [0.2103, 0.2077, 0.2048, 0.2023, 0.1988, 0.1968, 0.1949, 0.1937]

================================================================================

History file: model_versions/chess_elo_model_V9_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T03:54:11.061382Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game120001_game121500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V8
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.376738   (epoch 10)   mode=min
  policy_loss: 2.283012   (epoch 10)   mode=min
  policy_policy_acc: 0.321031   (epoch 10)   mode=max
  policy_top10_acc: 0.831768   (epoch 10)   mode=max
  policy_top5_acc: 0.680723   (epoch 10)   mode=max
  val_loss: 2.623485   (epoch 5)   mode=min
  val_policy_loss: 2.522009   (epoch 5)   mode=min
  val_policy_policy_acc: 0.282717   (epoch 5)   mode=max
  val_policy_top10_acc: 0.781518   (epoch 10)   mode=max
  val_policy_top5_acc: 0.619480   (epoch 10)   mode=max
  val_value_loss: 0.201380   (epoch 9)   mode=min
  val_value_value_mse: 0.201394   (epoch 9)   mode=min
  value_loss: 0.190276   (epoch 10)   mode=min
  value_value_mse: 0.190331   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6689, 2.6243, 2.5900, 2.5619, 2.4977, 2.4715, 2.4503, 2.4107, 2.3975, 2.3767]
  policy_loss : [2.5678, 2.5221, 2.4911, 2.4657, 2.3999, 2.3748, 2.3560, 2.3178, 2.3029, 2.2830]
  policy_policy_acc : [0.2641, 0.2730, 0.2792, 0.2849, 0.2978, 0.3022, 0.3057, 0.3148, 0.3175, 0.3210]
  policy_top10_acc : [0.7698, 0.7819, 0.7900, 0.7951, 0.8093, 0.8140, 0.8186, 0.8268, 0.8283, 0.8318]
  policy_top5_acc : [0.6044, 0.6183, 0.6283, 0.6339, 0.6504, 0.6578, 0.6611, 0.6726, 0.6765, 0.6807]
  val_loss : [2.7592, 2.6521, 2.7962, 2.9168, 2.6235, 2.7271, 2.6854, 2.6612, 2.6548, 2.7261]
  val_policy_loss : [2.6554, 2.5497, 2.6941, 2.8105, 2.5220, 2.6255, 2.5835, 2.5603, 2.5540, 2.6253]
  val_policy_policy_acc : [0.2716, 0.2745, 0.2783, 0.2773, 0.2827, 0.2809, 0.2819, 0.2817, 0.2826, 0.2791]
  val_policy_top10_acc : [0.7761, 0.7749, 0.7775, 0.7755, 0.7777, 0.7805, 0.7771, 0.7809, 0.7792, 0.7815]
  val_policy_top5_acc : [0.6125, 0.6105, 0.6192, 0.6114, 0.6187, 0.6190, 0.6165, 0.6186, 0.6182, 0.6195]
  val_value_loss : [0.2073, 0.2046, 0.2037, 0.2122, 0.2029, 0.2031, 0.2035, 0.2016, 0.2014, 0.2014]
  val_value_value_mse : [0.2074, 0.2047, 0.2037, 0.2122, 0.2029, 0.2031, 0.2035, 0.2016, 0.2014, 0.2014]
  value_loss : [0.2057, 0.2036, 0.2007, 0.1992, 0.1962, 0.1944, 0.1931, 0.1915, 0.1907, 0.1903]
  value_value_mse : [0.2057, 0.2036, 0.2007, 0.1992, 0.1962, 0.1944, 0.1931, 0.1915, 0.1906, 0.1903]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:03:49.904901Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game121501_game123000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.384041   (epoch 10)   mode=min
  policy_loss: 2.287722   (epoch 10)   mode=min
  policy_policy_acc: 0.320418   (epoch 10)   mode=max
  policy_top10_acc: 0.828686   (epoch 10)   mode=max
  policy_top5_acc: 0.678536   (epoch 10)   mode=max
  val_loss: 2.639526   (epoch 7)   mode=min
  val_policy_loss: 2.536888   (epoch 7)   mode=min
  val_policy_policy_acc: 0.279520   (epoch 9)   mode=max
  val_policy_top10_acc: 0.782318   (epoch 8)   mode=max
  val_policy_top5_acc: 0.620180   (epoch 9)   mode=max
  val_value_loss: 0.202785   (epoch 9)   mode=min
  val_value_value_mse: 0.202786   (epoch 9)   mode=min
  value_loss: 0.192645   (epoch 10)   mode=min
  value_value_mse: 0.192647   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.6809, 2.6339, 2.5934, 2.5660, 2.4994, 2.4737, 2.4341, 2.4194, 2.4043, 2.3840]
  policy_loss : [2.5769, 2.5305, 2.4914, 2.4648, 2.3996, 2.3753, 2.3364, 2.3226, 2.3078, 2.2877]
  policy_policy_acc : [0.2633, 0.2737, 0.2802, 0.2846, 0.2993, 0.3034, 0.3127, 0.3134, 0.3172, 0.3204]
  policy_top10_acc : [0.7670, 0.7775, 0.7870, 0.7944, 0.8074, 0.8120, 0.8210, 0.8233, 0.8244, 0.8287]
  policy_top5_acc : [0.6023, 0.6153, 0.6255, 0.6336, 0.6492, 0.6543, 0.6660, 0.6704, 0.6722, 0.6785]
  val_loss : [2.6571, 2.6435, 2.8679, 2.6967, 2.6675, 3.0560, 2.6395, 2.8629, 2.9720, 3.0039]
  val_policy_loss : [2.5521, 2.5413, 2.7649, 2.5898, 2.5625, 2.9512, 2.5369, 2.7608, 2.8704, 2.9021]
  val_policy_policy_acc : [0.2759, 0.2758, 0.2759, 0.2737, 0.2778, 0.2749, 0.2774, 0.2772, 0.2795, 0.2777]
  val_policy_top10_acc : [0.7769, 0.7766, 0.7821, 0.7787, 0.7793, 0.7744, 0.7790, 0.7823, 0.7799, 0.7808]
  val_policy_top5_acc : [0.6122, 0.6157, 0.6190, 0.6096, 0.6134, 0.6122, 0.6179, 0.6178, 0.6202, 0.6202]
  val_value_loss : [0.2098, 0.2042, 0.2056, 0.2138, 0.2100, 0.2091, 0.2052, 0.2040, 0.2028, 0.2030]
  val_value_value_mse : [0.2098, 0.2042, 0.2056, 0.2138, 0.2100, 0.2091, 0.2052, 0.2040, 0.2028, 0.2030]
  value_loss : [0.2080, 0.2067, 0.2041, 0.2025, 0.1995, 0.1966, 0.1955, 0.1935, 0.1930, 0.1926]
  value_value_mse : [0.2080, 0.2067, 0.2041, 0.2025, 0.1995, 0.1966, 0.1955, 0.1935, 0.1930, 0.1926]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:13:37.123398Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game123001_game124500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.370032   (epoch 10)   mode=min
  policy_loss: 2.276776   (epoch 10)   mode=min
  policy_policy_acc: 0.323704   (epoch 10)   mode=max
  policy_top10_acc: 0.833054   (epoch 10)   mode=max
  policy_top5_acc: 0.682220   (epoch 10)   mode=max
  val_loss: 2.622637   (epoch 5)   mode=min
  val_policy_loss: 2.519482   (epoch 5)   mode=min
  val_policy_policy_acc: 0.283417   (epoch 4)   mode=max
  val_policy_top10_acc: 0.780619   (epoch 4)   mode=max
  val_policy_top5_acc: 0.623776   (epoch 10)   mode=max
  val_value_loss: 0.202497   (epoch 10)   mode=min
  val_value_value_mse: 0.202506   (epoch 10)   mode=min
  value_loss: 0.186609   (epoch 10)   mode=min
  value_value_mse: 0.186604   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6636, 2.6152, 2.5767, 2.5068, 2.4840, 2.4596, 2.4427, 2.4039, 2.3894, 2.3700]
  policy_loss : [2.5624, 2.5158, 2.4775, 2.4095, 2.3876, 2.3634, 2.3475, 2.3102, 2.2958, 2.2768]
  policy_policy_acc : [0.2669, 0.2761, 0.2854, 0.2967, 0.3007, 0.3079, 0.3092, 0.3162, 0.3214, 0.3237]
  policy_top10_acc : [0.7730, 0.7827, 0.7921, 0.8058, 0.8117, 0.8165, 0.8192, 0.8262, 0.8287, 0.8331]
  policy_top5_acc : [0.6064, 0.6190, 0.6314, 0.6475, 0.6527, 0.6597, 0.6628, 0.6742, 0.6759, 0.6822]
  val_loss : [2.6338, 2.6388, 2.7700, 2.6797, 2.6226, 2.6614, 2.7862, 2.7207, 2.6650, 2.8216]
  val_policy_loss : [2.5319, 2.5333, 2.6649, 2.5765, 2.5195, 2.5590, 2.6838, 2.6182, 2.5628, 2.7201]
  val_policy_policy_acc : [0.2712, 0.2754, 0.2784, 0.2834, 0.2828, 0.2815, 0.2805, 0.2833, 0.2820, 0.2834]
  val_policy_top10_acc : [0.7743, 0.7742, 0.7711, 0.7806, 0.7784, 0.7783, 0.7742, 0.7788, 0.7795, 0.7788]
  val_policy_top5_acc : [0.6120, 0.6165, 0.6109, 0.6218, 0.6184, 0.6209, 0.6203, 0.6204, 0.6187, 0.6238]
  val_value_loss : [0.2038, 0.2107, 0.2098, 0.2062, 0.2062, 0.2043, 0.2043, 0.2045, 0.2042, 0.2025]
  val_value_value_mse : [0.2038, 0.2107, 0.2098, 0.2062, 0.2062, 0.2043, 0.2043, 0.2045, 0.2042, 0.2025]
  value_loss : [0.2024, 0.1988, 0.1985, 0.1947, 0.1928, 0.1926, 0.1904, 0.1876, 0.1874, 0.1866]
  value_value_mse : [0.2025, 0.1988, 0.1985, 0.1947, 0.1928, 0.1926, 0.1904, 0.1876, 0.1874, 0.1866]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:21:35.863639Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game124501_game126000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.412905   (epoch 8)   mode=min
  policy_loss: 2.313756   (epoch 8)   mode=min
  policy_policy_acc: 0.317003   (epoch 8)   mode=max
  policy_top10_acc: 0.826093   (epoch 8)   mode=max
  policy_top5_acc: 0.672537   (epoch 8)   mode=max
  val_loss: 2.628696   (epoch 2)   mode=min
  val_policy_loss: 2.524034   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282118   (epoch 7)   mode=max
  val_policy_top10_acc: 0.779720   (epoch 2)   mode=max
  val_policy_top5_acc: 0.618482   (epoch 7)   mode=max
  val_value_loss: 0.202981   (epoch 6)   mode=min
  val_value_value_mse: 0.202977   (epoch 6)   mode=min
  value_loss: 0.198317   (epoch 8)   mode=min
  value_value_mse: 0.198318   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6707, 2.6220, 2.5839, 2.5555, 2.4912, 2.4651, 2.4259, 2.4129]
  policy_loss : [2.5652, 2.5175, 2.4801, 2.4523, 2.3899, 2.3645, 2.3264, 2.3138]
  policy_policy_acc : [0.2658, 0.2753, 0.2833, 0.2877, 0.3009, 0.3044, 0.3137, 0.3170]
  policy_top10_acc : [0.7719, 0.7840, 0.7920, 0.7996, 0.8124, 0.8174, 0.8233, 0.8261]
  policy_top5_acc : [0.6072, 0.6202, 0.6288, 0.6377, 0.6546, 0.6622, 0.6682, 0.6725]
  val_loss : [2.6419, 2.6287, 2.6568, 2.6602, 2.6545, 2.8299, 2.7331, 2.7330]
  val_policy_loss : [2.5394, 2.5240, 2.5525, 2.5569, 2.5521, 2.7282, 2.6306, 2.6314]
  val_policy_policy_acc : [0.2701, 0.2777, 0.2714, 0.2705, 0.2758, 0.2782, 0.2821, 0.2809]
  val_policy_top10_acc : [0.7714, 0.7797, 0.7700, 0.7706, 0.7760, 0.7781, 0.7780, 0.7786]
  val_policy_top5_acc : [0.6065, 0.6105, 0.6039, 0.6068, 0.6140, 0.6144, 0.6185, 0.6158]
  val_value_loss : [0.2049, 0.2092, 0.2085, 0.2065, 0.2047, 0.2030, 0.2046, 0.2030]
  val_value_value_mse : [0.2049, 0.2092, 0.2085, 0.2065, 0.2047, 0.2030, 0.2046, 0.2030]
  value_loss : [0.2110, 0.2090, 0.2076, 0.2063, 0.2027, 0.2013, 0.1990, 0.1983]
  value_value_mse : [0.2110, 0.2090, 0.2076, 0.2063, 0.2027, 0.2013, 0.1990, 0.1983]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:31:36.418849Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game126001_game127500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.411778   (epoch 10)   mode=min
  policy_loss: 2.316102   (epoch 10)   mode=min
  policy_policy_acc: 0.316972   (epoch 10)   mode=max
  policy_top10_acc: 0.823645   (epoch 10)   mode=max
  policy_top5_acc: 0.668561   (epoch 10)   mode=max
  val_loss: 2.617234   (epoch 4)   mode=min
  val_policy_loss: 2.512411   (epoch 4)   mode=min
  val_policy_policy_acc: 0.284515   (epoch 10)   mode=max
  val_policy_top10_acc: 0.783816   (epoch 8)   mode=max
  val_policy_top5_acc: 0.624376   (epoch 4)   mode=max
  val_value_loss: 0.206032   (epoch 9)   mode=min
  val_value_value_mse: 0.206024   (epoch 9)   mode=min
  value_loss: 0.191441   (epoch 9)   mode=min
  value_value_mse: 0.191434   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6797, 2.6376, 2.6035, 2.5374, 2.5115, 2.4910, 2.4538, 2.4393, 2.4198, 2.4118]
  policy_loss : [2.5762, 2.5351, 2.5022, 2.4381, 2.4134, 2.3936, 2.3576, 2.3433, 2.3241, 2.3161]
  policy_policy_acc : [0.2640, 0.2716, 0.2800, 0.2930, 0.2964, 0.3032, 0.3083, 0.3114, 0.3147, 0.3170]
  policy_top10_acc : [0.7668, 0.7762, 0.7829, 0.7992, 0.8045, 0.8083, 0.8159, 0.8188, 0.8231, 0.8236]
  policy_top5_acc : [0.6006, 0.6126, 0.6228, 0.6394, 0.6458, 0.6518, 0.6597, 0.6633, 0.6675, 0.6686]
  val_loss : [2.6241, 2.6744, 2.6505, 2.6172, 2.6286, 2.6543, 2.6505, 2.6643, 2.6508, 2.6684]
  val_policy_loss : [2.5188, 2.5689, 2.5457, 2.5124, 2.5253, 2.5505, 2.5471, 2.5606, 2.5477, 2.5653]
  val_policy_policy_acc : [0.2752, 0.2725, 0.2768, 0.2794, 0.2815, 0.2833, 0.2838, 0.2818, 0.2832, 0.2845]
  val_policy_top10_acc : [0.7782, 0.7717, 0.7759, 0.7823, 0.7783, 0.7809, 0.7811, 0.7838, 0.7821, 0.7834]
  val_policy_top5_acc : [0.6130, 0.6089, 0.6165, 0.6244, 0.6178, 0.6200, 0.6188, 0.6215, 0.6240, 0.6237]
  val_value_loss : [0.2105, 0.2107, 0.2094, 0.2095, 0.2066, 0.2075, 0.2068, 0.2072, 0.2060, 0.2062]
  val_value_value_mse : [0.2105, 0.2108, 0.2094, 0.2095, 0.2066, 0.2075, 0.2068, 0.2072, 0.2060, 0.2062]
  value_loss : [0.2068, 0.2049, 0.2026, 0.1987, 0.1962, 0.1948, 0.1925, 0.1918, 0.1914, 0.1915]
  value_value_mse : [0.2068, 0.2049, 0.2026, 0.1987, 0.1962, 0.1948, 0.1925, 0.1918, 0.1914, 0.1915]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:39:31.730991Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game127501_game129000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.424567   (epoch 8)   mode=min
  policy_loss: 2.326362   (epoch 8)   mode=min
  policy_policy_acc: 0.314476   (epoch 8)   mode=max
  policy_top10_acc: 0.822060   (epoch 8)   mode=max
  policy_top5_acc: 0.667147   (epoch 8)   mode=max
  val_loss: 2.623612   (epoch 2)   mode=min
  val_policy_loss: 2.520821   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 7)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 5)   mode=max
  val_policy_top5_acc: 0.618082   (epoch 5)   mode=max
  val_value_loss: 0.201366   (epoch 8)   mode=min
  val_value_value_mse: 0.201376   (epoch 8)   mode=min
  value_loss: 0.196425   (epoch 8)   mode=min
  value_value_mse: 0.196426   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6736, 2.6308, 2.5979, 2.5652, 2.4981, 2.4743, 2.4363, 2.4246]
  policy_loss : [2.5682, 2.5265, 2.4948, 2.4626, 2.3975, 2.3744, 2.3377, 2.3264]
  policy_policy_acc : [0.2642, 0.2732, 0.2815, 0.2855, 0.2986, 0.3044, 0.3109, 0.3145]
  policy_top10_acc : [0.7693, 0.7788, 0.7865, 0.7928, 0.8067, 0.8117, 0.8192, 0.8221]
  policy_top5_acc : [0.6054, 0.6159, 0.6243, 0.6324, 0.6506, 0.6563, 0.6644, 0.6671]
  val_loss : [2.6404, 2.6236, 2.6338, 2.6310, 2.6241, 2.6277, 2.7789, 2.9184]
  val_policy_loss : [2.5393, 2.5208, 2.5297, 2.5283, 2.5217, 2.5260, 2.6777, 2.8175]
  val_policy_policy_acc : [0.2721, 0.2764, 0.2735, 0.2803, 0.2821, 0.2792, 0.2851, 0.2820]
  val_policy_top10_acc : [0.7721, 0.7786, 0.7792, 0.7808, 0.7857, 0.7813, 0.7821, 0.7835]
  val_policy_top5_acc : [0.6009, 0.6112, 0.6108, 0.6124, 0.6181, 0.6137, 0.6152, 0.6164]
  val_value_loss : [0.2020, 0.2055, 0.2080, 0.2053, 0.2047, 0.2034, 0.2021, 0.2014]
  val_value_value_mse : [0.2020, 0.2055, 0.2080, 0.2053, 0.2048, 0.2034, 0.2021, 0.2014]
  value_loss : [0.2108, 0.2086, 0.2063, 0.2052, 0.2011, 0.1998, 0.1972, 0.1964]
  value_value_mse : [0.2108, 0.2086, 0.2063, 0.2052, 0.2011, 0.1998, 0.1972, 0.1964]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:48:15.385451Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game129001_game130500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.382480   (epoch 9)   mode=min
  policy_loss: 2.289252   (epoch 9)   mode=min
  policy_policy_acc: 0.321219   (epoch 9)   mode=max
  policy_top10_acc: 0.830688   (epoch 9)   mode=max
  policy_top5_acc: 0.677612   (epoch 9)   mode=max
  val_loss: 2.628263   (epoch 3)   mode=min
  val_policy_loss: 2.526108   (epoch 3)   mode=min
  val_policy_policy_acc: 0.282318   (epoch 8)   mode=max
  val_policy_top10_acc: 0.780819   (epoch 8)   mode=max
  val_policy_top5_acc: 0.615485   (epoch 9)   mode=max
  val_value_loss: 0.202500   (epoch 7)   mode=min
  val_value_value_mse: 0.202529   (epoch 7)   mode=min
  value_loss: 0.186387   (epoch 9)   mode=min
  value_value_mse: 0.186397   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6597, 2.6130, 2.5783, 2.5500, 2.5238, 2.4569, 2.4346, 2.3939, 2.3825]
  policy_loss : [2.5587, 2.5130, 2.4791, 2.4523, 2.4265, 2.3612, 2.3399, 2.3003, 2.2893]
  policy_policy_acc : [0.2680, 0.2776, 0.2844, 0.2897, 0.2939, 0.3075, 0.3120, 0.3198, 0.3212]
  policy_top10_acc : [0.7717, 0.7839, 0.7914, 0.7966, 0.8037, 0.8168, 0.8207, 0.8270, 0.8307]
  policy_top5_acc : [0.6083, 0.6212, 0.6286, 0.6351, 0.6431, 0.6595, 0.6655, 0.6750, 0.6776]
  val_loss : [2.6864, 2.6451, 2.6283, 2.7073, 2.6764, 2.7012, 2.6645, 2.7686, 2.6982]
  val_policy_loss : [2.5841, 2.5431, 2.5261, 2.6051, 2.5741, 2.5993, 2.5632, 2.6664, 2.5968]
  val_policy_policy_acc : [0.2721, 0.2698, 0.2747, 0.2697, 0.2734, 0.2780, 0.2771, 0.2823, 0.2797]
  val_policy_top10_acc : [0.7774, 0.7746, 0.7794, 0.7751, 0.7748, 0.7751, 0.7783, 0.7808, 0.7793]
  val_policy_top5_acc : [0.6146, 0.6098, 0.6083, 0.6106, 0.6092, 0.6115, 0.6140, 0.6154, 0.6155]
  val_value_loss : [0.2044, 0.2038, 0.2044, 0.2043, 0.2045, 0.2034, 0.2025, 0.2041, 0.2025]
  val_value_value_mse : [0.2044, 0.2038, 0.2044, 0.2044, 0.2046, 0.2035, 0.2025, 0.2041, 0.2025]
  value_loss : [0.2020, 0.1999, 0.1985, 0.1954, 0.1946, 0.1914, 0.1892, 0.1870, 0.1864]
  value_value_mse : [0.2020, 0.1999, 0.1985, 0.1954, 0.1947, 0.1914, 0.1892, 0.1871, 0.1864]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T04:55:14.420941Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game130501_game132000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.439164   (epoch 7)   mode=min
  policy_loss: 2.344981   (epoch 7)   mode=min
  policy_policy_acc: 0.309773   (epoch 7)   mode=max
  policy_top10_acc: 0.817252   (epoch 7)   mode=max
  policy_top5_acc: 0.664138   (epoch 7)   mode=max
  val_loss: 2.609143   (epoch 1)   mode=min
  val_policy_loss: 2.507662   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286114   (epoch 7)   mode=max
  val_policy_top10_acc: 0.782817   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620380   (epoch 5)   mode=max
  val_value_loss: 0.201966   (epoch 6)   mode=min
  val_value_value_mse: 0.201982   (epoch 6)   mode=min
  value_loss: 0.188359   (epoch 7)   mode=min
  value_value_mse: 0.188359   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6648, 2.6147, 2.5824, 2.5186, 2.4921, 2.4540, 2.4392]
  policy_loss : [2.5635, 2.5145, 2.4835, 2.4219, 2.3960, 2.3591, 2.3450]
  policy_policy_acc : [0.2666, 0.2774, 0.2831, 0.2949, 0.2999, 0.3065, 0.3098]
  policy_top10_acc : [0.7703, 0.7822, 0.7884, 0.8017, 0.8074, 0.8149, 0.8173]
  policy_top5_acc : [0.6058, 0.6197, 0.6264, 0.6434, 0.6511, 0.6607, 0.6641]
  val_loss : [2.6091, 2.6279, 2.7224, 2.6348, 2.6175, 2.6417, 2.6847]
  val_policy_loss : [2.5077, 2.5250, 2.6178, 2.5329, 2.5154, 2.5406, 2.5833]
  val_policy_policy_acc : [0.2764, 0.2714, 0.2739, 0.2781, 0.2801, 0.2846, 0.2861]
  val_policy_top10_acc : [0.7812, 0.7740, 0.7711, 0.7815, 0.7814, 0.7799, 0.7828]
  val_policy_top5_acc : [0.6168, 0.6112, 0.6075, 0.6154, 0.6204, 0.6181, 0.6177]
  val_value_loss : [0.2028, 0.2055, 0.2089, 0.2037, 0.2039, 0.2020, 0.2025]
  val_value_value_mse : [0.2028, 0.2055, 0.2089, 0.2037, 0.2039, 0.2020, 0.2025]
  value_loss : [0.2025, 0.2003, 0.1979, 0.1936, 0.1923, 0.1899, 0.1884]
  value_value_mse : [0.2025, 0.2003, 0.1979, 0.1936, 0.1923, 0.1899, 0.1884]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:05:01.995941Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game132001_game133500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.409998   (epoch 10)   mode=min
  policy_loss: 2.313519   (epoch 10)   mode=min
  policy_policy_acc: 0.318113   (epoch 10)   mode=max
  policy_top10_acc: 0.824451   (epoch 9)   mode=max
  policy_top5_acc: 0.671801   (epoch 10)   mode=max
  val_loss: 2.610716   (epoch 4)   mode=min
  val_policy_loss: 2.509450   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279421   (epoch 9)   mode=max
  val_policy_top10_acc: 0.785814   (epoch 9)   mode=max
  val_policy_top5_acc: 0.621478   (epoch 9)   mode=max
  val_value_loss: 0.201505   (epoch 10)   mode=min
  val_value_value_mse: 0.201525   (epoch 10)   mode=min
  value_loss: 0.192903   (epoch 10)   mode=min
  value_value_mse: 0.192900   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6713, 2.6293, 2.5994, 2.5304, 2.5051, 2.4884, 2.4502, 2.4380, 2.4141, 2.4100]
  policy_loss : [2.5668, 2.5262, 2.4968, 2.4300, 2.4062, 2.3898, 2.3525, 2.3411, 2.3173, 2.3135]
  policy_policy_acc : [0.2662, 0.2758, 0.2813, 0.2944, 0.2980, 0.3010, 0.3105, 0.3128, 0.3167, 0.3181]
  policy_top10_acc : [0.7721, 0.7804, 0.7885, 0.8017, 0.8063, 0.8100, 0.8172, 0.8210, 0.8245, 0.8244]
  policy_top5_acc : [0.6055, 0.6182, 0.6260, 0.6417, 0.6476, 0.6536, 0.6618, 0.6643, 0.6695, 0.6718]
  val_loss : [2.6395, 2.7239, 2.6430, 2.6107, 2.6260, 2.6269, 2.6579, 2.6249, 2.6897, 2.6338]
  val_policy_loss : [2.5384, 2.6219, 2.5393, 2.5094, 2.5249, 2.5240, 2.5568, 2.5238, 2.5886, 2.5329]
  val_policy_policy_acc : [0.2729, 0.2729, 0.2692, 0.2776, 0.2747, 0.2766, 0.2767, 0.2762, 0.2794, 0.2771]
  val_policy_top10_acc : [0.7745, 0.7802, 0.7783, 0.7806, 0.7797, 0.7828, 0.7854, 0.7842, 0.7858, 0.7828]
  val_policy_top5_acc : [0.6091, 0.6129, 0.6071, 0.6191, 0.6173, 0.6190, 0.6189, 0.6170, 0.6215, 0.6160]
  val_value_loss : [0.2020, 0.2038, 0.2073, 0.2023, 0.2018, 0.2054, 0.2020, 0.2019, 0.2019, 0.2015]
  val_value_value_mse : [0.2020, 0.2038, 0.2073, 0.2023, 0.2018, 0.2055, 0.2021, 0.2019, 0.2019, 0.2015]
  value_loss : [0.2090, 0.2062, 0.2051, 0.2007, 0.1978, 0.1970, 0.1954, 0.1939, 0.1936, 0.1929]
  value_value_mse : [0.2090, 0.2062, 0.2051, 0.2007, 0.1978, 0.1970, 0.1954, 0.1939, 0.1936, 0.1929]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:13:38.566671Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game133501_game135000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.396799   (epoch 9)   mode=min
  policy_loss: 2.300030   (epoch 9)   mode=min
  policy_policy_acc: 0.318440   (epoch 9)   mode=max
  policy_top10_acc: 0.826487   (epoch 9)   mode=max
  policy_top5_acc: 0.672364   (epoch 9)   mode=max
  val_loss: 2.628063   (epoch 3)   mode=min
  val_policy_loss: 2.525482   (epoch 3)   mode=min
  val_policy_policy_acc: 0.280120   (epoch 9)   mode=max
  val_policy_top10_acc: 0.780919   (epoch 1)   mode=max
  val_policy_top5_acc: 0.620879   (epoch 2)   mode=max
  val_value_loss: 0.200985   (epoch 1)   mode=min
  val_value_value_mse: 0.201023   (epoch 1)   mode=min
  value_loss: 0.193566   (epoch 9)   mode=min
  value_value_mse: 0.193578   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6730, 2.6301, 2.5982, 2.5670, 2.5378, 2.4770, 2.4494, 2.4107, 2.3968]
  policy_loss : [2.5678, 2.5265, 2.4958, 2.4646, 2.4369, 2.3774, 2.3512, 2.3131, 2.3000]
  policy_policy_acc : [0.2659, 0.2738, 0.2810, 0.2855, 0.2910, 0.3037, 0.3066, 0.3149, 0.3184]
  policy_top10_acc : [0.7675, 0.7793, 0.7860, 0.7918, 0.7993, 0.8116, 0.8168, 0.8235, 0.8265]
  policy_top5_acc : [0.6047, 0.6156, 0.6240, 0.6320, 0.6387, 0.6540, 0.6611, 0.6704, 0.6724]
  val_loss : [2.8365, 2.7118, 2.6281, 2.6320, 2.6649, 2.9104, 2.6890, 2.7545, 2.8039]
  val_policy_loss : [2.7358, 2.6092, 2.5255, 2.5304, 2.5619, 2.8085, 2.5869, 2.6527, 2.7024]
  val_policy_policy_acc : [0.2737, 0.2769, 0.2734, 0.2745, 0.2692, 0.2778, 0.2757, 0.2791, 0.2801]
  val_policy_top10_acc : [0.7809, 0.7796, 0.7756, 0.7770, 0.7780, 0.7752, 0.7761, 0.7777, 0.7757]
  val_policy_top5_acc : [0.6130, 0.6209, 0.6110, 0.6115, 0.6116, 0.6147, 0.6117, 0.6160, 0.6161]
  val_value_loss : [0.2010, 0.2050, 0.2051, 0.2030, 0.2059, 0.2034, 0.2040, 0.2032, 0.2027]
  val_value_value_mse : [0.2010, 0.2051, 0.2051, 0.2030, 0.2059, 0.2034, 0.2040, 0.2033, 0.2028]
  value_loss : [0.2105, 0.2073, 0.2057, 0.2046, 0.2023, 0.1988, 0.1963, 0.1948, 0.1936]
  value_value_mse : [0.2105, 0.2073, 0.2057, 0.2046, 0.2023, 0.1988, 0.1964, 0.1948, 0.1936]

================================================================================

History file: model_versions/chess_elo_model_V10_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:22:21.483298Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game135001_game136500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V9
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399216   (epoch 9)   mode=min
  policy_loss: 2.309595   (epoch 9)   mode=min
  policy_policy_acc: 0.318574   (epoch 9)   mode=max
  policy_top10_acc: 0.825949   (epoch 9)   mode=max
  policy_top5_acc: 0.673167   (epoch 9)   mode=max
  val_loss: 2.624316   (epoch 3)   mode=min
  val_policy_loss: 2.521140   (epoch 3)   mode=min
  val_policy_policy_acc: 0.282717   (epoch 8)   mode=max
  val_policy_top10_acc: 0.779520   (epoch 6)   mode=max
  val_policy_top5_acc: 0.620380   (epoch 9)   mode=max
  val_value_loss: 0.203894   (epoch 8)   mode=min
  val_value_value_mse: 0.203898   (epoch 8)   mode=min
  value_loss: 0.179121   (epoch 9)   mode=min
  value_value_mse: 0.179129   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6781, 2.6289, 2.5935, 2.5642, 2.5359, 2.4759, 2.4540, 2.4138, 2.3992]
  policy_loss : [2.5784, 2.5309, 2.4971, 2.4691, 2.4413, 2.3839, 2.3632, 2.3239, 2.3096]
  policy_policy_acc : [0.2648, 0.2736, 0.2821, 0.2861, 0.2904, 0.3031, 0.3073, 0.3146, 0.3186]
  policy_top10_acc : [0.7707, 0.7801, 0.7881, 0.7947, 0.8017, 0.8112, 0.8171, 0.8235, 0.8259]
  policy_top5_acc : [0.6034, 0.6165, 0.6247, 0.6325, 0.6396, 0.6560, 0.6611, 0.6703, 0.6732]
  val_loss : [2.6276, 2.6261, 2.6243, 2.6344, 2.6619, 2.6701, 2.6664, 2.6557, 2.7094]
  val_policy_loss : [2.5255, 2.5237, 2.5211, 2.5321, 2.5588, 2.5665, 2.5635, 2.5537, 2.6061]
  val_policy_policy_acc : [0.2744, 0.2757, 0.2756, 0.2742, 0.2736, 0.2802, 0.2785, 0.2827, 0.2820]
  val_policy_top10_acc : [0.7770, 0.7765, 0.7777, 0.7743, 0.7737, 0.7795, 0.7784, 0.7777, 0.7774]
  val_policy_top5_acc : [0.6153, 0.6132, 0.6164, 0.6129, 0.6103, 0.6151, 0.6198, 0.6194, 0.6204]
  val_value_loss : [0.2042, 0.2047, 0.2062, 0.2045, 0.2061, 0.2070, 0.2056, 0.2039, 0.2062]
  val_value_value_mse : [0.2042, 0.2047, 0.2062, 0.2045, 0.2061, 0.2070, 0.2056, 0.2039, 0.2062]
  value_loss : [0.1996, 0.1959, 0.1930, 0.1905, 0.1891, 0.1844, 0.1817, 0.1798, 0.1791]
  value_value_mse : [0.1996, 0.1958, 0.1930, 0.1905, 0.1891, 0.1844, 0.1817, 0.1798, 0.1791]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:29:09.038494Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game136501_game138000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.454709   (epoch 7)   mode=min
  policy_loss: 2.354434   (epoch 7)   mode=min
  policy_policy_acc: 0.308984   (epoch 7)   mode=max
  policy_top10_acc: 0.817578   (epoch 7)   mode=max
  policy_top5_acc: 0.660664   (epoch 7)   mode=max
  val_loss: 2.619959   (epoch 1)   mode=min
  val_policy_loss: 2.517625   (epoch 1)   mode=min
  val_policy_policy_acc: 0.279021   (epoch 4)   mode=max
  val_policy_top10_acc: 0.787812   (epoch 6)   mode=max
  val_policy_top5_acc: 0.625175   (epoch 6)   mode=max
  val_value_loss: 0.202434   (epoch 4)   mode=min
  val_value_value_mse: 0.202445   (epoch 4)   mode=min
  value_loss: 0.200604   (epoch 7)   mode=min
  value_value_mse: 0.200601   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6711, 2.6292, 2.5932, 2.5307, 2.5054, 2.4681, 2.4547]
  policy_loss : [2.5648, 2.5240, 2.4885, 2.4279, 2.4035, 2.3674, 2.3544]
  policy_policy_acc : [0.2656, 0.2745, 0.2818, 0.2950, 0.2997, 0.3081, 0.3090]
  policy_top10_acc : [0.7718, 0.7832, 0.7883, 0.8015, 0.8072, 0.8144, 0.8176]
  policy_top5_acc : [0.6066, 0.6168, 0.6270, 0.6415, 0.6500, 0.6578, 0.6607]
  val_loss : [2.6200, 2.6281, 2.7457, 3.0135, 2.7962, 2.7351, 2.7116]
  val_policy_loss : [2.5176, 2.5239, 2.6418, 2.9120, 2.6947, 2.6333, 2.6097]
  val_policy_policy_acc : [0.2764, 0.2711, 0.2765, 0.2790, 0.2759, 0.2728, 0.2787]
  val_policy_top10_acc : [0.7799, 0.7804, 0.7803, 0.7798, 0.7813, 0.7878, 0.7865]
  val_policy_top5_acc : [0.6183, 0.6198, 0.6200, 0.6220, 0.6198, 0.6252, 0.6250]
  val_value_loss : [0.2046, 0.2082, 0.2075, 0.2024, 0.2025, 0.2033, 0.2035]
  val_value_value_mse : [0.2046, 0.2082, 0.2075, 0.2024, 0.2025, 0.2033, 0.2035]
  value_loss : [0.2125, 0.2103, 0.2093, 0.2055, 0.2039, 0.2014, 0.2006]
  value_value_mse : [0.2125, 0.2103, 0.2093, 0.2055, 0.2039, 0.2014, 0.2006]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:36:59.000467Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game138001_game139500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.421989   (epoch 8)   mode=min
  policy_loss: 2.325806   (epoch 8)   mode=min
  policy_policy_acc: 0.316229   (epoch 8)   mode=max
  policy_top10_acc: 0.821588   (epoch 8)   mode=max
  policy_top5_acc: 0.668234   (epoch 8)   mode=max
  val_loss: 2.610985   (epoch 2)   mode=min
  val_policy_loss: 2.510203   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283217   (epoch 7)   mode=max
  val_policy_top10_acc: 0.781019   (epoch 5)   mode=max
  val_policy_top5_acc: 0.619281   (epoch 7)   mode=max
  val_value_loss: 0.201424   (epoch 2)   mode=min
  val_value_value_mse: 0.201441   (epoch 2)   mode=min
  value_loss: 0.192406   (epoch 8)   mode=min
  value_value_mse: 0.192407   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6610, 2.6193, 2.5859, 2.5558, 2.4970, 2.4722, 2.4380, 2.4220]
  policy_loss : [2.5571, 2.5166, 2.4845, 2.4553, 2.3987, 2.3742, 2.3414, 2.3258]
  policy_policy_acc : [0.2686, 0.2770, 0.2851, 0.2883, 0.3008, 0.3061, 0.3121, 0.3162]
  policy_top10_acc : [0.7709, 0.7800, 0.7892, 0.7947, 0.8058, 0.8122, 0.8195, 0.8216]
  policy_top5_acc : [0.6087, 0.6186, 0.6278, 0.6354, 0.6497, 0.6549, 0.6641, 0.6682]
  val_loss : [2.6222, 2.6110, 2.6233, 2.6468, 2.6319, 2.7684, 2.6712, 2.6752]
  val_policy_loss : [2.5194, 2.5102, 2.5214, 2.5452, 2.5307, 2.6659, 2.5698, 2.5736]
  val_policy_policy_acc : [0.2735, 0.2783, 0.2791, 0.2728, 0.2779, 0.2814, 0.2832, 0.2816]
  val_policy_top10_acc : [0.7741, 0.7778, 0.7746, 0.7728, 0.7810, 0.7788, 0.7798, 0.7782]
  val_policy_top5_acc : [0.6140, 0.6189, 0.6122, 0.6092, 0.6156, 0.6179, 0.6193, 0.6185]
  val_value_loss : [0.2055, 0.2014, 0.2035, 0.2031, 0.2022, 0.2046, 0.2025, 0.2028]
  val_value_value_mse : [0.2055, 0.2014, 0.2036, 0.2031, 0.2022, 0.2046, 0.2025, 0.2028]
  value_loss : [0.2079, 0.2054, 0.2028, 0.2010, 0.1967, 0.1961, 0.1934, 0.1924]
  value_value_mse : [0.2079, 0.2054, 0.2028, 0.2010, 0.1967, 0.1961, 0.1934, 0.1924]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:46:27.164652Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game139501_game141000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.409298   (epoch 10)   mode=min
  policy_loss: 2.314362   (epoch 10)   mode=min
  policy_policy_acc: 0.318830   (epoch 10)   mode=max
  policy_top10_acc: 0.822502   (epoch 10)   mode=max
  policy_top5_acc: 0.669590   (epoch 10)   mode=max
  val_loss: 2.604811   (epoch 4)   mode=min
  val_policy_loss: 2.503856   (epoch 4)   mode=min
  val_policy_policy_acc: 0.287512   (epoch 9)   mode=max
  val_policy_top10_acc: 0.782517   (epoch 5)   mode=max
  val_policy_top5_acc: 0.619980   (epoch 5)   mode=max
  val_value_loss: 0.201403   (epoch 7)   mode=min
  val_value_value_mse: 0.201427   (epoch 7)   mode=min
  value_loss: 0.189890   (epoch 10)   mode=min
  value_value_mse: 0.189885   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6667, 2.6282, 2.5898, 2.5330, 2.5063, 2.4825, 2.4512, 2.4374, 2.4163, 2.4093]
  policy_loss : [2.5639, 2.5267, 2.4897, 2.4345, 2.4090, 2.3855, 2.3554, 2.3421, 2.3213, 2.3144]
  policy_policy_acc : [0.2697, 0.2767, 0.2820, 0.2943, 0.3000, 0.3035, 0.3113, 0.3131, 0.3165, 0.3188]
  policy_top10_acc : [0.7698, 0.7783, 0.7872, 0.7985, 0.8029, 0.8081, 0.8148, 0.8175, 0.8211, 0.8225]
  policy_top5_acc : [0.6055, 0.6147, 0.6266, 0.6383, 0.6456, 0.6527, 0.6596, 0.6629, 0.6675, 0.6696]
  val_loss : [2.6227, 2.7961, 2.8011, 2.6048, 2.6073, 2.6409, 2.6326, 2.6667, 2.6471, 2.7049]
  val_policy_loss : [2.5197, 2.6915, 2.6976, 2.5039, 2.5061, 2.5388, 2.5318, 2.5655, 2.5463, 2.6040]
  val_policy_policy_acc : [0.2798, 0.2777, 0.2787, 0.2859, 0.2830, 0.2864, 0.2861, 0.2861, 0.2875, 0.2863]
  val_policy_top10_acc : [0.7818, 0.7792, 0.7756, 0.7803, 0.7825, 0.7759, 0.7794, 0.7765, 0.7798, 0.7787]
  val_policy_top5_acc : [0.6179, 0.6152, 0.6065, 0.6176, 0.6200, 0.6133, 0.6185, 0.6164, 0.6194, 0.6174]
  val_value_loss : [0.2059, 0.2088, 0.2065, 0.2017, 0.2022, 0.2041, 0.2014, 0.2023, 0.2015, 0.2016]
  val_value_value_mse : [0.2060, 0.2088, 0.2065, 0.2017, 0.2022, 0.2041, 0.2014, 0.2023, 0.2015, 0.2017]
  value_loss : [0.2055, 0.2029, 0.2003, 0.1973, 0.1950, 0.1936, 0.1917, 0.1906, 0.1899, 0.1899]
  value_value_mse : [0.2055, 0.2029, 0.2003, 0.1973, 0.1950, 0.1936, 0.1917, 0.1906, 0.1899, 0.1899]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T05:56:21.253724Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game141001_game142500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.411030   (epoch 10)   mode=min
  policy_loss: 2.314739   (epoch 10)   mode=min
  policy_policy_acc: 0.316002   (epoch 10)   mode=max
  policy_top10_acc: 0.825049   (epoch 10)   mode=max
  policy_top5_acc: 0.668953   (epoch 10)   mode=max
  val_loss: 2.611249   (epoch 4)   mode=min
  val_policy_loss: 2.509486   (epoch 4)   mode=min
  val_policy_policy_acc: 0.285415   (epoch 4)   mode=max
  val_policy_top10_acc: 0.780420   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620979   (epoch 10)   mode=max
  val_value_loss: 0.200377   (epoch 10)   mode=min
  val_value_value_mse: 0.200390   (epoch 10)   mode=min
  value_loss: 0.192554   (epoch 10)   mode=min
  value_value_mse: 0.192546   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6691, 2.6262, 2.5929, 2.5296, 2.5058, 2.4884, 2.4489, 2.4380, 2.4182, 2.4110]
  policy_loss : [2.5643, 2.5229, 2.4903, 2.4294, 2.4063, 2.3899, 2.3512, 2.3408, 2.3216, 2.3147]
  policy_policy_acc : [0.2674, 0.2762, 0.2822, 0.2926, 0.2997, 0.3005, 0.3101, 0.3116, 0.3154, 0.3160]
  policy_top10_acc : [0.7707, 0.7801, 0.7866, 0.7993, 0.8040, 0.8076, 0.8165, 0.8192, 0.8224, 0.8250]
  policy_top5_acc : [0.6042, 0.6176, 0.6255, 0.6405, 0.6465, 0.6498, 0.6610, 0.6633, 0.6684, 0.6690]
  val_loss : [2.6300, 2.6341, 2.6371, 2.6112, 2.6128, 2.6278, 2.6181, 2.6257, 2.6192, 2.6434]
  val_policy_loss : [2.5255, 2.5309, 2.5331, 2.5095, 2.5103, 2.5249, 2.5175, 2.5245, 2.5188, 2.5430]
  val_policy_policy_acc : [0.2736, 0.2740, 0.2776, 0.2854, 0.2847, 0.2812, 0.2841, 0.2815, 0.2840, 0.2842]
  val_policy_top10_acc : [0.7741, 0.7733, 0.7785, 0.7794, 0.7787, 0.7795, 0.7804, 0.7793, 0.7790, 0.7800]
  val_policy_top5_acc : [0.6107, 0.6106, 0.6089, 0.6185, 0.6163, 0.6169, 0.6209, 0.6153, 0.6194, 0.6210]
  val_value_loss : [0.2087, 0.2062, 0.2077, 0.2033, 0.2048, 0.2055, 0.2010, 0.2021, 0.2005, 0.2004]
  val_value_value_mse : [0.2087, 0.2062, 0.2077, 0.2033, 0.2048, 0.2055, 0.2010, 0.2022, 0.2005, 0.2004]
  value_loss : [0.2090, 0.2069, 0.2060, 0.2005, 0.1987, 0.1975, 0.1952, 0.1944, 0.1935, 0.1926]
  value_value_mse : [0.2090, 0.2069, 0.2060, 0.2005, 0.1988, 0.1975, 0.1952, 0.1944, 0.1935, 0.1925]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T06:06:17.433459Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game142501_game144000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.374763   (epoch 10)   mode=min
  policy_loss: 2.281378   (epoch 10)   mode=min
  policy_policy_acc: 0.322966   (epoch 10)   mode=max
  policy_top10_acc: 0.831888   (epoch 10)   mode=max
  policy_top5_acc: 0.678546   (epoch 10)   mode=max
  val_loss: 2.622562   (epoch 5)   mode=min
  val_policy_loss: 2.521500   (epoch 5)   mode=min
  val_policy_policy_acc: 0.285315   (epoch 8)   mode=max
  val_policy_top10_acc: 0.780819   (epoch 8)   mode=max
  val_policy_top5_acc: 0.619680   (epoch 10)   mode=max
  val_value_loss: 0.201085   (epoch 10)   mode=min
  val_value_value_mse: 0.201100   (epoch 10)   mode=min
  value_loss: 0.186819   (epoch 10)   mode=min
  value_value_mse: 0.186809   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6521, 2.6096, 2.5776, 2.5507, 2.4875, 2.4625, 2.4431, 2.4088, 2.3924, 2.3748]
  policy_loss : [2.5502, 2.5092, 2.4778, 2.4518, 2.3903, 2.3666, 2.3477, 2.3148, 2.2988, 2.2814]
  policy_policy_acc : [0.2699, 0.2777, 0.2832, 0.2887, 0.3013, 0.3068, 0.3099, 0.3161, 0.3192, 0.3230]
  policy_top10_acc : [0.7752, 0.7834, 0.7914, 0.7985, 0.8113, 0.8152, 0.8203, 0.8260, 0.8301, 0.8319]
  policy_top5_acc : [0.6098, 0.6197, 0.6293, 0.6364, 0.6520, 0.6594, 0.6644, 0.6730, 0.6758, 0.6785]
  val_loss : [2.6363, 2.6288, 2.6328, 2.6322, 2.6226, 2.6318, 2.7538, 2.7004, 2.6890, 2.6921]
  val_policy_loss : [2.5324, 2.5269, 2.5319, 2.5298, 2.5215, 2.5303, 2.6523, 2.5994, 2.5881, 2.5914]
  val_policy_policy_acc : [0.2741, 0.2745, 0.2758, 0.2786, 0.2812, 0.2794, 0.2792, 0.2853, 0.2833, 0.2841]
  val_policy_top10_acc : [0.7709, 0.7767, 0.7768, 0.7787, 0.7790, 0.7768, 0.7771, 0.7808, 0.7786, 0.7778]
  val_policy_top5_acc : [0.6098, 0.6134, 0.6130, 0.6152, 0.6168, 0.6132, 0.6155, 0.6158, 0.6172, 0.6197]
  val_value_loss : [0.2076, 0.2036, 0.2018, 0.2047, 0.2020, 0.2030, 0.2027, 0.2020, 0.2017, 0.2011]
  val_value_value_mse : [0.2076, 0.2037, 0.2019, 0.2047, 0.2020, 0.2030, 0.2027, 0.2020, 0.2017, 0.2011]
  value_loss : [0.2040, 0.2008, 0.1999, 0.1981, 0.1944, 0.1915, 0.1903, 0.1879, 0.1877, 0.1868]
  value_value_mse : [0.2040, 0.2008, 0.2000, 0.1981, 0.1944, 0.1915, 0.1903, 0.1879, 0.1877, 0.1868]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T06:15:47.660619Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game144001_game145500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399666   (epoch 10)   mode=min
  policy_loss: 2.300492   (epoch 10)   mode=min
  policy_policy_acc: 0.321257   (epoch 10)   mode=max
  policy_top10_acc: 0.827716   (epoch 10)   mode=max
  policy_top5_acc: 0.674683   (epoch 10)   mode=max
  val_loss: 2.617010   (epoch 4)   mode=min
  val_policy_loss: 2.514589   (epoch 4)   mode=min
  val_policy_policy_acc: 0.286414   (epoch 10)   mode=max
  val_policy_top10_acc: 0.782817   (epoch 8)   mode=max
  val_policy_top5_acc: 0.619281   (epoch 7)   mode=max
  val_value_loss: 0.202065   (epoch 9)   mode=min
  val_value_value_mse: 0.202077   (epoch 9)   mode=min
  value_loss: 0.198037   (epoch 10)   mode=min
  value_value_mse: 0.198017   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6684, 2.6232, 2.5881, 2.5232, 2.5005, 2.4775, 2.4415, 2.4259, 2.4064, 2.3997]
  policy_loss : [2.5622, 2.5182, 2.4837, 2.4209, 2.3995, 2.3766, 2.3416, 2.3264, 2.3069, 2.3005]
  policy_policy_acc : [0.2684, 0.2779, 0.2845, 0.2973, 0.3018, 0.3061, 0.3137, 0.3167, 0.3194, 0.3213]
  policy_top10_acc : [0.7694, 0.7809, 0.7903, 0.8017, 0.8084, 0.8118, 0.8195, 0.8205, 0.8260, 0.8277]
  policy_top5_acc : [0.6055, 0.6185, 0.6264, 0.6428, 0.6483, 0.6545, 0.6637, 0.6668, 0.6724, 0.6747]
  val_loss : [2.6291, 2.6558, 2.6285, 2.6170, 2.8345, 2.8929, 2.6646, 2.8027, 2.8002, 2.7613]
  val_policy_loss : [2.5251, 2.5500, 2.5269, 2.5146, 2.7332, 2.7908, 2.5633, 2.7007, 2.6990, 2.6601]
  val_policy_policy_acc : [0.2760, 0.2708, 0.2793, 0.2853, 0.2846, 0.2794, 0.2859, 0.2852, 0.2858, 0.2864]
  val_policy_top10_acc : [0.7713, 0.7709, 0.7767, 0.7806, 0.7815, 0.7768, 0.7784, 0.7828, 0.7810, 0.7812]
  val_policy_top5_acc : [0.6121, 0.6048, 0.6092, 0.6171, 0.6141, 0.6170, 0.6193, 0.6167, 0.6181, 0.6184]
  val_value_loss : [0.2078, 0.2116, 0.2031, 0.2046, 0.2023, 0.2038, 0.2024, 0.2036, 0.2021, 0.2023]
  val_value_value_mse : [0.2078, 0.2116, 0.2031, 0.2047, 0.2023, 0.2039, 0.2024, 0.2037, 0.2021, 0.2023]
  value_loss : [0.2125, 0.2101, 0.2088, 0.2048, 0.2029, 0.2016, 0.1999, 0.1989, 0.1983, 0.1980]
  value_value_mse : [0.2125, 0.2101, 0.2088, 0.2048, 0.2029, 0.2016, 0.1999, 0.1989, 0.1983, 0.1980]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T06:22:57.677897Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game145501_game147000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.459898   (epoch 7)   mode=min
  policy_loss: 2.361509   (epoch 7)   mode=min
  policy_policy_acc: 0.305544   (epoch 7)   mode=max
  policy_top10_acc: 0.816418   (epoch 7)   mode=max
  policy_top5_acc: 0.657106   (epoch 7)   mode=max
  val_loss: 2.620752   (epoch 4)   mode=min
  val_policy_loss: 2.519112   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280719   (epoch 6)   mode=max
  val_policy_top10_acc: 0.780220   (epoch 7)   mode=max
  val_policy_top5_acc: 0.622577   (epoch 7)   mode=max
  val_value_loss: 0.199941   (epoch 7)   mode=min
  val_value_value_mse: 0.199965   (epoch 7)   mode=min
  value_loss: 0.196794   (epoch 7)   mode=min
  value_value_mse: 0.196795   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6784, 2.6325, 2.5982, 2.5375, 2.5112, 2.4745, 2.4599]
  policy_loss : [2.5740, 2.5291, 2.4959, 2.4372, 2.4116, 2.3759, 2.3615]
  policy_policy_acc : [0.2648, 0.2727, 0.2781, 0.2905, 0.2957, 0.3035, 0.3055]
  policy_top10_acc : [0.7690, 0.7795, 0.7873, 0.8001, 0.8054, 0.8137, 0.8164]
  policy_top5_acc : [0.6008, 0.6137, 0.6227, 0.6384, 0.6459, 0.6549, 0.6571]
  val_loss : [2.6224, 2.6403, 2.8387, 2.6208, 2.6210, 2.6372, 2.9176]
  val_policy_loss : [2.5191, 2.5386, 2.7363, 2.5194, 2.5194, 2.5366, 2.8173]
  val_policy_policy_acc : [0.2766, 0.2722, 0.2666, 0.2759, 0.2769, 0.2807, 0.2804]
  val_policy_top10_acc : [0.7781, 0.7763, 0.7734, 0.7780, 0.7795, 0.7794, 0.7802]
  val_policy_top5_acc : [0.6121, 0.6144, 0.6117, 0.6168, 0.6177, 0.6225, 0.6226]
  val_value_loss : [0.2064, 0.2033, 0.2044, 0.2024, 0.2030, 0.2008, 0.1999]
  val_value_value_mse : [0.2064, 0.2034, 0.2044, 0.2024, 0.2031, 0.2008, 0.2000]
  value_loss : [0.2087, 0.2069, 0.2046, 0.2005, 0.1992, 0.1971, 0.1968]
  value_value_mse : [0.2087, 0.2069, 0.2046, 0.2005, 0.1992, 0.1971, 0.1968]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T06:31:44.220406Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game147001_game148500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.382079   (epoch 9)   mode=min
  policy_loss: 2.288751   (epoch 9)   mode=min
  policy_policy_acc: 0.322476   (epoch 9)   mode=max
  policy_top10_acc: 0.831187   (epoch 9)   mode=max
  policy_top5_acc: 0.677684   (epoch 9)   mode=max
  val_loss: 2.636336   (epoch 3)   mode=min
  val_policy_loss: 2.534194   (epoch 3)   mode=min
  val_policy_policy_acc: 0.280619   (epoch 8)   mode=max
  val_policy_top10_acc: 0.779920   (epoch 7)   mode=max
  val_policy_top5_acc: 0.616583   (epoch 9)   mode=max
  val_value_loss: 0.200522   (epoch 7)   mode=min
  val_value_value_mse: 0.200538   (epoch 7)   mode=min
  value_loss: 0.186541   (epoch 9)   mode=min
  value_value_mse: 0.186510   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6503, 2.6050, 2.5762, 2.5481, 2.5192, 2.4593, 2.4324, 2.3950, 2.3821]
  policy_loss : [2.5493, 2.5052, 2.4772, 2.4496, 2.4214, 2.3637, 2.3374, 2.3013, 2.2888]
  policy_policy_acc : [0.2691, 0.2782, 0.2849, 0.2894, 0.2956, 0.3080, 0.3102, 0.3183, 0.3225]
  policy_top10_acc : [0.7765, 0.7849, 0.7927, 0.7997, 0.8063, 0.8171, 0.8219, 0.8289, 0.8312]
  policy_top5_acc : [0.6107, 0.6230, 0.6292, 0.6370, 0.6474, 0.6590, 0.6648, 0.6759, 0.6777]
  val_loss : [2.7399, 2.8089, 2.6363, 2.6443, 2.9169, 2.7431, 2.9647, 3.4989, 3.0718]
  val_policy_loss : [2.6361, 2.7036, 2.5342, 2.5422, 2.8093, 2.6420, 2.8642, 3.3970, 2.9688]
  val_policy_policy_acc : [0.2744, 0.2727, 0.2764, 0.2693, 0.2719, 0.2802, 0.2787, 0.2806, 0.2800]
  val_policy_top10_acc : [0.7742, 0.7764, 0.7732, 0.7739, 0.7699, 0.7773, 0.7799, 0.7791, 0.7775]
  val_policy_top5_acc : [0.6093, 0.6145, 0.6111, 0.6093, 0.6095, 0.6150, 0.6152, 0.6156, 0.6166]
  val_value_loss : [0.2074, 0.2104, 0.2043, 0.2041, 0.2150, 0.2021, 0.2005, 0.2025, 0.2054]
  val_value_value_mse : [0.2074, 0.2104, 0.2043, 0.2041, 0.2150, 0.2021, 0.2005, 0.2025, 0.2054]
  value_loss : [0.2019, 0.1997, 0.1982, 0.1968, 0.1950, 0.1911, 0.1899, 0.1871, 0.1865]
  value_value_mse : [0.2019, 0.1997, 0.1981, 0.1968, 0.1950, 0.1910, 0.1899, 0.1871, 0.1865]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T06:41:41.422359Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game148501_game150000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.388581   (epoch 10)   mode=min
  policy_loss: 2.294957   (epoch 10)   mode=min
  policy_policy_acc: 0.320927   (epoch 10)   mode=max
  policy_top10_acc: 0.828502   (epoch 10)   mode=max
  policy_top5_acc: 0.676496   (epoch 10)   mode=max
  val_loss: 2.611120   (epoch 5)   mode=min
  val_policy_loss: 2.510417   (epoch 5)   mode=min
  val_policy_policy_acc: 0.283516   (epoch 10)   mode=max
  val_policy_top10_acc: 0.784316   (epoch 7)   mode=max
  val_policy_top5_acc: 0.623477   (epoch 10)   mode=max
  val_value_loss: 0.200376   (epoch 9)   mode=min
  val_value_value_mse: 0.200401   (epoch 9)   mode=min
  value_loss: 0.187286   (epoch 10)   mode=min
  value_value_mse: 0.187284   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6543, 2.6099, 2.5824, 2.5169, 2.4921, 2.4745, 2.4561, 2.4203, 2.4052, 2.3886]
  policy_loss : [2.5532, 2.5100, 2.4829, 2.4199, 2.3958, 2.3783, 2.3606, 2.3257, 2.3113, 2.2950]
  policy_policy_acc : [0.2695, 0.2789, 0.2828, 0.2963, 0.3011, 0.3054, 0.3079, 0.3159, 0.3188, 0.3209]
  policy_top10_acc : [0.7740, 0.7835, 0.7909, 0.8043, 0.8085, 0.8124, 0.8156, 0.8221, 0.8261, 0.8285]
  policy_top5_acc : [0.6081, 0.6198, 0.6259, 0.6439, 0.6503, 0.6560, 0.6597, 0.6680, 0.6717, 0.6765]
  val_loss : [2.6307, 2.6583, 2.7059, 2.6235, 2.6111, 2.6949, 2.8322, 2.7729, 2.8935, 2.8289]
  val_policy_loss : [2.5288, 2.5555, 2.6052, 2.5228, 2.5104, 2.5934, 2.7309, 2.6724, 2.7930, 2.7283]
  val_policy_policy_acc : [0.2770, 0.2754, 0.2725, 0.2802, 0.2805, 0.2804, 0.2815, 0.2811, 0.2803, 0.2835]
  val_policy_top10_acc : [0.7752, 0.7714, 0.7755, 0.7815, 0.7801, 0.7800, 0.7843, 0.7823, 0.7802, 0.7810]
  val_policy_top5_acc : [0.6068, 0.6067, 0.6144, 0.6214, 0.6204, 0.6190, 0.6181, 0.6223, 0.6206, 0.6235]
  val_value_loss : [0.2035, 0.2053, 0.2010, 0.2013, 0.2011, 0.2025, 0.2020, 0.2005, 0.2004, 0.2005]
  val_value_value_mse : [0.2036, 0.2053, 0.2010, 0.2013, 0.2012, 0.2025, 0.2020, 0.2005, 0.2004, 0.2005]
  value_loss : [0.2023, 0.1998, 0.1990, 0.1940, 0.1927, 0.1924, 0.1910, 0.1892, 0.1878, 0.1873]
  value_value_mse : [0.2023, 0.1998, 0.1990, 0.1940, 0.1927, 0.1924, 0.1910, 0.1892, 0.1878, 0.1873]

================================================================================

History file: model_versions/chess_elo_model_V11_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T20:58:58.259063Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game1_game1500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V10
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.364132   (epoch 10)   mode=min
  policy_loss: 2.266397   (epoch 10)   mode=min
  policy_policy_acc: 0.326404   (epoch 10)   mode=max
  policy_top10_acc: 0.837589   (epoch 10)   mode=max
  policy_top5_acc: 0.686849   (epoch 10)   mode=max
  val_loss: 2.635029   (epoch 4)   mode=min
  val_policy_loss: 2.532224   (epoch 4)   mode=min
  val_policy_policy_acc: 0.283017   (epoch 9)   mode=max
  val_policy_top10_acc: 0.780819   (epoch 10)   mode=max
  val_policy_top5_acc: 0.619081   (epoch 10)   mode=max
  val_value_loss: 0.203149   (epoch 9)   mode=min
  val_value_value_mse: 0.203171   (epoch 9)   mode=min
  value_loss: 0.195343   (epoch 10)   mode=min
  value_value_mse: 0.195351   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6625, 2.6183, 2.5889, 2.5589, 2.5281, 2.5032, 2.4384, 2.4174, 2.3808, 2.3641]
  policy_loss : [2.5567, 2.5132, 2.4847, 2.4555, 2.4255, 2.4008, 2.3386, 2.3181, 2.2825, 2.2664]
  policy_policy_acc : [0.2681, 0.2762, 0.2836, 0.2878, 0.2939, 0.2984, 0.3117, 0.3156, 0.3236, 0.3264]
  policy_top10_acc : [0.7761, 0.7867, 0.7923, 0.8001, 0.8063, 0.8122, 0.8243, 0.8283, 0.8357, 0.8376]
  policy_top5_acc : [0.6103, 0.6226, 0.6297, 0.6385, 0.6456, 0.6533, 0.6688, 0.6731, 0.6808, 0.6868]
  val_loss : [2.7997, 2.6407, 2.6423, 2.6350, 2.6442, 2.7272, 2.7037, 2.6847, 2.7313, 2.6997]
  val_policy_loss : [2.6954, 2.5366, 2.5391, 2.5322, 2.5403, 2.6245, 2.6014, 2.5825, 2.6295, 2.5973]
  val_policy_policy_acc : [0.2747, 0.2782, 0.2795, 0.2774, 0.2751, 0.2744, 0.2817, 0.2804, 0.2830, 0.2781]
  val_policy_top10_acc : [0.7798, 0.7766, 0.7750, 0.7745, 0.7759, 0.7696, 0.7756, 0.7776, 0.7743, 0.7808]
  val_policy_top5_acc : [0.6109, 0.6148, 0.6140, 0.6134, 0.6128, 0.6084, 0.6163, 0.6187, 0.6186, 0.6191]
  val_value_loss : [0.2084, 0.2079, 0.2060, 0.2053, 0.2077, 0.2050, 0.2041, 0.2040, 0.2031, 0.2045]
  val_value_value_mse : [0.2084, 0.2079, 0.2060, 0.2054, 0.2077, 0.2051, 0.2041, 0.2041, 0.2032, 0.2045]
  value_loss : [0.2116, 0.2105, 0.2083, 0.2067, 0.2054, 0.2044, 0.1998, 0.1987, 0.1962, 0.1953]
  value_value_mse : [0.2116, 0.2105, 0.2083, 0.2067, 0.2054, 0.2044, 0.1998, 0.1987, 0.1962, 0.1954]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:07:23.436670Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game1501_game3000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.403478   (epoch 8)   mode=min
  policy_loss: 2.307805   (epoch 8)   mode=min
  policy_policy_acc: 0.318361   (epoch 8)   mode=max
  policy_top10_acc: 0.826581   (epoch 8)   mode=max
  policy_top5_acc: 0.674313   (epoch 8)   mode=max
  val_loss: 2.628832   (epoch 2)   mode=min
  val_policy_loss: 2.525480   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281518   (epoch 7)   mode=max
  val_policy_top10_acc: 0.780320   (epoch 6)   mode=max
  val_policy_top5_acc: 0.620080   (epoch 8)   mode=max
  val_value_loss: 0.202218   (epoch 6)   mode=min
  val_value_value_mse: 0.202243   (epoch 6)   mode=min
  value_loss: 0.191351   (epoch 8)   mode=min
  value_value_mse: 0.191350   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6486, 2.6011, 2.5713, 2.5364, 2.4755, 2.4505, 2.4167, 2.4035]
  policy_loss : [2.5454, 2.4994, 2.4705, 2.4370, 2.3780, 2.3534, 2.3207, 2.3078]
  policy_policy_acc : [0.2712, 0.2795, 0.2846, 0.2917, 0.3032, 0.3089, 0.3153, 0.3184]
  policy_top10_acc : [0.7761, 0.7868, 0.7946, 0.8002, 0.8129, 0.8191, 0.8242, 0.8266]
  policy_top5_acc : [0.6134, 0.6260, 0.6337, 0.6399, 0.6564, 0.6618, 0.6704, 0.6743]
  val_loss : [2.6466, 2.6288, 2.7414, 2.6850, 2.6401, 2.6387, 2.6383, 2.6411]
  val_policy_loss : [2.5438, 2.5255, 2.6345, 2.5798, 2.5380, 2.5376, 2.5349, 2.5386]
  val_policy_policy_acc : [0.2718, 0.2767, 0.2789, 0.2718, 0.2800, 0.2791, 0.2815, 0.2773]
  val_policy_top10_acc : [0.7754, 0.7798, 0.7741, 0.7757, 0.7767, 0.7803, 0.7794, 0.7793]
  val_policy_top5_acc : [0.6119, 0.6148, 0.6111, 0.6080, 0.6192, 0.6190, 0.6171, 0.6201]
  val_value_loss : [0.2055, 0.2065, 0.2135, 0.2101, 0.2040, 0.2022, 0.2066, 0.2047]
  val_value_value_mse : [0.2055, 0.2065, 0.2135, 0.2101, 0.2041, 0.2022, 0.2066, 0.2048]
  value_loss : [0.2063, 0.2034, 0.2015, 0.1988, 0.1949, 0.1941, 0.1920, 0.1914]
  value_value_mse : [0.2063, 0.2034, 0.2015, 0.1988, 0.1949, 0.1941, 0.1920, 0.1914]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:15:31.189092Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game3001_game4500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.428441   (epoch 8)   mode=min
  policy_loss: 2.330756   (epoch 8)   mode=min
  policy_policy_acc: 0.316647   (epoch 8)   mode=max
  policy_top10_acc: 0.821018   (epoch 8)   mode=max
  policy_top5_acc: 0.667539   (epoch 8)   mode=max
  val_loss: 2.618838   (epoch 2)   mode=min
  val_policy_loss: 2.515165   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281219   (epoch 5)   mode=max
  val_policy_top10_acc: 0.781319   (epoch 5)   mode=max
  val_policy_top5_acc: 0.618681   (epoch 5)   mode=max
  val_value_loss: 0.201246   (epoch 7)   mode=min
  val_value_value_mse: 0.201274   (epoch 7)   mode=min
  value_loss: 0.195486   (epoch 8)   mode=min
  value_value_mse: 0.195484   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6736, 2.6312, 2.5981, 2.5678, 2.5033, 2.4815, 2.4445, 2.4284]
  policy_loss : [2.5690, 2.5277, 2.4952, 2.4656, 2.4034, 2.3826, 2.3465, 2.3308]
  policy_policy_acc : [0.2695, 0.2769, 0.2839, 0.2890, 0.3021, 0.3062, 0.3139, 0.3166]
  policy_top10_acc : [0.7696, 0.7792, 0.7858, 0.7932, 0.8059, 0.8108, 0.8175, 0.8210]
  policy_top5_acc : [0.6065, 0.6173, 0.6260, 0.6343, 0.6498, 0.6544, 0.6626, 0.6675]
  val_loss : [2.6191, 2.6188, 2.6357, 2.6847, 2.6852, 2.9102, 2.8022, 2.8328]
  val_policy_loss : [2.5161, 2.5152, 2.5313, 2.5824, 2.5841, 2.8091, 2.7013, 2.7316]
  val_policy_policy_acc : [0.2779, 0.2741, 0.2729, 0.2744, 0.2812, 0.2774, 0.2773, 0.2791]
  val_policy_top10_acc : [0.7800, 0.7767, 0.7737, 0.7724, 0.7813, 0.7790, 0.7803, 0.7804]
  val_policy_top5_acc : [0.6136, 0.6133, 0.6132, 0.6111, 0.6187, 0.6187, 0.6166, 0.6182]
  val_value_loss : [0.2057, 0.2071, 0.2084, 0.2043, 0.2019, 0.2015, 0.2012, 0.2017]
  val_value_value_mse : [0.2058, 0.2071, 0.2084, 0.2043, 0.2019, 0.2016, 0.2013, 0.2017]
  value_loss : [0.2091, 0.2069, 0.2057, 0.2041, 0.1997, 0.1980, 0.1959, 0.1955]
  value_value_mse : [0.2091, 0.2069, 0.2057, 0.2041, 0.1997, 0.1980, 0.1959, 0.1955]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:26:05.832066Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game4501_game6000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.426560   (epoch 10)   mode=min
  policy_loss: 2.327588   (epoch 10)   mode=min
  policy_policy_acc: 0.311559   (epoch 10)   mode=max
  policy_top10_acc: 0.821373   (epoch 10)   mode=max
  policy_top5_acc: 0.666288   (epoch 10)   mode=max
  val_loss: 2.614871   (epoch 4)   mode=min
  val_policy_loss: 2.514903   (epoch 4)   mode=min
  val_policy_policy_acc: 0.282917   (epoch 8)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 3)   mode=max
  val_policy_top5_acc: 0.621478   (epoch 5)   mode=max
  val_value_loss: 0.199153   (epoch 10)   mode=min
  val_value_value_mse: 0.199183   (epoch 10)   mode=min
  value_loss: 0.197981   (epoch 10)   mode=min
  value_value_mse: 0.197981   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6822, 2.6388, 2.6056, 2.5470, 2.5221, 2.5045, 2.4682, 2.4514, 2.4340, 2.4266]
  policy_loss : [2.5755, 2.5337, 2.5015, 2.4448, 2.4202, 2.4036, 2.3683, 2.3518, 2.3348, 2.3276]
  policy_policy_acc : [0.2622, 0.2714, 0.2761, 0.2888, 0.2929, 0.2972, 0.3035, 0.3067, 0.3112, 0.3116]
  policy_top10_acc : [0.7668, 0.7760, 0.7852, 0.7979, 0.8030, 0.8054, 0.8135, 0.8176, 0.8201, 0.8214]
  policy_top5_acc : [0.6010, 0.6126, 0.6190, 0.6353, 0.6415, 0.6467, 0.6544, 0.6595, 0.6651, 0.6663]
  val_loss : [2.6199, 2.6391, 2.6278, 2.6149, 2.6743, 2.7448, 2.7372, 2.7498, 2.7016, 2.8826]
  val_policy_loss : [2.5183, 2.5388, 2.5261, 2.5149, 2.5739, 2.6437, 2.6367, 2.6497, 2.6016, 2.7827]
  val_policy_policy_acc : [0.2771, 0.2713, 0.2757, 0.2783, 0.2813, 0.2816, 0.2818, 0.2829, 0.2824, 0.2809]
  val_policy_top10_acc : [0.7785, 0.7752, 0.7814, 0.7774, 0.7789, 0.7793, 0.7779, 0.7795, 0.7773, 0.7771]
  val_policy_top5_acc : [0.6166, 0.6099, 0.6187, 0.6183, 0.6215, 0.6202, 0.6212, 0.6195, 0.6207, 0.6193]
  val_value_loss : [0.2028, 0.2005, 0.2032, 0.1997, 0.2005, 0.2020, 0.2004, 0.1997, 0.1996, 0.1992]
  val_value_value_mse : [0.2029, 0.2005, 0.2032, 0.1997, 0.2005, 0.2020, 0.2004, 0.1997, 0.1996, 0.1992]
  value_loss : [0.2132, 0.2103, 0.2079, 0.2045, 0.2039, 0.2018, 0.1999, 0.1992, 0.1984, 0.1980]
  value_value_mse : [0.2132, 0.2103, 0.2079, 0.2045, 0.2039, 0.2018, 0.1999, 0.1992, 0.1984, 0.1980]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:33:28.837708Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game6001_game7500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.459157   (epoch 7)   mode=min
  policy_loss: 2.364474   (epoch 7)   mode=min
  policy_policy_acc: 0.306496   (epoch 7)   mode=max
  policy_top10_acc: 0.813202   (epoch 7)   mode=max
  policy_top5_acc: 0.656774   (epoch 7)   mode=max
  val_loss: 2.632148   (epoch 1)   mode=min
  val_policy_loss: 2.528185   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280020   (epoch 7)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 7)   mode=max
  val_policy_top5_acc: 0.624975   (epoch 7)   mode=max
  val_value_loss: 0.200994   (epoch 5)   mode=min
  val_value_value_mse: 0.201015   (epoch 5)   mode=min
  value_loss: 0.189119   (epoch 7)   mode=min
  value_value_mse: 0.189096   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6653, 2.6279, 2.5942, 2.5341, 2.5088, 2.4729, 2.4592]
  policy_loss : [2.5651, 2.5282, 2.4958, 2.4367, 2.4135, 2.3784, 2.3645]
  policy_policy_acc : [0.2665, 0.2758, 0.2818, 0.2927, 0.2966, 0.3041, 0.3065]
  policy_top10_acc : [0.7698, 0.7770, 0.7861, 0.7986, 0.8040, 0.8108, 0.8132]
  policy_top5_acc : [0.6054, 0.6147, 0.6233, 0.6399, 0.6444, 0.6545, 0.6568]
  val_loss : [2.6321, 2.7333, 2.7157, 2.6462, 2.8393, 2.8134, 2.6757]
  val_policy_loss : [2.5282, 2.6302, 2.6125, 2.5442, 2.7386, 2.7126, 2.5749]
  val_policy_policy_acc : [0.2755, 0.2749, 0.2710, 0.2792, 0.2774, 0.2778, 0.2800]
  val_policy_top10_acc : [0.7748, 0.7787, 0.7767, 0.7845, 0.7827, 0.7825, 0.7853]
  val_policy_top5_acc : [0.6160, 0.6188, 0.6112, 0.6237, 0.6206, 0.6242, 0.6250]
  val_value_loss : [0.2078, 0.2059, 0.2062, 0.2036, 0.2010, 0.2011, 0.2012]
  val_value_value_mse : [0.2078, 0.2059, 0.2062, 0.2036, 0.2010, 0.2011, 0.2013]
  value_loss : [0.2000, 0.1988, 0.1968, 0.1941, 0.1911, 0.1896, 0.1891]
  value_value_mse : [0.1999, 0.1988, 0.1968, 0.1940, 0.1911, 0.1896, 0.1891]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:40:33.116444Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game7501_game9000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.466107   (epoch 7)   mode=min
  policy_loss: 2.366994   (epoch 7)   mode=min
  policy_policy_acc: 0.308539   (epoch 7)   mode=max
  policy_top10_acc: 0.815917   (epoch 7)   mode=max
  policy_top5_acc: 0.659702   (epoch 7)   mode=max
  val_loss: 2.636344   (epoch 1)   mode=min
  val_policy_loss: 2.531666   (epoch 1)   mode=min
  val_policy_policy_acc: 0.281419   (epoch 6)   mode=max
  val_policy_top10_acc: 0.783916   (epoch 7)   mode=max
  val_policy_top5_acc: 0.624975   (epoch 7)   mode=max
  val_value_loss: 0.202014   (epoch 6)   mode=min
  val_value_value_mse: 0.202020   (epoch 6)   mode=min
  value_loss: 0.198270   (epoch 7)   mode=min
  value_value_mse: 0.198268   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6732, 2.6334, 2.5980, 2.5390, 2.5146, 2.4764, 2.4661]
  policy_loss : [2.5687, 2.5297, 2.4943, 2.4379, 2.4145, 2.3767, 2.3670]
  policy_policy_acc : [0.2684, 0.2768, 0.2827, 0.2947, 0.2990, 0.3073, 0.3085]
  policy_top10_acc : [0.7717, 0.7801, 0.7890, 0.7994, 0.8042, 0.8134, 0.8159]
  policy_top5_acc : [0.6072, 0.6165, 0.6250, 0.6403, 0.6470, 0.6572, 0.6597]
  val_loss : [2.6363, 2.8250, 3.2106, 3.2875, 2.7641, 3.2215, 2.9744]
  val_policy_loss : [2.5317, 2.7191, 3.1068, 3.1837, 2.6604, 3.1200, 2.8726]
  val_policy_policy_acc : [0.2752, 0.2743, 0.2752, 0.2767, 0.2784, 0.2814, 0.2804]
  val_policy_top10_acc : [0.7779, 0.7768, 0.7786, 0.7814, 0.7806, 0.7830, 0.7839]
  val_policy_top5_acc : [0.6137, 0.6119, 0.6174, 0.6238, 0.6235, 0.6231, 0.6250]
  val_value_loss : [0.2093, 0.2113, 0.2068, 0.2065, 0.2070, 0.2020, 0.2029]
  val_value_value_mse : [0.2093, 0.2113, 0.2068, 0.2065, 0.2070, 0.2020, 0.2029]
  value_loss : [0.2090, 0.2075, 0.2075, 0.2021, 0.2001, 0.1994, 0.1983]
  value_value_mse : [0.2090, 0.2075, 0.2075, 0.2021, 0.2001, 0.1994, 0.1983]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:47:48.091755Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game9001_game10500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.471973   (epoch 7)   mode=min
  policy_loss: 2.373828   (epoch 7)   mode=min
  policy_policy_acc: 0.305768   (epoch 7)   mode=max
  policy_top10_acc: 0.813847   (epoch 7)   mode=max
  policy_top5_acc: 0.655023   (epoch 7)   mode=max
  val_loss: 2.626988   (epoch 1)   mode=min
  val_policy_loss: 2.525455   (epoch 1)   mode=min
  val_policy_policy_acc: 0.279321   (epoch 6)   mode=max
  val_policy_top10_acc: 0.781618   (epoch 6)   mode=max
  val_policy_top5_acc: 0.623377   (epoch 4)   mode=max
  val_value_loss: 0.201840   (epoch 6)   mode=min
  val_value_value_mse: 0.201844   (epoch 6)   mode=min
  value_loss: 0.196318   (epoch 7)   mode=min
  value_value_mse: 0.196317   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6795, 2.6395, 2.6057, 2.5466, 2.5214, 2.4880, 2.4720]
  policy_loss : [2.5751, 2.5361, 2.5029, 2.4459, 2.4219, 2.3893, 2.3738]
  policy_policy_acc : [0.2648, 0.2741, 0.2788, 0.2901, 0.2956, 0.3020, 0.3058]
  policy_top10_acc : [0.7694, 0.7788, 0.7868, 0.7989, 0.8033, 0.8115, 0.8138]
  policy_top5_acc : [0.6027, 0.6145, 0.6228, 0.6386, 0.6427, 0.6518, 0.6550]
  val_loss : [2.6270, 2.6457, 2.7032, 2.7782, 2.9306, 2.7204, 2.8195]
  val_policy_loss : [2.5255, 2.5416, 2.5995, 2.6756, 2.8284, 2.6193, 2.7182]
  val_policy_policy_acc : [0.2765, 0.2744, 0.2762, 0.2762, 0.2771, 0.2793, 0.2776]
  val_policy_top10_acc : [0.7778, 0.7805, 0.7770, 0.7799, 0.7800, 0.7816, 0.7795]
  val_policy_top5_acc : [0.6121, 0.6201, 0.6146, 0.6234, 0.6230, 0.6234, 0.6207]
  val_value_loss : [0.2029, 0.2082, 0.2071, 0.2050, 0.2038, 0.2018, 0.2022]
  val_value_value_mse : [0.2030, 0.2082, 0.2071, 0.2050, 0.2038, 0.2018, 0.2022]
  value_loss : [0.2087, 0.2067, 0.2055, 0.2014, 0.1990, 0.1974, 0.1963]
  value_value_mse : [0.2087, 0.2067, 0.2055, 0.2013, 0.1990, 0.1974, 0.1963]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T21:55:11.532577Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game10501_game12000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.450283   (epoch 7)   mode=min
  policy_loss: 2.353290   (epoch 7)   mode=min
  policy_policy_acc: 0.309969   (epoch 7)   mode=max
  policy_top10_acc: 0.817610   (epoch 7)   mode=max
  policy_top5_acc: 0.661229   (epoch 7)   mode=max
  val_loss: 2.650419   (epoch 1)   mode=min
  val_policy_loss: 2.547325   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283417   (epoch 6)   mode=max
  val_policy_top10_acc: 0.780919   (epoch 5)   mode=max
  val_policy_top5_acc: 0.625175   (epoch 6)   mode=max
  val_value_loss: 0.200783   (epoch 6)   mode=min
  val_value_value_mse: 0.200807   (epoch 6)   mode=min
  value_loss: 0.193983   (epoch 7)   mode=min
  value_value_mse: 0.193952   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6579, 2.6179, 2.5857, 2.5245, 2.5025, 2.4645, 2.4503]
  policy_loss : [2.5546, 2.5160, 2.4845, 2.4259, 2.4043, 2.3673, 2.3533]
  policy_policy_acc : [0.2697, 0.2789, 0.2832, 0.2958, 0.2992, 0.3091, 0.3100]
  policy_top10_acc : [0.7737, 0.7846, 0.7921, 0.8034, 0.8075, 0.8158, 0.8176]
  policy_top5_acc : [0.6080, 0.6199, 0.6288, 0.6432, 0.6480, 0.6595, 0.6612]
  val_loss : [2.6504, 2.8158, 2.8464, 2.7784, 4.5245, 3.3860, 3.1852]
  val_policy_loss : [2.5473, 2.7122, 2.7425, 2.6778, 4.4224, 3.2851, 3.0839]
  val_policy_policy_acc : [0.2776, 0.2786, 0.2753, 0.2823, 0.2791, 0.2834, 0.2825]
  val_policy_top10_acc : [0.7789, 0.7760, 0.7745, 0.7793, 0.7809, 0.7799, 0.7801]
  val_policy_top5_acc : [0.6185, 0.6131, 0.6094, 0.6172, 0.6209, 0.6252, 0.6206]
  val_value_loss : [0.2059, 0.2067, 0.2074, 0.2009, 0.2017, 0.2008, 0.2017]
  val_value_value_mse : [0.2060, 0.2067, 0.2074, 0.2009, 0.2018, 0.2008, 0.2017]
  value_loss : [0.2064, 0.2028, 0.2024, 0.1977, 0.1970, 0.1943, 0.1940]
  value_value_mse : [0.2064, 0.2028, 0.2024, 0.1977, 0.1970, 0.1943, 0.1940]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:04:10.853589Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game12001_game13500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.392142   (epoch 9)   mode=min
  policy_loss: 2.296338   (epoch 9)   mode=min
  policy_policy_acc: 0.320279   (epoch 9)   mode=max
  policy_top10_acc: 0.829462   (epoch 9)   mode=max
  policy_top5_acc: 0.675633   (epoch 9)   mode=max
  val_loss: 2.642227   (epoch 3)   mode=min
  val_policy_loss: 2.539500   (epoch 3)   mode=min
  val_policy_policy_acc: 0.281019   (epoch 9)   mode=max
  val_policy_top10_acc: 0.781618   (epoch 3)   mode=max
  val_policy_top5_acc: 0.619081   (epoch 8)   mode=max
  val_value_loss: 0.200961   (epoch 8)   mode=min
  val_value_value_mse: 0.200991   (epoch 8)   mode=min
  value_loss: 0.191600   (epoch 9)   mode=min
  value_value_mse: 0.191600   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6471, 2.6050, 2.5735, 2.5464, 2.5214, 2.4654, 2.4376, 2.4067, 2.3921]
  policy_loss : [2.5435, 2.5031, 2.4718, 2.4457, 2.4212, 2.3671, 2.3402, 2.3102, 2.2963]
  policy_policy_acc : [0.2707, 0.2787, 0.2860, 0.2899, 0.2942, 0.3067, 0.3119, 0.3183, 0.3203]
  policy_top10_acc : [0.7793, 0.7857, 0.7939, 0.7991, 0.8052, 0.8155, 0.8214, 0.8255, 0.8295]
  policy_top5_acc : [0.6116, 0.6226, 0.6300, 0.6387, 0.6445, 0.6572, 0.6654, 0.6712, 0.6756]
  val_loss : [2.7010, 2.6501, 2.6422, 2.8808, 2.7205, 2.7798, 2.8945, 2.8553, 2.7345]
  val_policy_loss : [2.5983, 2.5484, 2.5395, 2.7778, 2.6189, 2.6780, 2.7929, 2.7544, 2.6334]
  val_policy_policy_acc : [0.2740, 0.2768, 0.2783, 0.2732, 0.2762, 0.2807, 0.2758, 0.2786, 0.2810]
  val_policy_top10_acc : [0.7771, 0.7813, 0.7816, 0.7763, 0.7729, 0.7772, 0.7763, 0.7788, 0.7795]
  val_policy_top5_acc : [0.6117, 0.6145, 0.6138, 0.6145, 0.6148, 0.6135, 0.6134, 0.6191, 0.6171]
  val_value_loss : [0.2051, 0.2030, 0.2052, 0.2055, 0.2028, 0.2030, 0.2026, 0.2010, 0.2017]
  val_value_value_mse : [0.2051, 0.2030, 0.2052, 0.2055, 0.2028, 0.2030, 0.2026, 0.2010, 0.2018]
  value_loss : [0.2071, 0.2038, 0.2034, 0.2014, 0.2003, 0.1966, 0.1947, 0.1929, 0.1916]
  value_value_mse : [0.2071, 0.2038, 0.2034, 0.2014, 0.2003, 0.1966, 0.1947, 0.1929, 0.1916]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:11:14.552411Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game13501_game15000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441632   (epoch 7)   mode=min
  policy_loss: 2.345031   (epoch 7)   mode=min
  policy_policy_acc: 0.313094   (epoch 7)   mode=max
  policy_top10_acc: 0.819261   (epoch 7)   mode=max
  policy_top5_acc: 0.663136   (epoch 7)   mode=max
  val_loss: 2.615676   (epoch 1)   mode=min
  val_policy_loss: 2.512537   (epoch 1)   mode=min
  val_policy_policy_acc: 0.281319   (epoch 6)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 7)   mode=max
  val_policy_top5_acc: 0.619580   (epoch 5)   mode=max
  val_value_loss: 0.201302   (epoch 6)   mode=min
  val_value_value_mse: 0.201322   (epoch 6)   mode=min
  value_loss: 0.193206   (epoch 7)   mode=min
  value_value_mse: 0.193206   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6534, 2.6118, 2.5789, 2.5147, 2.4901, 2.4514, 2.4416]
  policy_loss : [2.5507, 2.5097, 2.4779, 2.4157, 2.3918, 2.3539, 2.3450]
  policy_policy_acc : [0.2712, 0.2799, 0.2863, 0.2988, 0.3045, 0.3106, 0.3131]
  policy_top10_acc : [0.7766, 0.7859, 0.7915, 0.8054, 0.8098, 0.8173, 0.8193]
  policy_top5_acc : [0.6085, 0.6202, 0.6287, 0.6452, 0.6504, 0.6602, 0.6631]
  val_loss : [2.6157, 2.6311, 2.7393, 2.7665, 2.8654, 2.7626, 2.6768]
  val_policy_loss : [2.5125, 2.5273, 2.6354, 2.6653, 2.7631, 2.6618, 2.5757]
  val_policy_policy_acc : [0.2769, 0.2727, 0.2695, 0.2804, 0.2807, 0.2813, 0.2810]
  val_policy_top10_acc : [0.7828, 0.7750, 0.7752, 0.7798, 0.7797, 0.7826, 0.7830]
  val_policy_top5_acc : [0.6185, 0.6089, 0.6061, 0.6188, 0.6196, 0.6186, 0.6188]
  val_value_loss : [0.2062, 0.2075, 0.2075, 0.2020, 0.2043, 0.2013, 0.2019]
  val_value_value_mse : [0.2062, 0.2075, 0.2075, 0.2021, 0.2043, 0.2013, 0.2020]
  value_loss : [0.2055, 0.2043, 0.2021, 0.1981, 0.1966, 0.1950, 0.1932]
  value_value_mse : [0.2055, 0.2043, 0.2021, 0.1981, 0.1966, 0.1950, 0.1932]

================================================================================

History file: model_versions/chess_elo_model_V12_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:21:30.663661Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game15001_game16500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V11
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.370955   (epoch 10)   mode=min
  policy_loss: 2.276292   (epoch 10)   mode=min
  policy_policy_acc: 0.324984   (epoch 10)   mode=max
  policy_top10_acc: 0.835171   (epoch 10)   mode=max
  policy_top5_acc: 0.681737   (epoch 10)   mode=max
  val_loss: 2.634408   (epoch 4)   mode=min
  val_policy_loss: 2.533270   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279520   (epoch 6)   mode=max
  val_policy_top10_acc: 0.784316   (epoch 1)   mode=max
  val_policy_top5_acc: 0.621878   (epoch 3)   mode=max
  val_value_loss: 0.200750   (epoch 8)   mode=min
  val_value_value_mse: 0.200781   (epoch 8)   mode=min
  value_loss: 0.189441   (epoch 10)   mode=min
  value_value_mse: 0.189445   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6513, 2.6135, 2.5792, 2.5554, 2.5310, 2.5040, 2.4439, 2.4212, 2.3847, 2.3710]
  policy_loss : [2.5483, 2.5109, 2.4777, 2.4546, 2.4309, 2.4046, 2.3469, 2.3244, 2.2893, 2.2763]
  policy_policy_acc : [0.2705, 0.2772, 0.2840, 0.2876, 0.2947, 0.2986, 0.3115, 0.3139, 0.3211, 0.3250]
  policy_top10_acc : [0.7775, 0.7862, 0.7934, 0.7981, 0.8046, 0.8085, 0.8196, 0.8260, 0.8325, 0.8352]
  policy_top5_acc : [0.6138, 0.6219, 0.6317, 0.6383, 0.6434, 0.6503, 0.6653, 0.6702, 0.6792, 0.6817]
  val_loss : [3.1084, 2.7570, 2.6845, 2.6344, 2.6421, 2.7454, 2.8449, 3.4945, 4.6772, 4.9923]
  val_policy_loss : [3.0060, 2.6533, 2.5808, 2.5333, 2.5410, 2.6435, 2.7426, 3.3936, 4.5744, 4.8891]
  val_policy_policy_acc : [0.2741, 0.2775, 0.2768, 0.2719, 0.2770, 0.2795, 0.2760, 0.2744, 0.2758, 0.2790]
  val_policy_top10_acc : [0.7843, 0.7797, 0.7822, 0.7821, 0.7828, 0.7768, 0.7824, 0.7796, 0.7825, 0.7825]
  val_policy_top5_acc : [0.6170, 0.6164, 0.6219, 0.6177, 0.6117, 0.6142, 0.6166, 0.6186, 0.6207, 0.6211]
  val_value_loss : [0.2041, 0.2071, 0.2072, 0.2022, 0.2021, 0.2036, 0.2044, 0.2007, 0.2031, 0.2036]
  val_value_value_mse : [0.2041, 0.2071, 0.2072, 0.2022, 0.2021, 0.2036, 0.2044, 0.2008, 0.2031, 0.2036]
  value_loss : [0.2069, 0.2053, 0.2033, 0.2013, 0.2000, 0.1988, 0.1941, 0.1932, 0.1906, 0.1894]
  value_value_mse : [0.2069, 0.2053, 0.2033, 0.2014, 0.2000, 0.1988, 0.1941, 0.1933, 0.1906, 0.1894]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:28:46.704470Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game16501_game18000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.456624   (epoch 7)   mode=min
  policy_loss: 2.360121   (epoch 7)   mode=min
  policy_policy_acc: 0.310836   (epoch 7)   mode=max
  policy_top10_acc: 0.815068   (epoch 7)   mode=max
  policy_top5_acc: 0.658922   (epoch 7)   mode=max
  val_loss: 2.624626   (epoch 1)   mode=min
  val_policy_loss: 2.523073   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285514   (epoch 6)   mode=max
  val_policy_top10_acc: 0.785914   (epoch 5)   mode=max
  val_policy_top5_acc: 0.620480   (epoch 6)   mode=max
  val_value_loss: 0.201169   (epoch 7)   mode=min
  val_value_value_mse: 0.201181   (epoch 7)   mode=min
  value_loss: 0.193024   (epoch 7)   mode=min
  value_value_mse: 0.193023   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6641, 2.6210, 2.5890, 2.5284, 2.5023, 2.4698, 2.4566]
  policy_loss : [2.5617, 2.5196, 2.4885, 2.4298, 2.4046, 2.3729, 2.3601]
  policy_policy_acc : [0.2695, 0.2783, 0.2840, 0.2947, 0.3009, 0.3072, 0.3108]
  policy_top10_acc : [0.7709, 0.7813, 0.7877, 0.8012, 0.8058, 0.8127, 0.8151]
  policy_top5_acc : [0.6071, 0.6204, 0.6252, 0.6430, 0.6474, 0.6559, 0.6589]
  val_loss : [2.6246, 2.9373, 2.8650, 2.9258, 2.6863, 2.8600, 2.8165]
  val_policy_loss : [2.5231, 2.8356, 2.7627, 2.8246, 2.5845, 2.7589, 2.7157]
  val_policy_policy_acc : [0.2746, 0.2779, 0.2753, 0.2763, 0.2810, 0.2855, 0.2820]
  val_policy_top10_acc : [0.7842, 0.7810, 0.7798, 0.7840, 0.7859, 0.7855, 0.7834]
  val_policy_top5_acc : [0.6115, 0.6135, 0.6077, 0.6160, 0.6178, 0.6205, 0.6203]
  val_value_loss : [0.2030, 0.2027, 0.2042, 0.2017, 0.2031, 0.2015, 0.2012]
  val_value_value_mse : [0.2030, 0.2027, 0.2042, 0.2017, 0.2031, 0.2015, 0.2012]
  value_loss : [0.2047, 0.2030, 0.2011, 0.1974, 0.1952, 0.1938, 0.1930]
  value_value_mse : [0.2047, 0.2031, 0.2011, 0.1974, 0.1952, 0.1938, 0.1930]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:36:55.331188Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game18001_game19500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.418444   (epoch 8)   mode=min
  policy_loss: 2.322084   (epoch 8)   mode=min
  policy_policy_acc: 0.314730   (epoch 8)   mode=max
  policy_top10_acc: 0.824322   (epoch 8)   mode=max
  policy_top5_acc: 0.671493   (epoch 8)   mode=max
  val_loss: 2.620716   (epoch 2)   mode=min
  val_policy_loss: 2.516232   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282517   (epoch 8)   mode=max
  val_policy_top10_acc: 0.783317   (epoch 8)   mode=max
  val_policy_top5_acc: 0.618681   (epoch 8)   mode=max
  val_value_loss: 0.202395   (epoch 7)   mode=min
  val_value_value_mse: 0.202412   (epoch 7)   mode=min
  value_loss: 0.192805   (epoch 8)   mode=min
  value_value_mse: 0.192793   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6552, 2.6127, 2.5825, 2.5506, 2.4912, 2.4682, 2.4360, 2.4184]
  policy_loss : [2.5522, 2.5110, 2.4813, 2.4498, 2.3922, 2.3704, 2.3393, 2.3221]
  policy_policy_acc : [0.2691, 0.2777, 0.2826, 0.2916, 0.3016, 0.3049, 0.3131, 0.3147]
  policy_top10_acc : [0.7750, 0.7866, 0.7928, 0.7992, 0.8117, 0.8159, 0.8220, 0.8243]
  policy_top5_acc : [0.6115, 0.6242, 0.6322, 0.6417, 0.6559, 0.6608, 0.6693, 0.6715]
  val_loss : [2.6244, 2.6207, 2.6714, 2.8437, 2.7452, 2.8312, 3.1269, 3.0411]
  val_policy_loss : [2.5215, 2.5162, 2.5693, 2.7400, 2.6424, 2.7296, 3.0253, 2.9390]
  val_policy_policy_acc : [0.2733, 0.2771, 0.2708, 0.2741, 0.2763, 0.2782, 0.2807, 0.2825]
  val_policy_top10_acc : [0.7735, 0.7809, 0.7744, 0.7766, 0.7818, 0.7805, 0.7812, 0.7833]
  val_policy_top5_acc : [0.6147, 0.6152, 0.6033, 0.6154, 0.6167, 0.6179, 0.6164, 0.6187]
  val_value_loss : [0.2057, 0.2088, 0.2039, 0.2069, 0.2052, 0.2028, 0.2024, 0.2035]
  val_value_value_mse : [0.2057, 0.2088, 0.2040, 0.2069, 0.2053, 0.2028, 0.2024, 0.2035]
  value_loss : [0.2063, 0.2036, 0.2025, 0.2012, 0.1983, 0.1957, 0.1937, 0.1928]
  value_value_mse : [0.2063, 0.2035, 0.2025, 0.2012, 0.1983, 0.1956, 0.1937, 0.1928]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:44:24.177673Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game19501_game21000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.460489   (epoch 7)   mode=min
  policy_loss: 2.359093   (epoch 7)   mode=min
  policy_policy_acc: 0.310680   (epoch 7)   mode=max
  policy_top10_acc: 0.816479   (epoch 7)   mode=max
  policy_top5_acc: 0.661631   (epoch 7)   mode=max
  val_loss: 2.613899   (epoch 1)   mode=min
  val_policy_loss: 2.509087   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285514   (epoch 5)   mode=max
  val_policy_top10_acc: 0.784216   (epoch 6)   mode=max
  val_policy_top5_acc: 0.620579   (epoch 6)   mode=max
  val_value_loss: 0.200483   (epoch 7)   mode=min
  val_value_value_mse: 0.200506   (epoch 7)   mode=min
  value_loss: 0.202209   (epoch 7)   mode=min
  value_value_mse: 0.202193   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6606, 2.6224, 2.5903, 2.5300, 2.5058, 2.4720, 2.4605]
  policy_loss : [2.5526, 2.5159, 2.4844, 2.4264, 2.4031, 2.3703, 2.3591]
  policy_policy_acc : [0.2700, 0.2786, 0.2837, 0.2962, 0.2994, 0.3060, 0.3107]
  policy_top10_acc : [0.7743, 0.7823, 0.7890, 0.8015, 0.8073, 0.8152, 0.8165]
  policy_top5_acc : [0.6089, 0.6190, 0.6284, 0.6432, 0.6499, 0.6572, 0.6616]
  val_loss : [2.6139, 2.6476, 2.6982, 2.6971, 2.6762, 2.7306, 2.6673]
  val_policy_loss : [2.5091, 2.5424, 2.5960, 2.5943, 2.5725, 2.6292, 2.5669]
  val_policy_policy_acc : [0.2770, 0.2732, 0.2777, 0.2817, 0.2855, 0.2843, 0.2845]
  val_policy_top10_acc : [0.7799, 0.7709, 0.7753, 0.7805, 0.7819, 0.7842, 0.7791]
  val_policy_top5_acc : [0.6142, 0.6103, 0.6135, 0.6192, 0.6189, 0.6206, 0.6183]
  val_value_loss : [0.2095, 0.2102, 0.2042, 0.2053, 0.2072, 0.2025, 0.2005]
  val_value_value_mse : [0.2095, 0.2103, 0.2042, 0.2053, 0.2072, 0.2025, 0.2005]
  value_loss : [0.2157, 0.2123, 0.2116, 0.2072, 0.2051, 0.2030, 0.2022]
  value_value_mse : [0.2157, 0.2124, 0.2116, 0.2072, 0.2051, 0.2030, 0.2022]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T22:52:51.652939Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game21001_game22500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441836   (epoch 8)   mode=min
  policy_loss: 2.347995   (epoch 8)   mode=min
  policy_policy_acc: 0.308949   (epoch 8)   mode=max
  policy_top10_acc: 0.821044   (epoch 8)   mode=max
  policy_top5_acc: 0.664171   (epoch 8)   mode=max
  val_loss: 3.514328   (epoch 2)   mode=min
  val_policy_loss: 3.409345   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285614   (epoch 7)   mode=max
  val_policy_top10_acc: 0.780420   (epoch 7)   mode=max
  val_policy_top5_acc: 0.622577   (epoch 8)   mode=max
  val_value_loss: 0.201614   (epoch 7)   mode=min
  val_value_value_mse: 0.201612   (epoch 7)   mode=min
  value_loss: 0.187628   (epoch 8)   mode=min
  value_value_mse: 0.187627   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6669, 2.6262, 2.5971, 2.5715, 2.5097, 2.4890, 2.4557, 2.4418]
  policy_loss : [2.5660, 2.5266, 2.4983, 2.4736, 2.4135, 2.3937, 2.3618, 2.3480]
  policy_policy_acc : [0.2661, 0.2742, 0.2800, 0.2847, 0.2965, 0.3005, 0.3064, 0.3089]
  policy_top10_acc : [0.7733, 0.7811, 0.7908, 0.7946, 0.8072, 0.8104, 0.8166, 0.8210]
  policy_top5_acc : [0.6075, 0.6177, 0.6264, 0.6327, 0.6484, 0.6539, 0.6605, 0.6642]
  val_loss : [5.4738, 3.5143, 3.6565, 4.2498, 5.2631, 5.3033, 5.0072, 4.7156]
  val_policy_loss : [5.3698, 3.4093, 3.5514, 4.1443, 5.1597, 5.1998, 4.9050, 4.6126]
  val_policy_policy_acc : [0.2783, 0.2731, 0.2713, 0.2704, 0.2807, 0.2818, 0.2856, 0.2820]
  val_policy_top10_acc : [0.7776, 0.7730, 0.7727, 0.7770, 0.7782, 0.7795, 0.7804, 0.7797]
  val_policy_top5_acc : [0.6130, 0.6090, 0.6122, 0.6119, 0.6190, 0.6174, 0.6224, 0.6226]
  val_value_loss : [0.2044, 0.2088, 0.2090, 0.2090, 0.2036, 0.2038, 0.2016, 0.2035]
  val_value_value_mse : [0.2044, 0.2088, 0.2090, 0.2090, 0.2036, 0.2037, 0.2016, 0.2035]
  value_loss : [0.2019, 0.1994, 0.1976, 0.1960, 0.1925, 0.1904, 0.1878, 0.1876]
  value_value_mse : [0.2019, 0.1994, 0.1976, 0.1960, 0.1925, 0.1904, 0.1878, 0.1876]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:02:02.236954Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game22501_game24000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.415844   (epoch 9)   mode=min
  policy_loss: 2.323187   (epoch 9)   mode=min
  policy_policy_acc: 0.314887   (epoch 9)   mode=max
  policy_top10_acc: 0.822872   (epoch 9)   mode=max
  policy_top5_acc: 0.669359   (epoch 9)   mode=max
  val_loss: 2.620151   (epoch 3)   mode=min
  val_policy_loss: 2.517941   (epoch 3)   mode=min
  val_policy_policy_acc: 0.281319   (epoch 6)   mode=max
  val_policy_top10_acc: 0.783516   (epoch 6)   mode=max
  val_policy_top5_acc: 0.620879   (epoch 7)   mode=max
  val_value_loss: 0.203202   (epoch 7)   mode=min
  val_value_value_mse: 0.203220   (epoch 7)   mode=min
  value_loss: 0.185317   (epoch 9)   mode=min
  value_value_mse: 0.185311   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6674, 2.6264, 2.5975, 2.5675, 2.5488, 2.4857, 2.4654, 2.4316, 2.4158]
  policy_loss : [2.5668, 2.5272, 2.4990, 2.4700, 2.4514, 2.3906, 2.3710, 2.3381, 2.3232]
  policy_policy_acc : [0.2667, 0.2750, 0.2799, 0.2841, 0.2899, 0.3026, 0.3047, 0.3109, 0.3149]
  policy_top10_acc : [0.7708, 0.7795, 0.7866, 0.7935, 0.7962, 0.8085, 0.8135, 0.8192, 0.8229]
  policy_top5_acc : [0.6034, 0.6182, 0.6246, 0.6330, 0.6372, 0.6523, 0.6586, 0.6661, 0.6694]
  val_loss : [2.7360, 2.9825, 2.6202, 2.9117, 2.7228, 2.8711, 3.0166, 2.9524, 2.7483]
  val_policy_loss : [2.6339, 2.8797, 2.5179, 2.8088, 2.6196, 2.7677, 2.9147, 2.8502, 2.6451]
  val_policy_policy_acc : [0.2764, 0.2797, 0.2794, 0.2760, 0.2745, 0.2813, 0.2756, 0.2790, 0.2787]
  val_policy_top10_acc : [0.7785, 0.7767, 0.7783, 0.7734, 0.7775, 0.7835, 0.7777, 0.7806, 0.7809]
  val_policy_top5_acc : [0.6170, 0.6153, 0.6160, 0.6096, 0.6102, 0.6202, 0.6209, 0.6178, 0.6190]
  val_value_loss : [0.2040, 0.2051, 0.2043, 0.2054, 0.2062, 0.2064, 0.2032, 0.2038, 0.2061]
  val_value_value_mse : [0.2040, 0.2051, 0.2043, 0.2054, 0.2062, 0.2064, 0.2032, 0.2038, 0.2061]
  value_loss : [0.2012, 0.1983, 0.1971, 0.1949, 0.1949, 0.1901, 0.1888, 0.1871, 0.1853]
  value_value_mse : [0.2012, 0.1983, 0.1971, 0.1949, 0.1949, 0.1901, 0.1888, 0.1871, 0.1853]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:09:16.739863Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game24001_game25500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451562   (epoch 7)   mode=min
  policy_loss: 2.357487   (epoch 7)   mode=min
  policy_policy_acc: 0.309093   (epoch 7)   mode=max
  policy_top10_acc: 0.817217   (epoch 7)   mode=max
  policy_top5_acc: 0.661443   (epoch 7)   mode=max
  val_loss: 2.622535   (epoch 1)   mode=min
  val_policy_loss: 2.521098   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285814   (epoch 5)   mode=max
  val_policy_top10_acc: 0.781119   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618581   (epoch 5)   mode=max
  val_value_loss: 0.197707   (epoch 7)   mode=min
  val_value_value_mse: 0.197728   (epoch 7)   mode=min
  value_loss: 0.188144   (epoch 7)   mode=min
  value_value_mse: 0.188143   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6614, 2.6165, 2.5890, 2.5241, 2.5023, 2.4639, 2.4516]
  policy_loss : [2.5608, 2.5168, 2.4901, 2.4270, 2.4063, 2.3695, 2.3575]
  policy_policy_acc : [0.2683, 0.2771, 0.2823, 0.2968, 0.3000, 0.3075, 0.3091]
  policy_top10_acc : [0.7746, 0.7845, 0.7909, 0.8030, 0.8084, 0.8161, 0.8172]
  policy_top5_acc : [0.6079, 0.6216, 0.6289, 0.6446, 0.6506, 0.6599, 0.6614]
  val_loss : [2.6225, 2.6419, 2.6597, 2.6794, 2.8865, 2.7687, 2.6948]
  val_policy_loss : [2.5211, 2.5380, 2.5535, 2.5787, 2.7869, 2.6694, 2.5958]
  val_policy_policy_acc : [0.2752, 0.2730, 0.2780, 0.2820, 0.2858, 0.2832, 0.2847]
  val_policy_top10_acc : [0.7787, 0.7710, 0.7730, 0.7752, 0.7774, 0.7789, 0.7811]
  val_policy_top5_acc : [0.6123, 0.6080, 0.6138, 0.6144, 0.6186, 0.6155, 0.6180]
  val_value_loss : [0.2028, 0.2077, 0.2122, 0.2011, 0.1985, 0.1981, 0.1977]
  val_value_value_mse : [0.2028, 0.2077, 0.2122, 0.2011, 0.1985, 0.1981, 0.1977]
  value_loss : [0.2013, 0.1993, 0.1976, 0.1943, 0.1923, 0.1889, 0.1881]
  value_value_mse : [0.2013, 0.1993, 0.1976, 0.1943, 0.1923, 0.1889, 0.1881]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:18:36.459965Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game25501_game27000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.393124   (epoch 9)   mode=min
  policy_loss: 2.300374   (epoch 9)   mode=min
  policy_policy_acc: 0.319690   (epoch 9)   mode=max
  policy_top10_acc: 0.827771   (epoch 9)   mode=max
  policy_top5_acc: 0.674913   (epoch 9)   mode=max
  val_loss: 2.602330   (epoch 3)   mode=min
  val_policy_loss: 2.501006   (epoch 3)   mode=min
  val_policy_policy_acc: 0.284316   (epoch 6)   mode=max
  val_policy_top10_acc: 0.782717   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620879   (epoch 6)   mode=max
  val_value_loss: 0.199410   (epoch 8)   mode=min
  val_value_value_mse: 0.199416   (epoch 8)   mode=min
  value_loss: 0.184803   (epoch 9)   mode=min
  value_value_mse: 0.184783   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6503, 2.6091, 2.5823, 2.5529, 2.5281, 2.4653, 2.4434, 2.4088, 2.3931]
  policy_loss : [2.5498, 2.5101, 2.4840, 2.4555, 2.4318, 2.3712, 2.3500, 2.3163, 2.3004]
  policy_policy_acc : [0.2721, 0.2780, 0.2853, 0.2899, 0.2934, 0.3073, 0.3098, 0.3174, 0.3197]
  policy_top10_acc : [0.7732, 0.7830, 0.7897, 0.7969, 0.8008, 0.8134, 0.8176, 0.8237, 0.8278]
  policy_top5_acc : [0.6102, 0.6212, 0.6284, 0.6357, 0.6408, 0.6572, 0.6627, 0.6708, 0.6749]
  val_loss : [2.6117, 2.6178, 2.6023, 2.6344, 2.6381, 2.6284, 2.6248, 2.6358, 2.6810]
  val_policy_loss : [2.5088, 2.5150, 2.5010, 2.5307, 2.5354, 2.5276, 2.5245, 2.5360, 2.5805]
  val_policy_policy_acc : [0.2758, 0.2768, 0.2796, 0.2756, 0.2753, 0.2843, 0.2789, 0.2813, 0.2792]
  val_policy_top10_acc : [0.7780, 0.7788, 0.7780, 0.7796, 0.7752, 0.7819, 0.7827, 0.7823, 0.7822]
  val_policy_top5_acc : [0.6129, 0.6106, 0.6179, 0.6146, 0.6097, 0.6209, 0.6205, 0.6194, 0.6191]
  val_value_loss : [0.2056, 0.2054, 0.2026, 0.2075, 0.2052, 0.2014, 0.2005, 0.1994, 0.2007]
  val_value_value_mse : [0.2056, 0.2054, 0.2026, 0.2075, 0.2052, 0.2014, 0.2005, 0.1994, 0.2007]
  value_loss : [0.2006, 0.1983, 0.1964, 0.1945, 0.1929, 0.1886, 0.1872, 0.1856, 0.1848]
  value_value_mse : [0.2006, 0.1983, 0.1964, 0.1945, 0.1929, 0.1886, 0.1872, 0.1856, 0.1848]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:26:59.309292Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game27001_game28500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.430177   (epoch 8)   mode=min
  policy_loss: 2.335655   (epoch 8)   mode=min
  policy_policy_acc: 0.311370   (epoch 8)   mode=max
  policy_top10_acc: 0.823886   (epoch 8)   mode=max
  policy_top5_acc: 0.665602   (epoch 8)   mode=max
  val_loss: 2.623380   (epoch 2)   mode=min
  val_policy_loss: 2.521462   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282617   (epoch 7)   mode=max
  val_policy_top10_acc: 0.781718   (epoch 8)   mode=max
  val_policy_top5_acc: 0.621079   (epoch 7)   mode=max
  val_value_loss: 0.198830   (epoch 6)   mode=min
  val_value_value_mse: 0.198831   (epoch 6)   mode=min
  value_loss: 0.188942   (epoch 8)   mode=min
  value_value_mse: 0.188942   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6613, 2.6219, 2.5913, 2.5623, 2.5056, 2.4793, 2.4461, 2.4302]
  policy_loss : [2.5589, 2.5213, 2.4912, 2.4631, 2.4082, 2.3836, 2.3511, 2.3357]
  policy_policy_acc : [0.2678, 0.2744, 0.2806, 0.2850, 0.2975, 0.3018, 0.3077, 0.3114]
  policy_top10_acc : [0.7735, 0.7833, 0.7902, 0.7973, 0.8083, 0.8142, 0.8201, 0.8239]
  policy_top5_acc : [0.6064, 0.6185, 0.6261, 0.6338, 0.6481, 0.6549, 0.6620, 0.6656]
  val_loss : [2.6516, 2.6234, 2.6308, 2.7336, 2.7005, 2.7163, 2.7230, 2.6772]
  val_policy_loss : [2.5506, 2.5215, 2.5297, 2.6328, 2.6005, 2.6167, 2.6229, 2.5777]
  val_policy_policy_acc : [0.2775, 0.2729, 0.2782, 0.2707, 0.2769, 0.2803, 0.2826, 0.2789]
  val_policy_top10_acc : [0.7792, 0.7803, 0.7771, 0.7745, 0.7792, 0.7795, 0.7812, 0.7817]
  val_policy_top5_acc : [0.6156, 0.6132, 0.6193, 0.6148, 0.6159, 0.6196, 0.6211, 0.6176]
  val_value_loss : [0.2019, 0.2037, 0.2019, 0.2013, 0.1997, 0.1988, 0.2000, 0.1989]
  val_value_value_mse : [0.2019, 0.2037, 0.2019, 0.2014, 0.1997, 0.1988, 0.2000, 0.1989]
  value_loss : [0.2048, 0.2014, 0.2004, 0.1983, 0.1946, 0.1913, 0.1900, 0.1889]
  value_value_mse : [0.2048, 0.2014, 0.2004, 0.1983, 0.1946, 0.1913, 0.1900, 0.1889]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:37:23.179121Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game28501_game30000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.434562   (epoch 10)   mode=min
  policy_loss: 2.338655   (epoch 10)   mode=min
  policy_policy_acc: 0.311513   (epoch 10)   mode=max
  policy_top10_acc: 0.821426   (epoch 10)   mode=max
  policy_top5_acc: 0.664218   (epoch 10)   mode=max
  val_loss: 2.689195   (epoch 7)   mode=min
  val_policy_loss: 2.590021   (epoch 7)   mode=min
  val_policy_policy_acc: 0.282817   (epoch 5)   mode=max
  val_policy_top10_acc: 0.784516   (epoch 7)   mode=max
  val_policy_top5_acc: 0.623576   (epoch 8)   mode=max
  val_value_loss: 0.197318   (epoch 9)   mode=min
  val_value_value_mse: 0.197337   (epoch 9)   mode=min
  value_loss: 0.191766   (epoch 10)   mode=min
  value_value_mse: 0.191758   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.6797, 2.6345, 2.6087, 2.5457, 2.5221, 2.4851, 2.4718, 2.4615, 2.4533, 2.4346]
  policy_loss : [2.5762, 2.5319, 2.5069, 2.4464, 2.4242, 2.3877, 2.3751, 2.3650, 2.3567, 2.3387]
  policy_policy_acc : [0.2633, 0.2731, 0.2787, 0.2906, 0.2949, 0.3020, 0.3050, 0.3072, 0.3067, 0.3115]
  policy_top10_acc : [0.7704, 0.7800, 0.7873, 0.8002, 0.8052, 0.8114, 0.8139, 0.8177, 0.8182, 0.8214]
  policy_top5_acc : [0.6027, 0.6138, 0.6214, 0.6375, 0.6422, 0.6527, 0.6566, 0.6589, 0.6597, 0.6642]
  val_loss : [2.6959, 2.7463, 2.8874, 2.8190, 2.9237, 2.7130, 2.6892, 2.9757, 2.8675, 2.7005]
  val_policy_loss : [2.5944, 2.6459, 2.7867, 2.7191, 2.8241, 2.6139, 2.5900, 2.8764, 2.7686, 2.6016]
  val_policy_policy_acc : [0.2761, 0.2799, 0.2787, 0.2776, 0.2828, 0.2798, 0.2812, 0.2802, 0.2814, 0.2821]
  val_policy_top10_acc : [0.7781, 0.7813, 0.7786, 0.7838, 0.7826, 0.7834, 0.7845, 0.7810, 0.7836, 0.7828]
  val_policy_top5_acc : [0.6116, 0.6192, 0.6156, 0.6208, 0.6208, 0.6235, 0.6234, 0.6236, 0.6232, 0.6208]
  val_value_loss : [0.2028, 0.2005, 0.2010, 0.1996, 0.1988, 0.1979, 0.1981, 0.1981, 0.1973, 0.1976]
  val_value_value_mse : [0.2028, 0.2005, 0.2011, 0.1996, 0.1988, 0.1979, 0.1982, 0.1981, 0.1973, 0.1976]
  value_loss : [0.2070, 0.2053, 0.2034, 0.1986, 0.1960, 0.1947, 0.1935, 0.1927, 0.1933, 0.1918]
  value_value_mse : [0.2070, 0.2053, 0.2034, 0.1986, 0.1960, 0.1947, 0.1935, 0.1927, 0.1933, 0.1918]

================================================================================

History file: model_versions/chess_elo_model_V13_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:44:31.485932Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game30001_game31500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V12
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.457382   (epoch 7)   mode=min
  policy_loss: 2.361846   (epoch 7)   mode=min
  policy_policy_acc: 0.307693   (epoch 7)   mode=max
  policy_top10_acc: 0.818238   (epoch 7)   mode=max
  policy_top5_acc: 0.663470   (epoch 7)   mode=max
  val_loss: 2.614895   (epoch 1)   mode=min
  val_policy_loss: 2.513574   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284715   (epoch 6)   mode=max
  val_policy_top10_acc: 0.784815   (epoch 7)   mode=max
  val_policy_top5_acc: 0.626374   (epoch 7)   mode=max
  val_value_loss: 0.198070   (epoch 6)   mode=min
  val_value_value_mse: 0.198089   (epoch 6)   mode=min
  value_loss: 0.191220   (epoch 7)   mode=min
  value_value_mse: 0.191208   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6664, 2.6252, 2.5950, 2.5313, 2.5060, 2.4704, 2.4574]
  policy_loss : [2.5639, 2.5240, 2.4948, 2.4331, 2.4086, 2.3744, 2.3618]
  policy_policy_acc : [0.2674, 0.2746, 0.2806, 0.2947, 0.2971, 0.3053, 0.3077]
  policy_top10_acc : [0.7744, 0.7848, 0.7911, 0.8048, 0.8093, 0.8158, 0.8182]
  policy_top5_acc : [0.6100, 0.6201, 0.6286, 0.6432, 0.6508, 0.6590, 0.6635]
  val_loss : [2.6149, 2.6276, 2.6166, 2.7083, 2.8386, 2.6609, 2.8401]
  val_policy_loss : [2.5136, 2.5253, 2.5169, 2.6082, 2.7390, 2.5618, 2.7403]
  val_policy_policy_acc : [0.2781, 0.2744, 0.2796, 0.2795, 0.2789, 0.2847, 0.2832]
  val_policy_top10_acc : [0.7802, 0.7829, 0.7795, 0.7838, 0.7806, 0.7807, 0.7848]
  val_policy_top5_acc : [0.6155, 0.6172, 0.6132, 0.6218, 0.6180, 0.6237, 0.6264]
  val_value_loss : [0.2027, 0.2048, 0.1993, 0.2002, 0.1990, 0.1981, 0.1992]
  val_value_value_mse : [0.2027, 0.2048, 0.1993, 0.2002, 0.1990, 0.1981, 0.1992]
  value_loss : [0.2053, 0.2028, 0.2007, 0.1966, 0.1947, 0.1920, 0.1912]
  value_value_mse : [0.2053, 0.2028, 0.2006, 0.1966, 0.1947, 0.1920, 0.1912]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-16T23:54:20.413033Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game31501_game33000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.388925   (epoch 10)   mode=min
  policy_loss: 2.291960   (epoch 10)   mode=min
  policy_policy_acc: 0.319665   (epoch 10)   mode=max
  policy_top10_acc: 0.830475   (epoch 10)   mode=max
  policy_top5_acc: 0.677696   (epoch 10)   mode=max
  val_loss: 2.625846   (epoch 4)   mode=min
  val_policy_loss: 2.523352   (epoch 4)   mode=min
  val_policy_policy_acc: 0.281219   (epoch 9)   mode=max
  val_policy_top10_acc: 0.781319   (epoch 4)   mode=max
  val_policy_top5_acc: 0.621379   (epoch 8)   mode=max
  val_value_loss: 0.198194   (epoch 8)   mode=min
  val_value_value_mse: 0.198220   (epoch 8)   mode=min
  value_loss: 0.193946   (epoch 10)   mode=min
  value_value_mse: 0.193948   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6701, 2.6250, 2.5906, 2.5689, 2.5432, 2.5178, 2.4602, 2.4364, 2.4019, 2.3889]
  policy_loss : [2.5649, 2.5206, 2.4877, 2.4658, 2.4410, 2.4167, 2.3608, 2.3379, 2.3043, 2.2920]
  policy_policy_acc : [0.2662, 0.2752, 0.2807, 0.2856, 0.2899, 0.2938, 0.3058, 0.3103, 0.3171, 0.3197]
  policy_top10_acc : [0.7732, 0.7824, 0.7911, 0.7950, 0.8014, 0.8068, 0.8161, 0.8232, 0.8294, 0.8305]
  policy_top5_acc : [0.6067, 0.6160, 0.6268, 0.6334, 0.6396, 0.6462, 0.6584, 0.6664, 0.6741, 0.6777]
  val_loss : [3.0125, 2.7497, 2.6297, 2.6258, 2.6525, 2.8489, 2.7264, 2.7672, 2.9577, 2.8954]
  val_policy_loss : [2.9123, 2.6471, 2.5281, 2.5234, 2.5510, 2.7477, 2.6267, 2.6679, 2.8579, 2.7952]
  val_policy_policy_acc : [0.2730, 0.2760, 0.2730, 0.2731, 0.2737, 0.2719, 0.2761, 0.2772, 0.2812, 0.2786]
  val_policy_top10_acc : [0.7743, 0.7779, 0.7776, 0.7813, 0.7760, 0.7750, 0.7762, 0.7803, 0.7780, 0.7761]
  val_policy_top5_acc : [0.6113, 0.6162, 0.6144, 0.6158, 0.6134, 0.6096, 0.6140, 0.6214, 0.6172, 0.6151]
  val_value_loss : [0.1999, 0.2052, 0.2030, 0.2048, 0.2028, 0.2020, 0.1991, 0.1982, 0.1990, 0.1998]
  val_value_value_mse : [0.1999, 0.2052, 0.2030, 0.2048, 0.2028, 0.2020, 0.1991, 0.1982, 0.1990, 0.1998]
  value_loss : [0.2108, 0.2087, 0.2063, 0.2063, 0.2046, 0.2021, 0.1986, 0.1970, 0.1949, 0.1939]
  value_value_mse : [0.2108, 0.2087, 0.2063, 0.2063, 0.2046, 0.2021, 0.1986, 0.1970, 0.1949, 0.1939]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:04:41.648331Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game33001_game34500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.417891   (epoch 10)   mode=min
  policy_loss: 2.323679   (epoch 10)   mode=min
  policy_policy_acc: 0.316022   (epoch 10)   mode=max
  policy_top10_acc: 0.823081   (epoch 10)   mode=max
  policy_top5_acc: 0.668947   (epoch 10)   mode=max
  val_loss: 2.862075   (epoch 4)   mode=min
  val_policy_loss: 2.763473   (epoch 4)   mode=min
  val_policy_policy_acc: 0.288511   (epoch 9)   mode=max
  val_policy_top10_acc: 0.787912   (epoch 7)   mode=max
  val_policy_top5_acc: 0.631568   (epoch 9)   mode=max
  val_value_loss: 0.196418   (epoch 8)   mode=min
  val_value_value_mse: 0.196425   (epoch 8)   mode=min
  value_loss: 0.188180   (epoch 9)   mode=min
  value_value_mse: 0.188182   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6708, 2.6268, 2.5949, 2.5373, 2.5121, 2.4969, 2.4576, 2.4457, 2.4254, 2.4179]
  policy_loss : [2.5688, 2.5262, 2.4951, 2.4395, 2.4159, 2.4011, 2.3628, 2.3508, 2.3313, 2.3237]
  policy_policy_acc : [0.2657, 0.2753, 0.2824, 0.2920, 0.2987, 0.2994, 0.3079, 0.3109, 0.3142, 0.3160]
  policy_top10_acc : [0.7710, 0.7819, 0.7880, 0.7993, 0.8028, 0.8072, 0.8156, 0.8166, 0.8203, 0.8231]
  policy_top5_acc : [0.6064, 0.6173, 0.6258, 0.6400, 0.6451, 0.6529, 0.6614, 0.6619, 0.6675, 0.6689]
  val_loss : [3.0050, 3.1584, 4.1353, 2.8621, 2.9488, 2.9428, 4.0096, 4.3354, 3.7157, 4.4726]
  val_policy_loss : [2.9012, 3.0570, 4.0329, 2.7635, 2.8489, 2.8437, 3.9091, 4.2361, 3.6168, 4.3730]
  val_policy_policy_acc : [0.2775, 0.2768, 0.2759, 0.2822, 0.2827, 0.2828, 0.2835, 0.2853, 0.2885, 0.2838]
  val_policy_top10_acc : [0.7803, 0.7811, 0.7818, 0.7857, 0.7860, 0.7833, 0.7879, 0.7871, 0.7876, 0.7853]
  val_policy_top5_acc : [0.6199, 0.6176, 0.6253, 0.6245, 0.6269, 0.6236, 0.6272, 0.6299, 0.6316, 0.6292]
  val_value_loss : [0.2070, 0.2020, 0.2030, 0.1968, 0.1992, 0.1978, 0.1993, 0.1964, 0.1965, 0.1971]
  val_value_value_mse : [0.2070, 0.2020, 0.2030, 0.1968, 0.1992, 0.1978, 0.1993, 0.1964, 0.1965, 0.1971]
  value_loss : [0.2040, 0.2011, 0.1995, 0.1955, 0.1924, 0.1916, 0.1896, 0.1899, 0.1882, 0.1884]
  value_value_mse : [0.2040, 0.2011, 0.1995, 0.1955, 0.1924, 0.1916, 0.1896, 0.1899, 0.1882, 0.1884]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:12:47.175647Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game34501_game36000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.421121   (epoch 8)   mode=min
  policy_loss: 2.324172   (epoch 8)   mode=min
  policy_policy_acc: 0.315795   (epoch 8)   mode=max
  policy_top10_acc: 0.824493   (epoch 8)   mode=max
  policy_top5_acc: 0.670932   (epoch 8)   mode=max
  val_loss: 2.623731   (epoch 2)   mode=min
  val_policy_loss: 2.520990   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284715   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624675   (epoch 7)   mode=max
  val_value_loss: 0.199699   (epoch 7)   mode=min
  val_value_value_mse: 0.199712   (epoch 7)   mode=min
  value_loss: 0.193861   (epoch 8)   mode=min
  value_value_mse: 0.193864   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6591, 2.6138, 2.5841, 2.5567, 2.4953, 2.4726, 2.4374, 2.4211]
  policy_loss : [2.5554, 2.5110, 2.4819, 2.4556, 2.3963, 2.3746, 2.3402, 2.3242]
  policy_policy_acc : [0.2694, 0.2776, 0.2830, 0.2894, 0.3016, 0.3046, 0.3124, 0.3158]
  policy_top10_acc : [0.7773, 0.7867, 0.7933, 0.7994, 0.8116, 0.8167, 0.8219, 0.8245]
  policy_top5_acc : [0.6120, 0.6230, 0.6318, 0.6375, 0.6547, 0.6582, 0.6669, 0.6709]
  val_loss : [2.6679, 2.6237, 2.7908, 2.6847, 2.6424, 2.8852, 2.8458, 2.8120]
  val_policy_loss : [2.5649, 2.5210, 2.6900, 2.5820, 2.5415, 2.7846, 2.7457, 2.7118]
  val_policy_policy_acc : [0.2742, 0.2696, 0.2795, 0.2735, 0.2797, 0.2811, 0.2847, 0.2819]
  val_policy_top10_acc : [0.7832, 0.7781, 0.7818, 0.7751, 0.7790, 0.7818, 0.7802, 0.7791]
  val_policy_top5_acc : [0.6156, 0.6126, 0.6206, 0.6149, 0.6174, 0.6180, 0.6247, 0.6234]
  val_value_loss : [0.2058, 0.2055, 0.2014, 0.2053, 0.2017, 0.2007, 0.1997, 0.2000]
  val_value_value_mse : [0.2058, 0.2055, 0.2014, 0.2053, 0.2017, 0.2007, 0.1997, 0.2000]
  value_loss : [0.2074, 0.2058, 0.2045, 0.2022, 0.1980, 0.1961, 0.1946, 0.1939]
  value_value_mse : [0.2074, 0.2058, 0.2045, 0.2022, 0.1980, 0.1961, 0.1946, 0.1939]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:21:04.879772Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game36001_game37500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.435513   (epoch 8)   mode=min
  policy_loss: 2.339694   (epoch 8)   mode=min
  policy_policy_acc: 0.312049   (epoch 8)   mode=max
  policy_top10_acc: 0.820972   (epoch 8)   mode=max
  policy_top5_acc: 0.664531   (epoch 8)   mode=max
  val_loss: 2.609669   (epoch 2)   mode=min
  val_policy_loss: 2.510639   (epoch 2)   mode=min
  val_policy_policy_acc: 0.281019   (epoch 7)   mode=max
  val_policy_top10_acc: 0.786114   (epoch 7)   mode=max
  val_policy_top5_acc: 0.625275   (epoch 7)   mode=max
  val_value_loss: 0.197936   (epoch 2)   mode=min
  val_value_value_mse: 0.197948   (epoch 2)   mode=min
  value_loss: 0.191790   (epoch 8)   mode=min
  value_value_mse: 0.191791   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6693, 2.6317, 2.5985, 2.5707, 2.5083, 2.4821, 2.4483, 2.4355]
  policy_loss : [2.5657, 2.5289, 2.4967, 2.4698, 2.4098, 2.3846, 2.3518, 2.3397]
  policy_policy_acc : [0.2654, 0.2750, 0.2797, 0.2863, 0.2968, 0.3033, 0.3092, 0.3120]
  policy_top10_acc : [0.7726, 0.7803, 0.7879, 0.7943, 0.8064, 0.8121, 0.8184, 0.8210]
  policy_top5_acc : [0.6060, 0.6165, 0.6257, 0.6318, 0.6492, 0.6543, 0.6625, 0.6645]
  val_loss : [2.7099, 2.6097, 2.6219, 2.6288, 2.7127, 2.7591, 2.8186, 2.7818]
  val_policy_loss : [2.6068, 2.5106, 2.5195, 2.5281, 2.6131, 2.6586, 2.7174, 2.6821]
  val_policy_policy_acc : [0.2705, 0.2747, 0.2767, 0.2697, 0.2776, 0.2797, 0.2810, 0.2782]
  val_policy_top10_acc : [0.7745, 0.7806, 0.7800, 0.7770, 0.7812, 0.7808, 0.7861, 0.7833]
  val_policy_top5_acc : [0.6131, 0.6156, 0.6161, 0.6118, 0.6193, 0.6217, 0.6253, 0.6219]
  val_value_loss : [0.2060, 0.1979, 0.2046, 0.2013, 0.1992, 0.2007, 0.2020, 0.1990]
  val_value_value_mse : [0.2060, 0.1979, 0.2046, 0.2014, 0.1992, 0.2007, 0.2020, 0.1991]
  value_loss : [0.2070, 0.2056, 0.2034, 0.2018, 0.1971, 0.1953, 0.1931, 0.1918]
  value_value_mse : [0.2070, 0.2056, 0.2034, 0.2018, 0.1971, 0.1953, 0.1931, 0.1918]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:28:13.942041Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game37501_game39000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.447856   (epoch 7)   mode=min
  policy_loss: 2.353372   (epoch 7)   mode=min
  policy_policy_acc: 0.310803   (epoch 7)   mode=max
  policy_top10_acc: 0.816310   (epoch 7)   mode=max
  policy_top5_acc: 0.663563   (epoch 7)   mode=max
  val_loss: 2.615434   (epoch 1)   mode=min
  val_policy_loss: 2.514338   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280819   (epoch 4)   mode=max
  val_policy_top10_acc: 0.779820   (epoch 6)   mode=max
  val_policy_top5_acc: 0.619181   (epoch 4)   mode=max
  val_value_loss: 0.198235   (epoch 4)   mode=min
  val_value_value_mse: 0.198248   (epoch 4)   mode=min
  value_loss: 0.188977   (epoch 7)   mode=min
  value_value_mse: 0.188977   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6541, 2.6142, 2.5810, 2.5232, 2.4992, 2.4634, 2.4479]
  policy_loss : [2.5513, 2.5132, 2.4817, 2.4256, 2.4030, 2.3681, 2.3534]
  policy_policy_acc : [0.2712, 0.2790, 0.2841, 0.2945, 0.3023, 0.3083, 0.3108]
  policy_top10_acc : [0.7745, 0.7840, 0.7897, 0.8022, 0.8078, 0.8140, 0.8163]
  policy_top5_acc : [0.6120, 0.6230, 0.6314, 0.6454, 0.6521, 0.6599, 0.6636]
  val_loss : [2.6154, 2.6316, 2.7448, 2.7586, 4.0615, 4.2714, 3.2244]
  val_policy_loss : [2.5143, 2.5304, 2.6416, 2.6593, 3.9600, 4.1710, 3.1245]
  val_policy_policy_acc : [0.2743, 0.2711, 0.2758, 0.2808, 0.2792, 0.2794, 0.2805]
  val_policy_top10_acc : [0.7778, 0.7749, 0.7743, 0.7795, 0.7779, 0.7798, 0.7793]
  val_policy_top5_acc : [0.6159, 0.6101, 0.6138, 0.6192, 0.6187, 0.6185, 0.6192]
  val_value_loss : [0.2022, 0.2023, 0.2061, 0.1982, 0.2013, 0.1988, 0.1990]
  val_value_value_mse : [0.2022, 0.2023, 0.2061, 0.1982, 0.2013, 0.1988, 0.1990]
  value_loss : [0.2055, 0.2019, 0.1986, 0.1952, 0.1924, 0.1906, 0.1890]
  value_value_mse : [0.2055, 0.2019, 0.1986, 0.1952, 0.1924, 0.1906, 0.1890]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:37:48.511997Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game39001_game40500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.375137   (epoch 10)   mode=min
  policy_loss: 2.280030   (epoch 10)   mode=min
  policy_policy_acc: 0.325439   (epoch 10)   mode=max
  policy_top10_acc: 0.832024   (epoch 10)   mode=max
  policy_top5_acc: 0.679565   (epoch 10)   mode=max
  val_loss: 2.631017   (epoch 5)   mode=min
  val_policy_loss: 2.530199   (epoch 5)   mode=min
  val_policy_policy_acc: 0.283616   (epoch 9)   mode=max
  val_policy_top10_acc: 0.780420   (epoch 2)   mode=max
  val_policy_top5_acc: 0.621379   (epoch 4)   mode=max
  val_value_loss: 0.201137   (epoch 6)   mode=min
  val_value_value_mse: 0.201141   (epoch 6)   mode=min
  value_loss: 0.190390   (epoch 10)   mode=min
  value_value_mse: 0.190362   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003]
  loss : [2.6698, 2.6289, 2.5964, 2.5678, 2.5451, 2.5159, 2.4933, 2.4317, 2.4100, 2.3751]
  policy_loss : [2.5660, 2.5266, 2.4948, 2.4677, 2.4454, 2.4165, 2.3945, 2.3347, 2.3138, 2.2800]
  policy_policy_acc : [0.2690, 0.2777, 0.2830, 0.2881, 0.2922, 0.2962, 0.3027, 0.3135, 0.3186, 0.3254]
  policy_top10_acc : [0.7707, 0.7811, 0.7875, 0.7942, 0.8006, 0.8059, 0.8110, 0.8204, 0.8268, 0.8320]
  policy_top5_acc : [0.6053, 0.6173, 0.6249, 0.6317, 0.6387, 0.6459, 0.6536, 0.6659, 0.6719, 0.6796]
  val_loss : [2.6390, 2.6478, 2.6386, 2.6438, 2.6310, 2.6422, 2.6607, 2.6597, 2.6460, 2.7168]
  val_policy_loss : [2.5378, 2.5464, 2.5357, 2.5413, 2.5302, 2.5416, 2.5590, 2.5589, 2.5438, 2.6153]
  val_policy_policy_acc : [0.2750, 0.2705, 0.2756, 0.2810, 0.2738, 0.2775, 0.2746, 0.2800, 0.2836, 0.2808]
  val_policy_top10_acc : [0.7739, 0.7804, 0.7800, 0.7800, 0.7766, 0.7725, 0.7703, 0.7736, 0.7779, 0.7745]
  val_policy_top5_acc : [0.6109, 0.6157, 0.6139, 0.6214, 0.6148, 0.6114, 0.6128, 0.6159, 0.6156, 0.6152]
  val_value_loss : [0.2024, 0.2027, 0.2057, 0.2048, 0.2015, 0.2011, 0.2034, 0.2014, 0.2044, 0.2029]
  val_value_value_mse : [0.2024, 0.2027, 0.2057, 0.2048, 0.2015, 0.2011, 0.2035, 0.2013, 0.2044, 0.2028]
  value_loss : [0.2077, 0.2045, 0.2032, 0.2003, 0.1999, 0.1987, 0.1971, 0.1941, 0.1923, 0.1904]
  value_value_mse : [0.2077, 0.2045, 0.2032, 0.2003, 0.1999, 0.1987, 0.1971, 0.1941, 0.1924, 0.1904]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:45:42.596969Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game40501_game42000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.425874   (epoch 8)   mode=min
  policy_loss: 2.330392   (epoch 8)   mode=min
  policy_policy_acc: 0.314154   (epoch 8)   mode=max
  policy_top10_acc: 0.824122   (epoch 8)   mode=max
  policy_top5_acc: 0.667689   (epoch 8)   mode=max
  val_loss: 2.617036   (epoch 2)   mode=min
  val_policy_loss: 2.516493   (epoch 2)   mode=min
  val_policy_policy_acc: 0.278322   (epoch 8)   mode=max
  val_policy_top10_acc: 0.780220   (epoch 7)   mode=max
  val_policy_top5_acc: 0.620080   (epoch 8)   mode=max
  val_value_loss: 0.197812   (epoch 8)   mode=min
  val_value_value_mse: 0.197826   (epoch 8)   mode=min
  value_loss: 0.191055   (epoch 8)   mode=min
  value_value_mse: 0.191054   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6681, 2.6236, 2.5892, 2.5606, 2.5010, 2.4739, 2.4395, 2.4259]
  policy_loss : [2.5659, 2.5224, 2.4885, 2.4613, 2.4036, 2.3772, 2.3438, 2.3304]
  policy_policy_acc : [0.2667, 0.2748, 0.2823, 0.2884, 0.2988, 0.3036, 0.3103, 0.3142]
  policy_top10_acc : [0.7722, 0.7817, 0.7928, 0.7959, 0.8087, 0.8140, 0.8210, 0.8241]
  policy_top5_acc : [0.6044, 0.6184, 0.6277, 0.6336, 0.6479, 0.6560, 0.6648, 0.6677]
  val_loss : [2.6257, 2.6170, 2.6272, 2.7381, 2.6225, 2.6826, 2.9275, 2.7432]
  val_policy_loss : [2.5217, 2.5165, 2.5265, 2.6366, 2.5219, 2.5833, 2.8281, 2.6442]
  val_policy_policy_acc : [0.2749, 0.2761, 0.2748, 0.2714, 0.2771, 0.2778, 0.2774, 0.2783]
  val_policy_top10_acc : [0.7777, 0.7774, 0.7753, 0.7732, 0.7801, 0.7767, 0.7802, 0.7795]
  val_policy_top5_acc : [0.6140, 0.6163, 0.6115, 0.6095, 0.6156, 0.6164, 0.6188, 0.6201]
  val_value_loss : [0.2078, 0.2011, 0.2014, 0.2028, 0.2011, 0.1985, 0.1981, 0.1978]
  val_value_value_mse : [0.2078, 0.2011, 0.2014, 0.2028, 0.2011, 0.1985, 0.1981, 0.1978]
  value_loss : [0.2044, 0.2024, 0.2012, 0.1985, 0.1949, 0.1932, 0.1916, 0.1911]
  value_value_mse : [0.2044, 0.2024, 0.2012, 0.1985, 0.1949, 0.1932, 0.1915, 0.1911]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T00:54:37.595340Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game42001_game43500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.402254   (epoch 9)   mode=min
  policy_loss: 2.305980   (epoch 9)   mode=min
  policy_policy_acc: 0.316610   (epoch 9)   mode=max
  policy_top10_acc: 0.826965   (epoch 9)   mode=max
  policy_top5_acc: 0.674792   (epoch 9)   mode=max
  val_loss: 2.619327   (epoch 3)   mode=min
  val_policy_loss: 2.517376   (epoch 3)   mode=min
  val_policy_policy_acc: 0.284515   (epoch 8)   mode=max
  val_policy_top10_acc: 0.778821   (epoch 9)   mode=max
  val_policy_top5_acc: 0.621978   (epoch 9)   mode=max
  val_value_loss: 0.198591   (epoch 1)   mode=min
  val_value_value_mse: 0.198597   (epoch 1)   mode=min
  value_loss: 0.192684   (epoch 9)   mode=min
  value_value_mse: 0.192646   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6632, 2.6215, 2.5878, 2.5630, 2.5342, 2.4777, 2.4527, 2.4177, 2.4023]
  policy_loss : [2.5599, 2.5192, 2.4863, 2.4619, 2.4339, 2.3793, 2.3553, 2.3211, 2.3060]
  policy_policy_acc : [0.2671, 0.2758, 0.2822, 0.2868, 0.2898, 0.3030, 0.3062, 0.3158, 0.3166]
  policy_top10_acc : [0.7726, 0.7834, 0.7912, 0.7968, 0.8031, 0.8138, 0.8192, 0.8266, 0.8270]
  policy_top5_acc : [0.6076, 0.6210, 0.6280, 0.6359, 0.6429, 0.6575, 0.6617, 0.6718, 0.6748]
  val_loss : [2.6231, 2.7315, 2.6193, 2.6345, 2.6728, 2.6232, 2.6370, 2.6652, 2.7228]
  val_policy_loss : [2.5239, 2.6300, 2.5174, 2.5338, 2.5718, 2.5233, 2.5367, 2.5656, 2.6233]
  val_policy_policy_acc : [0.2788, 0.2787, 0.2820, 0.2733, 0.2759, 0.2815, 0.2800, 0.2845, 0.2819]
  val_policy_top10_acc : [0.7761, 0.7727, 0.7774, 0.7722, 0.7699, 0.7761, 0.7750, 0.7768, 0.7788]
  val_policy_top5_acc : [0.6173, 0.6126, 0.6195, 0.6123, 0.6102, 0.6144, 0.6181, 0.6182, 0.6220]
  val_value_loss : [0.1986, 0.2028, 0.2039, 0.2012, 0.2021, 0.1996, 0.2005, 0.1990, 0.1989]
  val_value_value_mse : [0.1986, 0.2028, 0.2039, 0.2012, 0.2021, 0.1996, 0.2005, 0.1990, 0.1989]
  value_loss : [0.2065, 0.2048, 0.2028, 0.2021, 0.2006, 0.1968, 0.1948, 0.1931, 0.1927]
  value_value_mse : [0.2066, 0.2048, 0.2028, 0.2021, 0.2006, 0.1968, 0.1948, 0.1931, 0.1926]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:03:33.911802Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game43501_game45000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.392530   (epoch 9)   mode=min
  policy_loss: 2.296578   (epoch 9)   mode=min
  policy_policy_acc: 0.323516   (epoch 9)   mode=max
  policy_top10_acc: 0.832388   (epoch 9)   mode=max
  policy_top5_acc: 0.675678   (epoch 9)   mode=max
  val_loss: 2.621020   (epoch 3)   mode=min
  val_policy_loss: 2.517926   (epoch 3)   mode=min
  val_policy_policy_acc: 0.286813   (epoch 8)   mode=max
  val_policy_top10_acc: 0.782118   (epoch 6)   mode=max
  val_policy_top5_acc: 0.620480   (epoch 8)   mode=max
  val_value_loss: 0.199255   (epoch 7)   mode=min
  val_value_value_mse: 0.199274   (epoch 7)   mode=min
  value_loss: 0.191932   (epoch 9)   mode=min
  value_value_mse: 0.191935   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6521, 2.6118, 2.5785, 2.5478, 2.5255, 2.4636, 2.4418, 2.4068, 2.3925]
  policy_loss : [2.5485, 2.5102, 2.4772, 2.4474, 2.4256, 2.3653, 2.3446, 2.3103, 2.2966]
  policy_policy_acc : [0.2705, 0.2795, 0.2850, 0.2907, 0.2946, 0.3084, 0.3137, 0.3189, 0.3235]
  policy_top10_acc : [0.7782, 0.7884, 0.7951, 0.8004, 0.8064, 0.8183, 0.8215, 0.8268, 0.8324]
  policy_top5_acc : [0.6125, 0.6216, 0.6312, 0.6387, 0.6441, 0.6602, 0.6658, 0.6723, 0.6757]
  val_loss : [2.6493, 2.6231, 2.6210, 2.6288, 2.6528, 2.6275, 2.6666, 2.6625, 2.6262]
  val_policy_loss : [2.5465, 2.5225, 2.5179, 2.5272, 2.5503, 2.5270, 2.5670, 2.5624, 2.5263]
  val_policy_policy_acc : [0.2726, 0.2793, 0.2797, 0.2792, 0.2781, 0.2819, 0.2807, 0.2868, 0.2853]
  val_policy_top10_acc : [0.7728, 0.7747, 0.7742, 0.7770, 0.7787, 0.7821, 0.7807, 0.7814, 0.7810]
  val_policy_top5_acc : [0.6065, 0.6134, 0.6169, 0.6159, 0.6132, 0.6200, 0.6183, 0.6205, 0.6186]
  val_value_loss : [0.2056, 0.2012, 0.2062, 0.2032, 0.2050, 0.2009, 0.1993, 0.2003, 0.1999]
  val_value_value_mse : [0.2056, 0.2012, 0.2062, 0.2032, 0.2050, 0.2009, 0.1993, 0.2003, 0.2000]
  value_loss : [0.2072, 0.2033, 0.2028, 0.2009, 0.1999, 0.1966, 0.1944, 0.1933, 0.1919]
  value_value_mse : [0.2072, 0.2033, 0.2028, 0.2009, 0.1999, 0.1966, 0.1944, 0.1933, 0.1919]

================================================================================

History file: model_versions/chess_elo_model_V14_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:13:33.074043Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game45001_game46500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V13
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.390324   (epoch 10)   mode=min
  policy_loss: 2.298685   (epoch 10)   mode=min
  policy_policy_acc: 0.319323   (epoch 10)   mode=max
  policy_top10_acc: 0.829938   (epoch 10)   mode=max
  policy_top5_acc: 0.675664   (epoch 10)   mode=max
  val_loss: 2.611274   (epoch 6)   mode=min
  val_policy_loss: 2.512701   (epoch 6)   mode=min
  val_policy_policy_acc: 0.281818   (epoch 10)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 5)   mode=max
  val_policy_top5_acc: 0.619480   (epoch 5)   mode=max
  val_value_loss: 0.197098   (epoch 6)   mode=min
  val_value_value_mse: 0.197116   (epoch 6)   mode=min
  value_loss: 0.183460   (epoch 10)   mode=min
  value_value_mse: 0.183455   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6565, 2.6123, 2.5783, 2.5532, 2.4952, 2.4714, 2.4530, 2.4377, 2.4044, 2.3903]
  policy_loss : [2.5563, 2.5132, 2.4803, 2.4567, 2.4001, 2.3773, 2.3594, 2.3447, 2.3124, 2.2987]
  policy_policy_acc : [0.2689, 0.2772, 0.2830, 0.2858, 0.2995, 0.3048, 0.3080, 0.3106, 0.3175, 0.3193]
  policy_top10_acc : [0.7747, 0.7835, 0.7925, 0.7975, 0.8095, 0.8138, 0.8174, 0.8207, 0.8273, 0.8299]
  policy_top5_acc : [0.6109, 0.6201, 0.6317, 0.6359, 0.6524, 0.6562, 0.6612, 0.6652, 0.6726, 0.6757]
  val_loss : [3.4628, 2.6179, 2.6234, 2.7266, 2.6335, 2.6113, 2.6712, 2.9011, 2.7458, 3.1491]
  val_policy_loss : [3.3627, 2.5186, 2.5240, 2.6253, 2.5335, 2.5127, 2.5705, 2.8009, 2.6464, 3.0496]
  val_policy_policy_acc : [0.2752, 0.2775, 0.2728, 0.2757, 0.2813, 0.2809, 0.2807, 0.2802, 0.2797, 0.2818]
  val_policy_top10_acc : [0.7723, 0.7745, 0.7745, 0.7702, 0.7830, 0.7794, 0.7782, 0.7747, 0.7759, 0.7749]
  val_policy_top5_acc : [0.6062, 0.6074, 0.6096, 0.6061, 0.6195, 0.6166, 0.6168, 0.6169, 0.6113, 0.6164]
  val_value_loss : [0.1992, 0.1985, 0.1987, 0.2024, 0.2001, 0.1971, 0.2011, 0.1998, 0.1985, 0.1983]
  val_value_value_mse : [0.1992, 0.1985, 0.1987, 0.2024, 0.2001, 0.1971, 0.2012, 0.1998, 0.1985, 0.1983]
  value_loss : [0.2005, 0.1983, 0.1960, 0.1933, 0.1902, 0.1880, 0.1874, 0.1862, 0.1837, 0.1835]
  value_value_mse : [0.2005, 0.1983, 0.1960, 0.1933, 0.1902, 0.1880, 0.1874, 0.1862, 0.1838, 0.1835]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:23:33.131568Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game46501_game48000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.383514   (epoch 10)   mode=min
  policy_loss: 2.287767   (epoch 10)   mode=min
  policy_policy_acc: 0.323584   (epoch 10)   mode=max
  policy_top10_acc: 0.831583   (epoch 10)   mode=max
  policy_top5_acc: 0.678199   (epoch 10)   mode=max
  val_loss: 2.612798   (epoch 4)   mode=min
  val_policy_loss: 2.511945   (epoch 4)   mode=min
  val_policy_policy_acc: 0.279021   (epoch 10)   mode=max
  val_policy_top10_acc: 0.780719   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618881   (epoch 10)   mode=max
  val_value_loss: 0.200070   (epoch 2)   mode=min
  val_value_value_mse: 0.200083   (epoch 2)   mode=min
  value_loss: 0.190903   (epoch 9)   mode=min
  value_value_mse: 0.190915   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6675, 2.6243, 2.5904, 2.5635, 2.5381, 2.5175, 2.4566, 2.4303, 2.3959, 2.3835]
  policy_loss : [2.5644, 2.5226, 2.4891, 2.4632, 2.4384, 2.4184, 2.3591, 2.3340, 2.3005, 2.2878]
  policy_policy_acc : [0.2676, 0.2772, 0.2856, 0.2880, 0.2927, 0.2972, 0.3084, 0.3148, 0.3221, 0.3236]
  policy_top10_acc : [0.7701, 0.7813, 0.7883, 0.7947, 0.8010, 0.8053, 0.8165, 0.8213, 0.8281, 0.8316]
  policy_top5_acc : [0.6074, 0.6186, 0.6272, 0.6341, 0.6403, 0.6468, 0.6616, 0.6680, 0.6768, 0.6782]
  val_loss : [2.6684, 2.6448, 2.6272, 2.6128, 2.6661, 2.8794, 2.9254, 2.6909, 2.7155, 2.6571]
  val_policy_loss : [2.5676, 2.5447, 2.5267, 2.5119, 2.5630, 2.7774, 2.8242, 2.5902, 2.6153, 2.5565]
  val_policy_policy_acc : [0.2703, 0.2743, 0.2714, 0.2738, 0.2707, 0.2664, 0.2763, 0.2779, 0.2789, 0.2790]
  val_policy_top10_acc : [0.7743, 0.7750, 0.7784, 0.7796, 0.7757, 0.7630, 0.7807, 0.7743, 0.7766, 0.7764]
  val_policy_top5_acc : [0.6099, 0.6151, 0.6113, 0.6150, 0.6098, 0.6011, 0.6166, 0.6103, 0.6160, 0.6189]
  val_value_loss : [0.2015, 0.2001, 0.2009, 0.2017, 0.2061, 0.2038, 0.2020, 0.2012, 0.2001, 0.2010]
  val_value_value_mse : [0.2015, 0.2001, 0.2009, 0.2017, 0.2061, 0.2038, 0.2020, 0.2013, 0.2001, 0.2010]
  value_loss : [0.2059, 0.2036, 0.2022, 0.2006, 0.1993, 0.1980, 0.1945, 0.1926, 0.1909, 0.1910]
  value_value_mse : [0.2059, 0.2036, 0.2023, 0.2006, 0.1993, 0.1980, 0.1945, 0.1926, 0.1909, 0.1910]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:30:44.435016Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game48001_game49500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441816   (epoch 7)   mode=min
  policy_loss: 2.341515   (epoch 7)   mode=min
  policy_policy_acc: 0.311496   (epoch 7)   mode=max
  policy_top10_acc: 0.821565   (epoch 7)   mode=max
  policy_top5_acc: 0.665086   (epoch 7)   mode=max
  val_loss: 2.624678   (epoch 1)   mode=min
  val_policy_loss: 2.522874   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285814   (epoch 4)   mode=max
  val_policy_top10_acc: 0.781219   (epoch 6)   mode=max
  val_policy_top5_acc: 0.626573   (epoch 6)   mode=max
  val_value_loss: 0.197980   (epoch 7)   mode=min
  val_value_value_mse: 0.197984   (epoch 7)   mode=min
  value_loss: 0.200459   (epoch 7)   mode=min
  value_value_mse: 0.200460   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6523, 2.6129, 2.5763, 2.5150, 2.4889, 2.4571, 2.4418]
  policy_loss : [2.5459, 2.5078, 2.4720, 2.4125, 2.3875, 2.3567, 2.3415]
  policy_policy_acc : [0.2699, 0.2786, 0.2842, 0.2967, 0.3020, 0.3088, 0.3115]
  policy_top10_acc : [0.7778, 0.7850, 0.7945, 0.8068, 0.8101, 0.8170, 0.8216]
  policy_top5_acc : [0.6116, 0.6221, 0.6318, 0.6478, 0.6540, 0.6616, 0.6651]
  val_loss : [2.6247, 2.7361, 3.0667, 2.7800, 2.7605, 2.6847, 3.0692]
  val_policy_loss : [2.5229, 2.6347, 2.9648, 2.6796, 2.6607, 2.5853, 2.9698]
  val_policy_policy_acc : [0.2775, 0.2740, 0.2755, 0.2858, 0.2819, 0.2850, 0.2841]
  val_policy_top10_acc : [0.7750, 0.7747, 0.7776, 0.7810, 0.7752, 0.7812, 0.7783]
  val_policy_top5_acc : [0.6109, 0.6123, 0.6149, 0.6200, 0.6182, 0.6266, 0.6224]
  val_value_loss : [0.2035, 0.2026, 0.2031, 0.2004, 0.1991, 0.1986, 0.1980]
  val_value_value_mse : [0.2035, 0.2026, 0.2031, 0.2004, 0.1991, 0.1986, 0.1980]
  value_loss : [0.2129, 0.2101, 0.2084, 0.2049, 0.2026, 0.2007, 0.2005]
  value_value_mse : [0.2129, 0.2101, 0.2084, 0.2049, 0.2026, 0.2007, 0.2005]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:40:45.925455Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game49501_game51000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.409503   (epoch 10)   mode=min
  policy_loss: 2.312462   (epoch 10)   mode=min
  policy_policy_acc: 0.318464   (epoch 10)   mode=max
  policy_top10_acc: 0.826149   (epoch 10)   mode=max
  policy_top5_acc: 0.672855   (epoch 10)   mode=max
  val_loss: 2.615902   (epoch 5)   mode=min
  val_policy_loss: 2.515988   (epoch 5)   mode=min
  val_policy_policy_acc: 0.282318   (epoch 10)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 10)   mode=max
  val_policy_top5_acc: 0.621478   (epoch 4)   mode=max
  val_value_loss: 0.198106   (epoch 2)   mode=min
  val_value_value_mse: 0.198122   (epoch 2)   mode=min
  value_loss: 0.194040   (epoch 10)   mode=min
  value_value_mse: 0.194040   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6643, 2.6242, 2.5951, 2.5330, 2.5081, 2.4915, 2.4741, 2.4402, 2.4296, 2.4095]
  policy_loss : [2.5601, 2.5210, 2.4930, 2.4326, 2.4084, 2.3922, 2.3752, 2.3423, 2.3321, 2.3125]
  policy_policy_acc : [0.2674, 0.2764, 0.2818, 0.2934, 0.2990, 0.3013, 0.3046, 0.3120, 0.3144, 0.3185]
  policy_top10_acc : [0.7733, 0.7826, 0.7888, 0.8017, 0.8067, 0.8104, 0.8133, 0.8215, 0.8227, 0.8261]
  policy_top5_acc : [0.6095, 0.6203, 0.6278, 0.6419, 0.6502, 0.6531, 0.6587, 0.6674, 0.6693, 0.6729]
  val_loss : [2.6199, 2.6284, 2.6373, 2.7530, 2.6159, 2.7287, 2.8113, 3.4059, 2.9808, 3.1809]
  val_policy_loss : [2.5178, 2.5293, 2.5340, 2.6530, 2.5160, 2.6282, 2.7112, 3.3055, 2.8800, 3.0810]
  val_policy_policy_acc : [0.2706, 0.2683, 0.2728, 0.2810, 0.2795, 0.2788, 0.2806, 0.2817, 0.2800, 0.2823]
  val_policy_top10_acc : [0.7796, 0.7764, 0.7784, 0.7829, 0.7822, 0.7821, 0.7786, 0.7818, 0.7829, 0.7830]
  val_policy_top5_acc : [0.6186, 0.6113, 0.6116, 0.6215, 0.6212, 0.6215, 0.6177, 0.6211, 0.6195, 0.6199]
  val_value_loss : [0.2040, 0.1981, 0.2066, 0.1998, 0.1996, 0.2007, 0.1997, 0.1997, 0.2010, 0.1991]
  val_value_value_mse : [0.2040, 0.1981, 0.2066, 0.1998, 0.1996, 0.2007, 0.1997, 0.1997, 0.2010, 0.1991]
  value_loss : [0.2085, 0.2064, 0.2043, 0.2007, 0.1995, 0.1986, 0.1978, 0.1956, 0.1952, 0.1940]
  value_value_mse : [0.2085, 0.2064, 0.2043, 0.2007, 0.1995, 0.1986, 0.1978, 0.1956, 0.1952, 0.1940]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:48:45.650841Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game51001_game52500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451041   (epoch 8)   mode=min
  policy_loss: 2.351783   (epoch 8)   mode=min
  policy_policy_acc: 0.309974   (epoch 8)   mode=max
  policy_top10_acc: 0.816449   (epoch 8)   mode=max
  policy_top5_acc: 0.660237   (epoch 8)   mode=max
  val_loss: 2.626408   (epoch 2)   mode=min
  val_policy_loss: 2.526025   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282018   (epoch 8)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 5)   mode=max
  val_policy_top5_acc: 0.621079   (epoch 5)   mode=max
  val_value_loss: 0.198476   (epoch 1)   mode=min
  val_value_value_mse: 0.198494   (epoch 1)   mode=min
  value_loss: 0.198377   (epoch 8)   mode=min
  value_value_mse: 0.198369   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6826, 2.6421, 2.6129, 2.5860, 2.5230, 2.5000, 2.4664, 2.4510]
  policy_loss : [2.5767, 2.5374, 2.5090, 2.4827, 2.4218, 2.3996, 2.3668, 2.3518]
  policy_policy_acc : [0.2644, 0.2720, 0.2772, 0.2842, 0.2980, 0.3007, 0.3069, 0.3100]
  policy_top10_acc : [0.7680, 0.7769, 0.7845, 0.7887, 0.8016, 0.8070, 0.8135, 0.8164]
  policy_top5_acc : [0.6038, 0.6124, 0.6213, 0.6290, 0.6455, 0.6504, 0.6576, 0.6602]
  val_loss : [2.6359, 2.6264, 2.8082, 2.6755, 2.6503, 2.6466, 2.6589, 2.6432]
  val_policy_loss : [2.5366, 2.5260, 2.7060, 2.5738, 2.5492, 2.5468, 2.5585, 2.5425]
  val_policy_policy_acc : [0.2723, 0.2766, 0.2762, 0.2794, 0.2816, 0.2789, 0.2799, 0.2820]
  val_policy_top10_acc : [0.7768, 0.7795, 0.7746, 0.7766, 0.7814, 0.7732, 0.7790, 0.7808]
  val_policy_top5_acc : [0.6125, 0.6179, 0.6094, 0.6130, 0.6211, 0.6132, 0.6199, 0.6210]
  val_value_loss : [0.1985, 0.2007, 0.2039, 0.2030, 0.2019, 0.1993, 0.2005, 0.2011]
  val_value_value_mse : [0.1985, 0.2008, 0.2039, 0.2031, 0.2019, 0.1993, 0.2006, 0.2011]
  value_loss : [0.2119, 0.2098, 0.2077, 0.2064, 0.2020, 0.2008, 0.1990, 0.1984]
  value_value_mse : [0.2119, 0.2098, 0.2077, 0.2063, 0.2020, 0.2007, 0.1991, 0.1984]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T01:58:46.806121Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game52501_game54000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.423445   (epoch 10)   mode=min
  policy_loss: 2.324387   (epoch 10)   mode=min
  policy_policy_acc: 0.316405   (epoch 10)   mode=max
  policy_top10_acc: 0.823114   (epoch 10)   mode=max
  policy_top5_acc: 0.668721   (epoch 10)   mode=max
  val_loss: 2.612306   (epoch 4)   mode=min
  val_policy_loss: 2.512465   (epoch 4)   mode=min
  val_policy_policy_acc: 0.287712   (epoch 9)   mode=max
  val_policy_top10_acc: 0.786813   (epoch 8)   mode=max
  val_policy_top5_acc: 0.623477   (epoch 8)   mode=max
  val_value_loss: 0.196943   (epoch 7)   mode=min
  val_value_value_mse: 0.196963   (epoch 7)   mode=min
  value_loss: 0.198054   (epoch 10)   mode=min
  value_value_mse: 0.198048   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6728, 2.6294, 2.5969, 2.5354, 2.5165, 2.4953, 2.4629, 2.4504, 2.4301, 2.4234]
  policy_loss : [2.5665, 2.5240, 2.4923, 2.4330, 2.4148, 2.3940, 2.3627, 2.3506, 2.3309, 2.3244]
  policy_policy_acc : [0.2678, 0.2753, 0.2809, 0.2932, 0.2989, 0.3033, 0.3099, 0.3099, 0.3150, 0.3164]
  policy_top10_acc : [0.7709, 0.7813, 0.7873, 0.8020, 0.8046, 0.8093, 0.8166, 0.8181, 0.8228, 0.8231]
  policy_top5_acc : [0.6057, 0.6186, 0.6260, 0.6419, 0.6461, 0.6517, 0.6599, 0.6636, 0.6681, 0.6687]
  val_loss : [2.6252, 2.7304, 2.6418, 2.6123, 2.8648, 2.9227, 3.1627, 3.1975, 3.5642, 3.6561]
  val_policy_loss : [2.5246, 2.6283, 2.5412, 2.5125, 2.7655, 2.8235, 3.0638, 3.0981, 3.4646, 3.5564]
  val_policy_policy_acc : [0.2814, 0.2742, 0.2783, 0.2837, 0.2860, 0.2832, 0.2853, 0.2864, 0.2877, 0.2854]
  val_policy_top10_acc : [0.7778, 0.7805, 0.7795, 0.7824, 0.7845, 0.7842, 0.7839, 0.7868, 0.7863, 0.7864]
  val_policy_top5_acc : [0.6217, 0.6143, 0.6122, 0.6218, 0.6222, 0.6198, 0.6210, 0.6235, 0.6221, 0.6228]
  val_value_loss : [0.2011, 0.2039, 0.2010, 0.1995, 0.1983, 0.1978, 0.1969, 0.1979, 0.1980, 0.1980]
  val_value_value_mse : [0.2011, 0.2039, 0.2010, 0.1995, 0.1983, 0.1978, 0.1970, 0.1979, 0.1980, 0.1980]
  value_loss : [0.2126, 0.2107, 0.2093, 0.2049, 0.2033, 0.2027, 0.2003, 0.1995, 0.1983, 0.1981]
  value_value_mse : [0.2126, 0.2107, 0.2093, 0.2049, 0.2033, 0.2027, 0.2003, 0.1995, 0.1983, 0.1980]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:07:43.474514Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game54001_game55500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.454105   (epoch 8)   mode=min
  policy_loss: 2.354422   (epoch 8)   mode=min
  policy_policy_acc: 0.307697   (epoch 8)   mode=max
  policy_top10_acc: 0.817416   (epoch 8)   mode=max
  policy_top5_acc: 0.660954   (epoch 8)   mode=max
  val_loss: 2.685542   (epoch 2)   mode=min
  val_policy_loss: 2.584976   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285614   (epoch 7)   mode=max
  val_policy_top10_acc: 0.781219   (epoch 1)   mode=max
  val_policy_top5_acc: 0.620180   (epoch 8)   mode=max
  val_value_loss: 0.197470   (epoch 5)   mode=min
  val_value_value_mse: 0.197480   (epoch 5)   mode=min
  value_loss: 0.199308   (epoch 8)   mode=min
  value_value_mse: 0.199310   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6786, 2.6407, 2.6091, 2.5830, 2.5246, 2.4996, 2.4689, 2.4541]
  policy_loss : [2.5721, 2.5355, 2.5051, 2.4798, 2.4227, 2.3986, 2.3687, 2.3544]
  policy_policy_acc : [0.2642, 0.2726, 0.2782, 0.2827, 0.2944, 0.2996, 0.3063, 0.3077]
  policy_top10_acc : [0.7691, 0.7791, 0.7849, 0.7922, 0.8046, 0.8079, 0.8142, 0.8174]
  policy_top5_acc : [0.6024, 0.6155, 0.6221, 0.6290, 0.6425, 0.6505, 0.6578, 0.6610]
  val_loss : [2.7809, 2.6855, 3.9149, 3.0080, 2.8945, 3.2730, 3.1933, 3.1310]
  val_policy_loss : [2.6808, 2.5850, 3.8146, 2.9064, 2.7955, 3.1725, 3.0941, 3.0314]
  val_policy_policy_acc : [0.2773, 0.2749, 0.2817, 0.2770, 0.2778, 0.2755, 0.2856, 0.2834]
  val_policy_top10_acc : [0.7812, 0.7796, 0.7758, 0.7731, 0.7781, 0.7772, 0.7797, 0.7789]
  val_policy_top5_acc : [0.6181, 0.6128, 0.6123, 0.6086, 0.6134, 0.6165, 0.6179, 0.6202]
  val_value_loss : [0.2000, 0.2009, 0.1990, 0.2027, 0.1975, 0.2001, 0.1977, 0.1985]
  val_value_value_mse : [0.2000, 0.2009, 0.1990, 0.2027, 0.1975, 0.2001, 0.1977, 0.1985]
  value_loss : [0.2130, 0.2104, 0.2081, 0.2064, 0.2036, 0.2021, 0.2004, 0.1993]
  value_value_mse : [0.2130, 0.2104, 0.2081, 0.2064, 0.2036, 0.2021, 0.2004, 0.1993]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:18:04.719905Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game55501_game57000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.395773   (epoch 10)   mode=min
  policy_loss: 2.304922   (epoch 10)   mode=min
  policy_policy_acc: 0.318246   (epoch 10)   mode=max
  policy_top10_acc: 0.827137   (epoch 10)   mode=max
  policy_top5_acc: 0.671939   (epoch 10)   mode=max
  val_loss: 2.622531   (epoch 6)   mode=min
  val_policy_loss: 2.522973   (epoch 6)   mode=min
  val_policy_policy_acc: 0.287313   (epoch 10)   mode=max
  val_policy_top10_acc: 0.780819   (epoch 8)   mode=max
  val_policy_top5_acc: 0.623377   (epoch 9)   mode=max
  val_value_loss: 0.197313   (epoch 1)   mode=min
  val_value_value_mse: 0.197330   (epoch 1)   mode=min
  value_loss: 0.181697   (epoch 10)   mode=min
  value_value_mse: 0.181696   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6574, 2.6177, 2.5836, 2.5603, 2.5364, 2.4770, 2.4558, 2.4400, 2.4061, 2.3958]
  policy_loss : [2.5584, 2.5202, 2.4875, 2.4647, 2.4415, 2.3840, 2.3634, 2.3485, 2.3152, 2.3049]
  policy_policy_acc : [0.2688, 0.2759, 0.2814, 0.2853, 0.2911, 0.3035, 0.3064, 0.3082, 0.3164, 0.3182]
  policy_top10_acc : [0.7744, 0.7834, 0.7905, 0.7966, 0.8012, 0.8135, 0.8173, 0.8193, 0.8261, 0.8271]
  policy_top5_acc : [0.6075, 0.6165, 0.6263, 0.6345, 0.6383, 0.6517, 0.6602, 0.6627, 0.6709, 0.6719]
  val_loss : [2.6230, 2.6736, 2.6274, 2.7407, 2.6404, 2.6225, 2.7025, 2.7238, 2.9326, 2.7537]
  val_policy_loss : [2.5243, 2.5737, 2.5238, 2.6390, 2.5412, 2.5230, 2.6026, 2.6219, 2.8326, 2.6540]
  val_policy_policy_acc : [0.2763, 0.2753, 0.2760, 0.2787, 0.2753, 0.2810, 0.2821, 0.2837, 0.2867, 0.2873]
  val_policy_top10_acc : [0.7779, 0.7782, 0.7763, 0.7737, 0.7792, 0.7796, 0.7777, 0.7808, 0.7796, 0.7802]
  val_policy_top5_acc : [0.6144, 0.6166, 0.6108, 0.6154, 0.6145, 0.6206, 0.6183, 0.6200, 0.6234, 0.6226]
  val_value_loss : [0.1973, 0.1995, 0.2070, 0.2032, 0.1983, 0.1989, 0.1995, 0.2034, 0.1992, 0.1990]
  val_value_value_mse : [0.1973, 0.1995, 0.2070, 0.2032, 0.1983, 0.1989, 0.1995, 0.2034, 0.1993, 0.1990]
  value_loss : [0.1980, 0.1951, 0.1924, 0.1911, 0.1897, 0.1861, 0.1848, 0.1829, 0.1819, 0.1817]
  value_value_mse : [0.1980, 0.1951, 0.1924, 0.1911, 0.1897, 0.1861, 0.1848, 0.1829, 0.1819, 0.1817]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:28:36.746771Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game57001_game58500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.413547   (epoch 10)   mode=min
  policy_loss: 2.322305   (epoch 10)   mode=min
  policy_policy_acc: 0.313932   (epoch 10)   mode=max
  policy_top10_acc: 0.824934   (epoch 10)   mode=max
  policy_top5_acc: 0.667977   (epoch 10)   mode=max
  val_loss: 2.599573   (epoch 4)   mode=min
  val_policy_loss: 2.500968   (epoch 4)   mode=min
  val_policy_policy_acc: 0.288412   (epoch 8)   mode=max
  val_policy_top10_acc: 0.782418   (epoch 5)   mode=max
  val_policy_top5_acc: 0.623377   (epoch 5)   mode=max
  val_value_loss: 0.195868   (epoch 5)   mode=min
  val_value_value_mse: 0.195882   (epoch 5)   mode=min
  value_loss: 0.182360   (epoch 10)   mode=min
  value_value_mse: 0.182372   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6609, 2.6140, 2.5856, 2.5231, 2.5015, 2.4831, 2.4491, 2.4354, 2.4207, 2.4135]
  policy_loss : [2.5619, 2.5168, 2.4894, 2.4285, 2.4077, 2.3900, 2.3568, 2.3436, 2.3289, 2.3223]
  policy_policy_acc : [0.2646, 0.2723, 0.2800, 0.2939, 0.2968, 0.2994, 0.3068, 0.3103, 0.3128, 0.3139]
  policy_top10_acc : [0.7721, 0.7842, 0.7907, 0.8036, 0.8076, 0.8115, 0.8172, 0.8206, 0.8223, 0.8249]
  policy_top5_acc : [0.6043, 0.6196, 0.6265, 0.6433, 0.6469, 0.6530, 0.6596, 0.6636, 0.6671, 0.6680]
  val_loss : [2.6166, 2.6217, 2.6199, 2.5996, 2.6118, 2.6078, 2.6764, 3.0188, 2.8271, 2.9085]
  val_policy_loss : [2.5164, 2.5219, 2.5202, 2.5010, 2.5137, 2.5092, 2.5779, 2.9201, 2.7280, 2.8094]
  val_policy_policy_acc : [0.2754, 0.2791, 0.2778, 0.2809, 0.2832, 0.2842, 0.2849, 0.2884, 0.2861, 0.2868]
  val_policy_top10_acc : [0.7746, 0.7734, 0.7770, 0.7787, 0.7824, 0.7806, 0.7763, 0.7780, 0.7775, 0.7794]
  val_policy_top5_acc : [0.6165, 0.6114, 0.6105, 0.6225, 0.6234, 0.6208, 0.6194, 0.6218, 0.6205, 0.6219]
  val_value_loss : [0.2003, 0.1994, 0.1990, 0.1971, 0.1959, 0.1969, 0.1967, 0.1968, 0.1977, 0.1977]
  val_value_value_mse : [0.2003, 0.1994, 0.1990, 0.1972, 0.1959, 0.1969, 0.1967, 0.1968, 0.1977, 0.1977]
  value_loss : [0.1978, 0.1947, 0.1921, 0.1890, 0.1874, 0.1859, 0.1841, 0.1835, 0.1831, 0.1824]
  value_value_mse : [0.1978, 0.1947, 0.1921, 0.1890, 0.1874, 0.1859, 0.1841, 0.1835, 0.1831, 0.1824]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:36:29.473991Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game58501_game60000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441881   (epoch 8)   mode=min
  policy_loss: 2.344314   (epoch 8)   mode=min
  policy_policy_acc: 0.311593   (epoch 8)   mode=max
  policy_top10_acc: 0.818935   (epoch 8)   mode=max
  policy_top5_acc: 0.662289   (epoch 8)   mode=max
  val_loss: 2.616732   (epoch 2)   mode=min
  val_policy_loss: 2.516685   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283916   (epoch 7)   mode=max
  val_policy_top10_acc: 0.784016   (epoch 7)   mode=max
  val_policy_top5_acc: 0.621678   (epoch 8)   mode=max
  val_value_loss: 0.197739   (epoch 3)   mode=min
  val_value_value_mse: 0.197748   (epoch 3)   mode=min
  value_loss: 0.195809   (epoch 8)   mode=min
  value_value_mse: 0.195801   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6714, 2.6268, 2.5981, 2.5661, 2.5113, 2.4882, 2.4534, 2.4419]
  policy_loss : [2.5672, 2.5234, 2.4955, 2.4644, 2.4115, 2.3895, 2.3552, 2.3443]
  policy_policy_acc : [0.2676, 0.2741, 0.2807, 0.2882, 0.2990, 0.3027, 0.3093, 0.3116]
  policy_top10_acc : [0.7694, 0.7801, 0.7866, 0.7942, 0.8045, 0.8096, 0.8156, 0.8189]
  policy_top5_acc : [0.6054, 0.6166, 0.6232, 0.6322, 0.6454, 0.6514, 0.6595, 0.6623]
  val_loss : [2.6191, 2.6167, 2.6190, 2.6311, 2.6641, 2.7724, 2.7768, 3.5455]
  val_policy_loss : [2.5186, 2.5167, 2.5200, 2.5311, 2.5642, 2.6724, 2.6765, 3.4444]
  val_policy_policy_acc : [0.2738, 0.2734, 0.2775, 0.2806, 0.2833, 0.2796, 0.2839, 0.2832]
  val_policy_top10_acc : [0.7784, 0.7768, 0.7803, 0.7790, 0.7817, 0.7816, 0.7840, 0.7832]
  val_policy_top5_acc : [0.6086, 0.6125, 0.6174, 0.6132, 0.6194, 0.6203, 0.6208, 0.6217]
  val_value_loss : [0.2007, 0.2000, 0.1977, 0.1996, 0.1996, 0.1996, 0.2000, 0.2007]
  val_value_value_mse : [0.2007, 0.2000, 0.1977, 0.1996, 0.1996, 0.1996, 0.2000, 0.2007]
  value_loss : [0.2090, 0.2066, 0.2051, 0.2034, 0.2000, 0.1986, 0.1970, 0.1958]
  value_value_mse : [0.2090, 0.2066, 0.2051, 0.2034, 0.2000, 0.1985, 0.1970, 0.1958]

================================================================================

History file: model_versions/chess_elo_model_V15_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:46:32.487836Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game60001_game61500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V14
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399989   (epoch 10)   mode=min
  policy_loss: 2.307173   (epoch 10)   mode=min
  policy_policy_acc: 0.318606   (epoch 10)   mode=max
  policy_top10_acc: 0.826642   (epoch 10)   mode=max
  policy_top5_acc: 0.675686   (epoch 10)   mode=max
  val_loss: 2.601984   (epoch 5)   mode=min
  val_policy_loss: 2.503484   (epoch 5)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 10)   mode=max
  val_policy_top10_acc: 0.784915   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623976   (epoch 5)   mode=max
  val_value_loss: 0.196832   (epoch 4)   mode=min
  val_value_value_mse: 0.196848   (epoch 4)   mode=min
  value_loss: 0.185172   (epoch 10)   mode=min
  value_value_mse: 0.185179   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6528, 2.6157, 2.5877, 2.5645, 2.5041, 2.4831, 2.4645, 2.4315, 2.4180, 2.4000]
  policy_loss : [2.5520, 2.5170, 2.4895, 2.4673, 2.4086, 2.3883, 2.3705, 2.3386, 2.3249, 2.3072]
  policy_policy_acc : [0.2690, 0.2754, 0.2824, 0.2848, 0.2975, 0.3023, 0.3068, 0.3110, 0.3140, 0.3186]
  policy_top10_acc : [0.7758, 0.7849, 0.7893, 0.7956, 0.8068, 0.8111, 0.8160, 0.8216, 0.8258, 0.8266]
  policy_top5_acc : [0.6117, 0.6205, 0.6289, 0.6345, 0.6487, 0.6538, 0.6579, 0.6671, 0.6704, 0.6757]
  val_loss : [2.7648, 2.6135, 2.6144, 2.6134, 2.6020, 2.6353, 2.6192, 2.6492, 2.7256, 2.8028]
  val_policy_loss : [2.6651, 2.5147, 2.5150, 2.5149, 2.5035, 2.5358, 2.5193, 2.5503, 2.6262, 2.7037]
  val_policy_policy_acc : [0.2697, 0.2730, 0.2781, 0.2761, 0.2812, 0.2826, 0.2816, 0.2820, 0.2831, 0.2857]
  val_policy_top10_acc : [0.7745, 0.7849, 0.7797, 0.7822, 0.7818, 0.7805, 0.7802, 0.7809, 0.7773, 0.7830]
  val_policy_top5_acc : [0.6095, 0.6168, 0.6209, 0.6169, 0.6240, 0.6187, 0.6206, 0.6226, 0.6215, 0.6228]
  val_value_loss : [0.1993, 0.1976, 0.1987, 0.1968, 0.1969, 0.1988, 0.1997, 0.1976, 0.1986, 0.1979]
  val_value_value_mse : [0.1993, 0.1976, 0.1987, 0.1968, 0.1969, 0.1988, 0.1997, 0.1976, 0.1986, 0.1979]
  value_loss : [0.2013, 0.1974, 0.1962, 0.1943, 0.1907, 0.1893, 0.1880, 0.1864, 0.1862, 0.1852]
  value_value_mse : [0.2013, 0.1974, 0.1962, 0.1943, 0.1908, 0.1893, 0.1880, 0.1864, 0.1861, 0.1852]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T02:53:51.637954Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game61501_game63000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.452120   (epoch 7)   mode=min
  policy_loss: 2.359416   (epoch 7)   mode=min
  policy_policy_acc: 0.309387   (epoch 6)   mode=max
  policy_top10_acc: 0.816654   (epoch 7)   mode=max
  policy_top5_acc: 0.659810   (epoch 7)   mode=max
  val_loss: 2.623724   (epoch 1)   mode=min
  val_policy_loss: 2.524663   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285914   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 4)   mode=max
  val_policy_top5_acc: 0.624476   (epoch 7)   mode=max
  val_value_loss: 0.197116   (epoch 5)   mode=min
  val_value_value_mse: 0.197131   (epoch 5)   mode=min
  value_loss: 0.185374   (epoch 7)   mode=min
  value_value_mse: 0.185368   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6566, 2.6159, 2.5801, 2.5222, 2.4968, 2.4625, 2.4521]
  policy_loss : [2.5569, 2.5187, 2.4836, 2.4271, 2.4028, 2.3696, 2.3594]
  policy_policy_acc : [0.2681, 0.2786, 0.2818, 0.2943, 0.3011, 0.3094, 0.3094]
  policy_top10_acc : [0.7737, 0.7831, 0.7913, 0.8033, 0.8082, 0.8152, 0.8167]
  policy_top5_acc : [0.6100, 0.6199, 0.6300, 0.6441, 0.6526, 0.6589, 0.6598]
  val_loss : [2.6237, 2.7027, 3.2575, 3.1438, 3.0309, 3.1413, 3.4027]
  val_policy_loss : [2.5247, 2.6037, 3.1571, 3.0444, 2.9322, 3.0424, 3.3033]
  val_policy_policy_acc : [0.2775, 0.2805, 0.2746, 0.2813, 0.2799, 0.2858, 0.2859]
  val_policy_top10_acc : [0.7818, 0.7755, 0.7741, 0.7830, 0.7813, 0.7822, 0.7812]
  val_policy_top5_acc : [0.6148, 0.6148, 0.6156, 0.6215, 0.6220, 0.6227, 0.6245]
  val_value_loss : [0.1981, 0.1978, 0.2001, 0.1980, 0.1971, 0.1972, 0.1979]
  val_value_value_mse : [0.1981, 0.1978, 0.2001, 0.1981, 0.1971, 0.1972, 0.1979]
  value_loss : [0.1985, 0.1955, 0.1936, 0.1899, 0.1884, 0.1859, 0.1854]
  value_value_mse : [0.1985, 0.1955, 0.1936, 0.1898, 0.1884, 0.1859, 0.1854]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:01:01.434528Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game63001_game64500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.449205   (epoch 7)   mode=min
  policy_loss: 2.353892   (epoch 7)   mode=min
  policy_policy_acc: 0.310552   (epoch 7)   mode=max
  policy_top10_acc: 0.815437   (epoch 7)   mode=max
  policy_top5_acc: 0.660756   (epoch 7)   mode=max
  val_loss: 2.612189   (epoch 1)   mode=min
  val_policy_loss: 2.513827   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287812   (epoch 7)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 6)   mode=max
  val_policy_top5_acc: 0.619780   (epoch 6)   mode=max
  val_value_loss: 0.196667   (epoch 1)   mode=min
  val_value_value_mse: 0.196682   (epoch 1)   mode=min
  value_loss: 0.190730   (epoch 7)   mode=min
  value_value_mse: 0.190737   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6524, 2.6137, 2.5795, 2.5198, 2.4978, 2.4619, 2.4492]
  policy_loss : [2.5508, 2.5133, 2.4802, 2.4225, 2.4011, 2.3661, 2.3539]
  policy_policy_acc : [0.2701, 0.2776, 0.2826, 0.2958, 0.3010, 0.3074, 0.3106]
  policy_top10_acc : [0.7718, 0.7813, 0.7896, 0.8015, 0.8058, 0.8137, 0.8154]
  policy_top5_acc : [0.6104, 0.6208, 0.6300, 0.6455, 0.6500, 0.6591, 0.6608]
  val_loss : [2.6122, 2.8130, 2.6694, 2.9447, 2.7404, 3.1944, 3.2376]
  val_policy_loss : [2.5138, 2.7124, 2.5686, 2.8436, 2.6411, 3.0954, 3.1376]
  val_policy_policy_acc : [0.2777, 0.2777, 0.2823, 0.2847, 0.2833, 0.2837, 0.2878]
  val_policy_top10_acc : [0.7768, 0.7769, 0.7774, 0.7824, 0.7837, 0.7851, 0.7850]
  val_policy_top5_acc : [0.6182, 0.6142, 0.6098, 0.6191, 0.6189, 0.6198, 0.6183]
  val_value_loss : [0.1967, 0.2009, 0.2012, 0.2017, 0.1982, 0.1972, 0.1990]
  val_value_value_mse : [0.1967, 0.2009, 0.2012, 0.2017, 0.1982, 0.1972, 0.1990]
  value_loss : [0.2033, 0.2007, 0.1988, 0.1945, 0.1935, 0.1915, 0.1907]
  value_value_mse : [0.2033, 0.2007, 0.1988, 0.1945, 0.1935, 0.1915, 0.1907]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:08:05.442835Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game64501_game66000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.465437   (epoch 7)   mode=min
  policy_loss: 2.368005   (epoch 7)   mode=min
  policy_policy_acc: 0.304992   (epoch 7)   mode=max
  policy_top10_acc: 0.816310   (epoch 7)   mode=max
  policy_top5_acc: 0.656146   (epoch 7)   mode=max
  val_loss: 2.614647   (epoch 1)   mode=min
  val_policy_loss: 2.513270   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288212   (epoch 6)   mode=max
  val_policy_top10_acc: 0.784516   (epoch 5)   mode=max
  val_policy_top5_acc: 0.623177   (epoch 6)   mode=max
  val_value_loss: 0.195728   (epoch 4)   mode=min
  val_value_value_mse: 0.195748   (epoch 4)   mode=min
  value_loss: 0.195051   (epoch 7)   mode=min
  value_value_mse: 0.195057   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6595, 2.6223, 2.5918, 2.5349, 2.5107, 2.4762, 2.4654]
  policy_loss : [2.5560, 2.5204, 2.4907, 2.4356, 2.4118, 2.3784, 2.3680]
  policy_policy_acc : [0.2664, 0.2728, 0.2793, 0.2909, 0.2961, 0.3039, 0.3050]
  policy_top10_acc : [0.7752, 0.7837, 0.7906, 0.8008, 0.8070, 0.8133, 0.8163]
  policy_top5_acc : [0.6066, 0.6178, 0.6254, 0.6407, 0.6449, 0.6552, 0.6561]
  val_loss : [2.6146, 2.8199, 3.6766, 3.0999, 2.9328, 2.9328, 3.2055]
  val_policy_loss : [2.5133, 2.7212, 3.5761, 3.0015, 2.8340, 2.8341, 3.1065]
  val_policy_policy_acc : [0.2780, 0.2751, 0.2781, 0.2873, 0.2852, 0.2882, 0.2881]
  val_policy_top10_acc : [0.7787, 0.7722, 0.7753, 0.7809, 0.7845, 0.7835, 0.7822]
  val_policy_top5_acc : [0.6175, 0.6107, 0.6154, 0.6207, 0.6162, 0.6232, 0.6207]
  val_value_loss : [0.2025, 0.1970, 0.1997, 0.1957, 0.1968, 0.1966, 0.1970]
  val_value_value_mse : [0.2025, 0.1971, 0.1997, 0.1957, 0.1969, 0.1966, 0.1970]
  value_loss : [0.2070, 0.2038, 0.2023, 0.1987, 0.1978, 0.1956, 0.1951]
  value_value_mse : [0.2070, 0.2038, 0.2023, 0.1987, 0.1978, 0.1956, 0.1951]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:17:57.026098Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game66001_game67500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.431903   (epoch 10)   mode=min
  policy_loss: 2.334123   (epoch 10)   mode=min
  policy_policy_acc: 0.313115   (epoch 10)   mode=max
  policy_top10_acc: 0.822339   (epoch 10)   mode=max
  policy_top5_acc: 0.667879   (epoch 10)   mode=max
  val_loss: 2.595838   (epoch 4)   mode=min
  val_policy_loss: 2.497087   (epoch 4)   mode=min
  val_policy_policy_acc: 0.291109   (epoch 10)   mode=max
  val_policy_top10_acc: 0.781818   (epoch 10)   mode=max
  val_policy_top5_acc: 0.625475   (epoch 9)   mode=max
  val_value_loss: 0.197296   (epoch 4)   mode=min
  val_value_value_mse: 0.197313   (epoch 4)   mode=min
  value_loss: 0.194850   (epoch 9)   mode=min
  value_value_mse: 0.194874   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6708, 2.6327, 2.6018, 2.5448, 2.5207, 2.5037, 2.4710, 2.4571, 2.4395, 2.4319]
  policy_loss : [2.5663, 2.5293, 2.5001, 2.4441, 2.4207, 2.4045, 2.3725, 2.3594, 2.3423, 2.3341]
  policy_policy_acc : [0.2677, 0.2744, 0.2809, 0.2917, 0.2948, 0.2999, 0.3058, 0.3096, 0.3118, 0.3131]
  policy_top10_acc : [0.7726, 0.7815, 0.7881, 0.7987, 0.8036, 0.8080, 0.8139, 0.8177, 0.8201, 0.8223]
  policy_top5_acc : [0.6063, 0.6179, 0.6256, 0.6397, 0.6461, 0.6505, 0.6584, 0.6616, 0.6670, 0.6679]
  val_loss : [2.6080, 2.6170, 2.6166, 2.5958, 2.6304, 2.6166, 2.7092, 2.7198, 2.6427, 2.6797]
  val_policy_loss : [2.5045, 2.5161, 2.5177, 2.4971, 2.5311, 2.5169, 2.6094, 2.6205, 2.5434, 2.5803]
  val_policy_policy_acc : [0.2835, 0.2797, 0.2819, 0.2873, 0.2858, 0.2819, 0.2864, 0.2901, 0.2906, 0.2911]
  val_policy_top10_acc : [0.7761, 0.7748, 0.7757, 0.7798, 0.7755, 0.7750, 0.7806, 0.7803, 0.7814, 0.7818]
  val_policy_top5_acc : [0.6168, 0.6120, 0.6129, 0.6213, 0.6175, 0.6173, 0.6225, 0.6247, 0.6255, 0.6253]
  val_value_loss : [0.2067, 0.2015, 0.1976, 0.1973, 0.1984, 0.1994, 0.1994, 0.1982, 0.1983, 0.1986]
  val_value_value_mse : [0.2067, 0.2015, 0.1976, 0.1973, 0.1984, 0.1994, 0.1994, 0.1982, 0.1983, 0.1986]
  value_loss : [0.2095, 0.2070, 0.2039, 0.2015, 0.1995, 0.1986, 0.1970, 0.1960, 0.1948, 0.1954]
  value_value_mse : [0.2095, 0.2070, 0.2038, 0.2015, 0.1995, 0.1986, 0.1970, 0.1960, 0.1949, 0.1953]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:25:56.946846Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game67501_game69000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.435324   (epoch 8)   mode=min
  policy_loss: 2.339409   (epoch 8)   mode=min
  policy_policy_acc: 0.313156   (epoch 8)   mode=max
  policy_top10_acc: 0.819393   (epoch 8)   mode=max
  policy_top5_acc: 0.665285   (epoch 8)   mode=max
  val_loss: 2.609977   (epoch 2)   mode=min
  val_policy_loss: 2.510642   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 8)   mode=max
  val_policy_top10_acc: 0.782118   (epoch 8)   mode=max
  val_policy_top5_acc: 0.619780   (epoch 7)   mode=max
  val_value_loss: 0.195533   (epoch 6)   mode=min
  val_value_value_mse: 0.195539   (epoch 6)   mode=min
  value_loss: 0.191664   (epoch 8)   mode=min
  value_value_mse: 0.191669   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6640, 2.6221, 2.5934, 2.5620, 2.5061, 2.4807, 2.4492, 2.4353]
  policy_loss : [2.5611, 2.5205, 2.4927, 2.4621, 2.4081, 2.3835, 2.3527, 2.3394]
  policy_policy_acc : [0.2692, 0.2764, 0.2808, 0.2880, 0.2987, 0.3032, 0.3116, 0.3132]
  policy_top10_acc : [0.7706, 0.7793, 0.7871, 0.7927, 0.8058, 0.8096, 0.8159, 0.8194]
  policy_top5_acc : [0.6069, 0.6166, 0.6268, 0.6342, 0.6467, 0.6550, 0.6606, 0.6653]
  val_loss : [2.7811, 2.6100, 2.6222, 3.0673, 2.7273, 2.8773, 2.7783, 2.7329]
  val_policy_loss : [2.6789, 2.5106, 2.5226, 2.9663, 2.6279, 2.7793, 2.6796, 2.6349]
  val_policy_policy_acc : [0.2739, 0.2763, 0.2806, 0.2764, 0.2826, 0.2828, 0.2852, 0.2857]
  val_policy_top10_acc : [0.7746, 0.7723, 0.7755, 0.7758, 0.7790, 0.7773, 0.7806, 0.7821]
  val_policy_top5_acc : [0.6097, 0.6162, 0.6139, 0.6097, 0.6192, 0.6167, 0.6198, 0.6196]
  val_value_loss : [0.2042, 0.1986, 0.1991, 0.2015, 0.1986, 0.1955, 0.1972, 0.1957]
  val_value_value_mse : [0.2042, 0.1986, 0.1991, 0.2015, 0.1986, 0.1955, 0.1972, 0.1957]
  value_loss : [0.2057, 0.2031, 0.2012, 0.1998, 0.1960, 0.1946, 0.1926, 0.1917]
  value_value_mse : [0.2057, 0.2031, 0.2012, 0.1998, 0.1960, 0.1946, 0.1926, 0.1917]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:34:47.661698Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game69001_game70500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.419210   (epoch 9)   mode=min
  policy_loss: 2.324536   (epoch 9)   mode=min
  policy_policy_acc: 0.316322   (epoch 9)   mode=max
  policy_top10_acc: 0.821965   (epoch 9)   mode=max
  policy_top5_acc: 0.671537   (epoch 9)   mode=max
  val_loss: 2.645575   (epoch 3)   mode=min
  val_policy_loss: 2.546987   (epoch 3)   mode=min
  val_policy_policy_acc: 0.277323   (epoch 9)   mode=max
  val_policy_top10_acc: 0.779321   (epoch 9)   mode=max
  val_policy_top5_acc: 0.620979   (epoch 7)   mode=max
  val_value_loss: 0.197150   (epoch 3)   mode=min
  val_value_value_mse: 0.197163   (epoch 3)   mode=min
  value_loss: 0.189228   (epoch 9)   mode=min
  value_value_mse: 0.189225   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6763, 2.6312, 2.6019, 2.5748, 2.5489, 2.4886, 2.4643, 2.4304, 2.4192]
  policy_loss : [2.5738, 2.5304, 2.5016, 2.4756, 2.4503, 2.3919, 2.3683, 2.3356, 2.3245]
  policy_policy_acc : [0.2685, 0.2738, 0.2796, 0.2859, 0.2893, 0.3033, 0.3075, 0.3150, 0.3163]
  policy_top10_acc : [0.7691, 0.7797, 0.7856, 0.7913, 0.7970, 0.8088, 0.8153, 0.8196, 0.8220]
  policy_top5_acc : [0.6064, 0.6182, 0.6263, 0.6335, 0.6397, 0.6546, 0.6613, 0.6678, 0.6715]
  val_loss : [3.2918, 2.6741, 2.6456, 2.9415, 3.4645, 3.8748, 3.8298, 3.9716, 3.6507]
  val_policy_loss : [3.1918, 2.5746, 2.5470, 2.8414, 3.3641, 3.7747, 3.7294, 3.8720, 3.5512]
  val_policy_policy_acc : [0.2698, 0.2738, 0.2716, 0.2722, 0.2708, 0.2754, 0.2772, 0.2751, 0.2773]
  val_policy_top10_acc : [0.7686, 0.7779, 0.7739, 0.7716, 0.7735, 0.7781, 0.7784, 0.7774, 0.7793]
  val_policy_top5_acc : [0.6075, 0.6157, 0.6094, 0.6175, 0.6154, 0.6135, 0.6210, 0.6174, 0.6166]
  val_value_loss : [0.1990, 0.1989, 0.1972, 0.1998, 0.1997, 0.1986, 0.1994, 0.1975, 0.1976]
  val_value_value_mse : [0.1990, 0.1990, 0.1972, 0.1998, 0.1997, 0.1986, 0.1994, 0.1975, 0.1976]
  value_loss : [0.2049, 0.2018, 0.2006, 0.1984, 0.1973, 0.1934, 0.1921, 0.1894, 0.1892]
  value_value_mse : [0.2049, 0.2018, 0.2006, 0.1984, 0.1973, 0.1934, 0.1921, 0.1894, 0.1892]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:44:25.981553Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game70501_game72000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.415363   (epoch 10)   mode=min
  policy_loss: 2.321579   (epoch 10)   mode=min
  policy_policy_acc: 0.317374   (epoch 10)   mode=max
  policy_top10_acc: 0.821038   (epoch 10)   mode=max
  policy_top5_acc: 0.671315   (epoch 10)   mode=max
  val_loss: 2.623133   (epoch 4)   mode=min
  val_policy_loss: 2.524983   (epoch 4)   mode=min
  val_policy_policy_acc: 0.289510   (epoch 7)   mode=max
  val_policy_top10_acc: 0.780619   (epoch 7)   mode=max
  val_policy_top5_acc: 0.624176   (epoch 9)   mode=max
  val_value_loss: 0.196231   (epoch 4)   mode=min
  val_value_value_mse: 0.196254   (epoch 4)   mode=min
  value_loss: 0.187537   (epoch 10)   mode=min
  value_value_mse: 0.187543   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6598, 2.6164, 2.5844, 2.5235, 2.5030, 2.4854, 2.4520, 2.4392, 2.4238, 2.4154]
  policy_loss : [2.5581, 2.5164, 2.4851, 2.4263, 2.4069, 2.3896, 2.3570, 2.3445, 2.3298, 2.3216]
  policy_policy_acc : [0.2699, 0.2788, 0.2853, 0.2956, 0.2986, 0.3032, 0.3102, 0.3125, 0.3159, 0.3174]
  policy_top10_acc : [0.7718, 0.7804, 0.7880, 0.8011, 0.8048, 0.8084, 0.8151, 0.8167, 0.8198, 0.8210]
  policy_top5_acc : [0.6103, 0.6211, 0.6303, 0.6460, 0.6513, 0.6552, 0.6613, 0.6642, 0.6690, 0.6713]
  val_loss : [2.6467, 2.7276, 2.7283, 2.6231, 2.6490, 2.7864, 2.6895, 2.8517, 2.7743, 2.8831]
  val_policy_loss : [2.5479, 2.6290, 2.6294, 2.5250, 2.5500, 2.6865, 2.5907, 2.7528, 2.6756, 2.7837]
  val_policy_policy_acc : [0.2801, 0.2756, 0.2798, 0.2858, 0.2887, 0.2859, 0.2895, 0.2866, 0.2893, 0.2874]
  val_policy_top10_acc : [0.7766, 0.7798, 0.7799, 0.7795, 0.7793, 0.7785, 0.7806, 0.7801, 0.7799, 0.7788]
  val_policy_top5_acc : [0.6128, 0.6155, 0.6160, 0.6198, 0.6187, 0.6210, 0.6193, 0.6225, 0.6242, 0.6224]
  val_value_loss : [0.1975, 0.1970, 0.1977, 0.1962, 0.1978, 0.1993, 0.1975, 0.1975, 0.1971, 0.1984]
  val_value_value_mse : [0.1975, 0.1970, 0.1977, 0.1963, 0.1978, 0.1994, 0.1975, 0.1975, 0.1971, 0.1984]
  value_loss : [0.2034, 0.1999, 0.1985, 0.1944, 0.1925, 0.1915, 0.1901, 0.1894, 0.1882, 0.1875]
  value_value_mse : [0.2034, 0.1999, 0.1985, 0.1944, 0.1925, 0.1915, 0.1901, 0.1894, 0.1882, 0.1875]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T03:52:04.074353Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game72001_game73500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.417403   (epoch 8)   mode=min
  policy_loss: 2.319100   (epoch 8)   mode=min
  policy_policy_acc: 0.318094   (epoch 8)   mode=max
  policy_top10_acc: 0.821586   (epoch 8)   mode=max
  policy_top5_acc: 0.667999   (epoch 8)   mode=max
  val_loss: 2.609621   (epoch 2)   mode=min
  val_policy_loss: 2.511571   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283017   (epoch 5)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 7)   mode=max
  val_policy_top5_acc: 0.618182   (epoch 7)   mode=max
  val_value_loss: 0.195981   (epoch 2)   mode=min
  val_value_value_mse: 0.195995   (epoch 2)   mode=min
  value_loss: 0.196635   (epoch 8)   mode=min
  value_value_mse: 0.196635   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6461, 2.6057, 2.5728, 2.5458, 2.4872, 2.4663, 2.4329, 2.4174]
  policy_loss : [2.5410, 2.5015, 2.4699, 2.4437, 2.3867, 2.3665, 2.3342, 2.3191]
  policy_policy_acc : [0.2729, 0.2819, 0.2869, 0.2942, 0.3043, 0.3082, 0.3146, 0.3181]
  policy_top10_acc : [0.7753, 0.7840, 0.7919, 0.7982, 0.8091, 0.8133, 0.8201, 0.8216]
  policy_top5_acc : [0.6117, 0.6231, 0.6307, 0.6380, 0.6535, 0.6582, 0.6647, 0.6680]
  val_loss : [2.6176, 2.6096, 2.6149, 2.6244, 2.6420, 2.6923, 2.9206, 2.7739]
  val_policy_loss : [2.5190, 2.5116, 2.5149, 2.5243, 2.5435, 2.5926, 2.8218, 2.6752]
  val_policy_policy_acc : [0.2798, 0.2802, 0.2766, 0.2790, 0.2830, 0.2778, 0.2828, 0.2826]
  val_policy_top10_acc : [0.7704, 0.7777, 0.7805, 0.7780, 0.7796, 0.7770, 0.7832, 0.7796]
  val_policy_top5_acc : [0.6129, 0.6161, 0.6108, 0.6160, 0.6154, 0.6170, 0.6182, 0.6166]
  val_value_loss : [0.1972, 0.1960, 0.1999, 0.2001, 0.1968, 0.1992, 0.1971, 0.1973]
  val_value_value_mse : [0.1972, 0.1960, 0.1999, 0.2002, 0.1968, 0.1992, 0.1972, 0.1973]
  value_loss : [0.2104, 0.2084, 0.2057, 0.2041, 0.2009, 0.1995, 0.1974, 0.1966]
  value_value_mse : [0.2104, 0.2084, 0.2057, 0.2041, 0.2009, 0.1995, 0.1974, 0.1966]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:01:51.800987Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game73501_game75000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.375446   (epoch 10)   mode=min
  policy_loss: 2.281324   (epoch 10)   mode=min
  policy_policy_acc: 0.326641   (epoch 10)   mode=max
  policy_top10_acc: 0.831163   (epoch 10)   mode=max
  policy_top5_acc: 0.680335   (epoch 10)   mode=max
  val_loss: 2.612954   (epoch 4)   mode=min
  val_policy_loss: 2.513766   (epoch 4)   mode=min
  val_policy_policy_acc: 0.285914   (epoch 8)   mode=max
  val_policy_top10_acc: 0.781119   (epoch 4)   mode=max
  val_policy_top5_acc: 0.622877   (epoch 9)   mode=max
  val_value_loss: 0.196236   (epoch 3)   mode=min
  val_value_value_mse: 0.196246   (epoch 3)   mode=min
  value_loss: 0.188250   (epoch 10)   mode=min
  value_value_mse: 0.188249   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6477, 2.6051, 2.5728, 2.5481, 2.5217, 2.5017, 2.4424, 2.4219, 2.3895, 2.3754]
  policy_loss : [2.5457, 2.5044, 2.4728, 2.4487, 2.4235, 2.4043, 2.3463, 2.3266, 2.2948, 2.2813]
  policy_policy_acc : [0.2723, 0.2830, 0.2885, 0.2914, 0.2970, 0.3019, 0.3137, 0.3191, 0.3239, 0.3266]
  policy_top10_acc : [0.7767, 0.7856, 0.7933, 0.7990, 0.8041, 0.8081, 0.8190, 0.8238, 0.8290, 0.8312]
  policy_top5_acc : [0.6114, 0.6238, 0.6304, 0.6383, 0.6453, 0.6505, 0.6632, 0.6700, 0.6782, 0.6803]
  val_loss : [2.6278, 2.6215, 2.6233, 2.6130, 2.6515, 2.6338, 2.6562, 2.6590, 2.7060, 2.7575]
  val_policy_loss : [2.5281, 2.5222, 2.5251, 2.5138, 2.5519, 2.5334, 2.5565, 2.5599, 2.6071, 2.6585]
  val_policy_policy_acc : [0.2771, 0.2799, 0.2762, 0.2782, 0.2808, 0.2808, 0.2824, 0.2859, 0.2854, 0.2839]
  val_policy_top10_acc : [0.7782, 0.7805, 0.7780, 0.7811, 0.7733, 0.7758, 0.7772, 0.7784, 0.7802, 0.7777]
  val_policy_top5_acc : [0.6167, 0.6147, 0.6112, 0.6183, 0.6135, 0.6118, 0.6170, 0.6214, 0.6229, 0.6211]
  val_value_loss : [0.1994, 0.1985, 0.1962, 0.1983, 0.1992, 0.2006, 0.1992, 0.1982, 0.1975, 0.1977]
  val_value_value_mse : [0.1994, 0.1985, 0.1962, 0.1983, 0.1993, 0.2006, 0.1992, 0.1982, 0.1975, 0.1977]
  value_loss : [0.2040, 0.2014, 0.2000, 0.1989, 0.1962, 0.1948, 0.1922, 0.1905, 0.1894, 0.1882]
  value_value_mse : [0.2040, 0.2014, 0.2000, 0.1989, 0.1962, 0.1948, 0.1922, 0.1905, 0.1894, 0.1882]

================================================================================

History file: model_versions/chess_elo_model_V16_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:09:47.425662Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game75001_game76500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V15
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.426363   (epoch 8)   mode=min
  policy_loss: 2.331096   (epoch 8)   mode=min
  policy_policy_acc: 0.314126   (epoch 8)   mode=max
  policy_top10_acc: 0.822830   (epoch 8)   mode=max
  policy_top5_acc: 0.667788   (epoch 8)   mode=max
  val_loss: 2.609546   (epoch 2)   mode=min
  val_policy_loss: 2.510911   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286613   (epoch 8)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 5)   mode=max
  val_policy_top5_acc: 0.622777   (epoch 8)   mode=max
  val_value_loss: 0.196864   (epoch 5)   mode=min
  val_value_value_mse: 0.196881   (epoch 5)   mode=min
  value_loss: 0.190557   (epoch 8)   mode=min
  value_value_mse: 0.190561   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6545, 2.6096, 2.5761, 2.5529, 2.4905, 2.4725, 2.4373, 2.4264]
  policy_loss : [2.5530, 2.5091, 2.4764, 2.4541, 2.3936, 2.3757, 2.3417, 2.3311]
  policy_policy_acc : [0.2708, 0.2811, 0.2855, 0.2902, 0.3044, 0.3062, 0.3135, 0.3141]
  policy_top10_acc : [0.7723, 0.7835, 0.7929, 0.7976, 0.8094, 0.8138, 0.8194, 0.8228]
  policy_top5_acc : [0.6095, 0.6224, 0.6309, 0.6369, 0.6523, 0.6571, 0.6661, 0.6678]
  val_loss : [2.6154, 2.6095, 2.6255, 2.6995, 2.7449, 2.8990, 2.7665, 3.0462]
  val_policy_loss : [2.5166, 2.5109, 2.5253, 2.6001, 2.6462, 2.7992, 2.6676, 2.9474]
  val_policy_policy_acc : [0.2758, 0.2768, 0.2767, 0.2781, 0.2830, 0.2813, 0.2813, 0.2866]
  val_policy_top10_acc : [0.7756, 0.7779, 0.7745, 0.7730, 0.7814, 0.7768, 0.7810, 0.7804]
  val_policy_top5_acc : [0.6123, 0.6174, 0.6162, 0.6079, 0.6216, 0.6186, 0.6226, 0.6228]
  val_value_loss : [0.1975, 0.1971, 0.2003, 0.1986, 0.1969, 0.1990, 0.1974, 0.1969]
  val_value_value_mse : [0.1975, 0.1971, 0.2004, 0.1986, 0.1969, 0.1990, 0.1974, 0.1969]
  value_loss : [0.2030, 0.2011, 0.1994, 0.1976, 0.1939, 0.1935, 0.1912, 0.1906]
  value_value_mse : [0.2030, 0.2010, 0.1994, 0.1976, 0.1939, 0.1935, 0.1912, 0.1906]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:17:50.049112Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game76501_game78000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.419172   (epoch 8)   mode=min
  policy_loss: 2.321163   (epoch 8)   mode=min
  policy_policy_acc: 0.319311   (epoch 8)   mode=max
  policy_top10_acc: 0.824658   (epoch 8)   mode=max
  policy_top5_acc: 0.671633   (epoch 8)   mode=max
  val_loss: 2.610785   (epoch 2)   mode=min
  val_policy_loss: 2.510227   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 7)   mode=max
  val_policy_top10_acc: 0.780120   (epoch 7)   mode=max
  val_policy_top5_acc: 0.625375   (epoch 8)   mode=max
  val_value_loss: 0.197293   (epoch 1)   mode=min
  val_value_value_mse: 0.197312   (epoch 1)   mode=min
  value_loss: 0.195990   (epoch 8)   mode=min
  value_value_mse: 0.195985   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6477, 2.6078, 2.5771, 2.5493, 2.4904, 2.4660, 2.4341, 2.4192]
  policy_loss : [2.5436, 2.5048, 2.4749, 2.4480, 2.3908, 2.3672, 2.3359, 2.3212]
  policy_policy_acc : [0.2730, 0.2828, 0.2867, 0.2931, 0.3055, 0.3098, 0.3179, 0.3193]
  policy_top10_acc : [0.7758, 0.7850, 0.7919, 0.7984, 0.8106, 0.8140, 0.8211, 0.8247]
  policy_top5_acc : [0.6136, 0.6231, 0.6326, 0.6397, 0.6534, 0.6593, 0.6674, 0.6716]
  val_loss : [2.6216, 2.6108, 2.6230, 2.6421, 2.7021, 2.7306, 2.6989, 2.8642]
  val_policy_loss : [2.5229, 2.5102, 2.5227, 2.5419, 2.6023, 2.6318, 2.6001, 2.7652]
  val_policy_policy_acc : [0.2748, 0.2827, 0.2827, 0.2770, 0.2783, 0.2808, 0.2851, 0.2827]
  val_policy_top10_acc : [0.7785, 0.7791, 0.7781, 0.7726, 0.7759, 0.7786, 0.7801, 0.7777]
  val_policy_top5_acc : [0.6187, 0.6194, 0.6141, 0.6123, 0.6176, 0.6233, 0.6233, 0.6254]
  val_value_loss : [0.1973, 0.2010, 0.2005, 0.2003, 0.1993, 0.1973, 0.1975, 0.1975]
  val_value_value_mse : [0.1973, 0.2010, 0.2005, 0.2003, 0.1993, 0.1974, 0.1975, 0.1975]
  value_loss : [0.2081, 0.2062, 0.2043, 0.2029, 0.1993, 0.1978, 0.1966, 0.1960]
  value_value_mse : [0.2081, 0.2062, 0.2043, 0.2029, 0.1993, 0.1978, 0.1966, 0.1960]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:27:56.300303Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game78001_game79500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.428366   (epoch 10)   mode=min
  policy_loss: 2.333509   (epoch 10)   mode=min
  policy_policy_acc: 0.315971   (epoch 10)   mode=max
  policy_top10_acc: 0.823181   (epoch 10)   mode=max
  policy_top5_acc: 0.668467   (epoch 10)   mode=max
  val_loss: 2.602798   (epoch 4)   mode=min
  val_policy_loss: 2.503908   (epoch 7)   mode=min
  val_policy_policy_acc: 0.287912   (epoch 9)   mode=max
  val_policy_top10_acc: 0.781618   (epoch 10)   mode=max
  val_policy_top5_acc: 0.626973   (epoch 10)   mode=max
  val_value_loss: 0.195713   (epoch 1)   mode=min
  val_value_value_mse: 0.195716   (epoch 1)   mode=min
  value_loss: 0.189709   (epoch 10)   mode=min
  value_value_mse: 0.189711   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001]
  loss : [2.6733, 2.6338, 2.6032, 2.5422, 2.5201, 2.5010, 2.4692, 2.4577, 2.4472, 2.4284]
  policy_loss : [2.5709, 2.5329, 2.5035, 2.4445, 2.4229, 2.4041, 2.3733, 2.3621, 2.3519, 2.3335]
  policy_policy_acc : [0.2677, 0.2765, 0.2804, 0.2942, 0.2981, 0.3021, 0.3081, 0.3103, 0.3119, 0.3160]
  policy_top10_acc : [0.7700, 0.7797, 0.7857, 0.7995, 0.8029, 0.8080, 0.8137, 0.8159, 0.8183, 0.8232]
  policy_top5_acc : [0.6040, 0.6145, 0.6238, 0.6373, 0.6442, 0.6482, 0.6569, 0.6598, 0.6625, 0.6685]
  val_loss : [2.6109, 2.6260, 2.6156, 2.6028, 2.6062, 2.6180, 2.6028, 2.6099, 2.6083, 2.6127]
  val_policy_loss : [2.5129, 2.5262, 2.5158, 2.5045, 2.5075, 2.5181, 2.5039, 2.5106, 2.5089, 2.5136]
  val_policy_policy_acc : [0.2800, 0.2786, 0.2812, 0.2867, 0.2854, 0.2833, 0.2870, 0.2870, 0.2879, 0.2878]
  val_policy_top10_acc : [0.7765, 0.7732, 0.7755, 0.7787, 0.7800, 0.7788, 0.7811, 0.7815, 0.7815, 0.7816]
  val_policy_top5_acc : [0.6172, 0.6142, 0.6171, 0.6246, 0.6234, 0.6223, 0.6237, 0.6242, 0.6246, 0.6270]
  val_value_loss : [0.1957, 0.1993, 0.1991, 0.1963, 0.1972, 0.1995, 0.1975, 0.1983, 0.1984, 0.1980]
  val_value_value_mse : [0.1957, 0.1993, 0.1991, 0.1963, 0.1972, 0.1995, 0.1975, 0.1984, 0.1985, 0.1980]
  value_loss : [0.2047, 0.2018, 0.1994, 0.1956, 0.1947, 0.1936, 0.1918, 0.1913, 0.1904, 0.1897]
  value_value_mse : [0.2047, 0.2018, 0.1994, 0.1956, 0.1947, 0.1936, 0.1918, 0.1913, 0.1905, 0.1897]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:34:59.065137Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game79501_game81000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.440662   (epoch 7)   mode=min
  policy_loss: 2.347506   (epoch 7)   mode=min
  policy_policy_acc: 0.311685   (epoch 7)   mode=max
  policy_top10_acc: 0.817702   (epoch 7)   mode=max
  policy_top5_acc: 0.665761   (epoch 7)   mode=max
  val_loss: 2.627173   (epoch 1)   mode=min
  val_policy_loss: 2.527414   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283916   (epoch 6)   mode=max
  val_policy_top10_acc: 0.787313   (epoch 7)   mode=max
  val_policy_top5_acc: 0.624076   (epoch 7)   mode=max
  val_value_loss: 0.197546   (epoch 7)   mode=min
  val_value_value_mse: 0.197558   (epoch 7)   mode=min
  value_loss: 0.186191   (epoch 7)   mode=min
  value_value_mse: 0.186156   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6439, 2.6019, 2.5680, 2.5078, 2.4872, 2.4556, 2.4407]
  policy_loss : [2.5438, 2.5034, 2.4709, 2.4127, 2.3927, 2.3616, 2.3475]
  policy_policy_acc : [0.2710, 0.2816, 0.2855, 0.2986, 0.3025, 0.3091, 0.3117]
  policy_top10_acc : [0.7745, 0.7831, 0.7928, 0.8051, 0.8105, 0.8159, 0.8177]
  policy_top5_acc : [0.6133, 0.6242, 0.6335, 0.6482, 0.6543, 0.6605, 0.6658]
  val_loss : [2.6272, 2.6960, 2.8031, 3.2464, 3.1909, 3.5868, 3.6537]
  val_policy_loss : [2.5274, 2.5962, 2.7026, 3.1470, 3.0904, 3.4872, 3.5541]
  val_policy_policy_acc : [0.2754, 0.2783, 0.2795, 0.2820, 0.2825, 0.2839, 0.2802]
  val_policy_top10_acc : [0.7772, 0.7794, 0.7774, 0.7815, 0.7822, 0.7836, 0.7873]
  val_policy_top5_acc : [0.6116, 0.6134, 0.6144, 0.6219, 0.6205, 0.6220, 0.6241]
  val_value_loss : [0.1994, 0.1995, 0.2005, 0.1978, 0.2001, 0.1978, 0.1975]
  val_value_value_mse : [0.1994, 0.1995, 0.2005, 0.1978, 0.2002, 0.1978, 0.1976]
  value_loss : [0.1997, 0.1965, 0.1946, 0.1905, 0.1893, 0.1879, 0.1862]
  value_value_mse : [0.1997, 0.1965, 0.1946, 0.1905, 0.1893, 0.1879, 0.1862]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:43:02.434575Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game81001_game82500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.427481   (epoch 8)   mode=min
  policy_loss: 2.336509   (epoch 8)   mode=min
  policy_policy_acc: 0.312778   (epoch 8)   mode=max
  policy_top10_acc: 0.820918   (epoch 8)   mode=max
  policy_top5_acc: 0.667036   (epoch 8)   mode=max
  val_loss: 2.603075   (epoch 2)   mode=min
  val_policy_loss: 2.503074   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783616   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627972   (epoch 7)   mode=max
  val_value_loss: 0.197088   (epoch 1)   mode=min
  val_value_value_mse: 0.197098   (epoch 1)   mode=min
  value_loss: 0.181787   (epoch 8)   mode=min
  value_value_mse: 0.181790   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6556, 2.6128, 2.5841, 2.5558, 2.4957, 2.4735, 2.4392, 2.4275]
  policy_loss : [2.5567, 2.5157, 2.4876, 2.4612, 2.4028, 2.3814, 2.3477, 2.3365]
  policy_policy_acc : [0.2688, 0.2768, 0.2829, 0.2866, 0.2985, 0.3032, 0.3108, 0.3128]
  policy_top10_acc : [0.7708, 0.7817, 0.7874, 0.7944, 0.8061, 0.8104, 0.8177, 0.8209]
  policy_top5_acc : [0.6068, 0.6205, 0.6263, 0.6351, 0.6494, 0.6549, 0.6637, 0.6670]
  val_loss : [2.6167, 2.6031, 2.6166, 2.6142, 2.6122, 2.6179, 2.6100, 2.6161]
  val_policy_loss : [2.5180, 2.5031, 2.5163, 2.5150, 2.5134, 2.5190, 2.5110, 2.5168]
  val_policy_policy_acc : [0.2761, 0.2833, 0.2803, 0.2821, 0.2817, 0.2827, 0.2857, 0.2841]
  val_policy_top10_acc : [0.7790, 0.7836, 0.7820, 0.7820, 0.7817, 0.7795, 0.7788, 0.7795]
  val_policy_top5_acc : [0.6202, 0.6232, 0.6207, 0.6249, 0.6193, 0.6230, 0.6280, 0.6252]
  val_value_loss : [0.1971, 0.1998, 0.2002, 0.1982, 0.1973, 0.1976, 0.1977, 0.1982]
  val_value_value_mse : [0.1971, 0.1998, 0.2003, 0.1982, 0.1973, 0.1976, 0.1977, 0.1982]
  value_loss : [0.1980, 0.1942, 0.1927, 0.1896, 0.1860, 0.1844, 0.1828, 0.1818]
  value_value_mse : [0.1980, 0.1942, 0.1927, 0.1896, 0.1860, 0.1844, 0.1828, 0.1818]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T04:53:12.443510Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game82501_game84000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.417941   (epoch 10)   mode=min
  policy_loss: 2.326667   (epoch 10)   mode=min
  policy_policy_acc: 0.316371   (epoch 10)   mode=max
  policy_top10_acc: 0.821995   (epoch 10)   mode=max
  policy_top5_acc: 0.666989   (epoch 10)   mode=max
  val_loss: 2.611007   (epoch 5)   mode=min
  val_policy_loss: 2.513113   (epoch 5)   mode=min
  val_policy_policy_acc: 0.286613   (epoch 10)   mode=max
  val_policy_top10_acc: 0.782617   (epoch 7)   mode=max
  val_policy_top5_acc: 0.627672   (epoch 10)   mode=max
  val_value_loss: 0.195879   (epoch 5)   mode=min
  val_value_value_mse: 0.195891   (epoch 5)   mode=min
  value_loss: 0.184203   (epoch 10)   mode=min
  value_value_mse: 0.184198   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6565, 2.6185, 2.5887, 2.5293, 2.5092, 2.4926, 2.4762, 2.4431, 2.4319, 2.4179]
  policy_loss : [2.5576, 2.5220, 2.4920, 2.4346, 2.4148, 2.4001, 2.3829, 2.3503, 2.3406, 2.3267]
  policy_policy_acc : [0.2670, 0.2752, 0.2822, 0.2935, 0.2967, 0.2997, 0.3022, 0.3098, 0.3133, 0.3164]
  policy_top10_acc : [0.7725, 0.7803, 0.7885, 0.8004, 0.8047, 0.8085, 0.8119, 0.8173, 0.8199, 0.8220]
  policy_top5_acc : [0.6063, 0.6152, 0.6248, 0.6403, 0.6440, 0.6493, 0.6539, 0.6619, 0.6632, 0.6670]
  val_loss : [2.6478, 2.7407, 2.6820, 2.6351, 2.6110, 2.7801, 2.9586, 2.9479, 2.9487, 2.7252]
  val_policy_loss : [2.5491, 2.6399, 2.5825, 2.5365, 2.5131, 2.6813, 2.8595, 2.8491, 2.8495, 2.6267]
  val_policy_policy_acc : [0.2790, 0.2765, 0.2781, 0.2814, 0.2838, 0.2845, 0.2847, 0.2843, 0.2842, 0.2866]
  val_policy_top10_acc : [0.7730, 0.7724, 0.7773, 0.7752, 0.7776, 0.7790, 0.7826, 0.7794, 0.7791, 0.7796]
  val_policy_top5_acc : [0.6160, 0.6087, 0.6133, 0.6197, 0.6228, 0.6218, 0.6225, 0.6242, 0.6244, 0.6277]
  val_value_loss : [0.1973, 0.2015, 0.1988, 0.1970, 0.1959, 0.1972, 0.1977, 0.1971, 0.1978, 0.1969]
  val_value_value_mse : [0.1973, 0.2015, 0.1988, 0.1970, 0.1959, 0.1972, 0.1977, 0.1971, 0.1978, 0.1969]
  value_loss : [0.1968, 0.1945, 0.1931, 0.1893, 0.1887, 0.1869, 0.1870, 0.1853, 0.1842, 0.1842]
  value_value_mse : [0.1968, 0.1945, 0.1931, 0.1892, 0.1887, 0.1869, 0.1869, 0.1852, 0.1842, 0.1842]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:00:07.290770Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game84001_game85500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.454046   (epoch 7)   mode=min
  policy_loss: 2.363244   (epoch 7)   mode=min
  policy_policy_acc: 0.308745   (epoch 7)   mode=max
  policy_top10_acc: 0.815049   (epoch 7)   mode=max
  policy_top5_acc: 0.659036   (epoch 7)   mode=max
  val_loss: 2.602159   (epoch 1)   mode=min
  val_policy_loss: 2.502871   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286613   (epoch 5)   mode=max
  val_policy_top10_acc: 0.782917   (epoch 4)   mode=max
  val_policy_top5_acc: 0.626374   (epoch 6)   mode=max
  val_value_loss: 0.194542   (epoch 4)   mode=min
  val_value_value_mse: 0.194561   (epoch 4)   mode=min
  value_loss: 0.181667   (epoch 7)   mode=min
  value_value_mse: 0.181655   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6569, 2.6150, 2.5843, 2.5252, 2.5004, 2.4686, 2.4540]
  policy_loss : [2.5595, 2.5190, 2.4895, 2.4321, 2.4079, 2.3776, 2.3632]
  policy_policy_acc : [0.2684, 0.2751, 0.2811, 0.2935, 0.2990, 0.3048, 0.3087]
  policy_top10_acc : [0.7727, 0.7815, 0.7910, 0.8016, 0.8072, 0.8130, 0.8150]
  policy_top5_acc : [0.6082, 0.6181, 0.6272, 0.6420, 0.6484, 0.6578, 0.6590]
  val_loss : [2.6022, 2.6219, 2.6615, 2.7511, 2.6884, 2.7843, 3.0812]
  val_policy_loss : [2.5029, 2.5228, 2.5614, 2.6537, 2.5894, 2.6869, 2.9824]
  val_policy_policy_acc : [0.2806, 0.2811, 0.2763, 0.2842, 0.2866, 0.2854, 0.2863]
  val_policy_top10_acc : [0.7817, 0.7789, 0.7744, 0.7829, 0.7787, 0.7829, 0.7813]
  val_policy_top5_acc : [0.6155, 0.6181, 0.6145, 0.6259, 0.6199, 0.6264, 0.6242]
  val_value_loss : [0.1984, 0.1980, 0.2000, 0.1945, 0.1980, 0.1946, 0.1968]
  val_value_value_mse : [0.1985, 0.1980, 0.2001, 0.1946, 0.1980, 0.1946, 0.1968]
  value_loss : [0.1944, 0.1922, 0.1897, 0.1863, 0.1848, 0.1820, 0.1817]
  value_value_mse : [0.1944, 0.1922, 0.1897, 0.1863, 0.1848, 0.1820, 0.1817]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:10:15.727337Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game85501_game87000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.404485   (epoch 10)   mode=min
  policy_loss: 2.312086   (epoch 10)   mode=min
  policy_policy_acc: 0.320644   (epoch 10)   mode=max
  policy_top10_acc: 0.824557   (epoch 10)   mode=max
  policy_top5_acc: 0.671471   (epoch 10)   mode=max
  val_loss: 2.700043   (epoch 6)   mode=min
  val_policy_loss: 2.599510   (epoch 6)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 6)   mode=max
  val_policy_top10_acc: 0.781219   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626474   (epoch 6)   mode=max
  val_value_loss: 0.196061   (epoch 2)   mode=min
  val_value_value_mse: 0.196079   (epoch 2)   mode=min
  value_loss: 0.184762   (epoch 10)   mode=min
  value_value_mse: 0.184746   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6596, 2.6161, 2.5847, 2.5604, 2.5022, 2.4813, 2.4641, 2.4493, 2.4163, 2.4045]
  policy_loss : [2.5592, 2.5180, 2.4858, 2.4626, 2.4069, 2.3868, 2.3693, 2.3559, 2.3231, 2.3121]
  policy_policy_acc : [0.2706, 0.2776, 0.2829, 0.2888, 0.3001, 0.3042, 0.3073, 0.3096, 0.3178, 0.3206]
  policy_top10_acc : [0.7703, 0.7811, 0.7882, 0.7930, 0.8058, 0.8093, 0.8132, 0.8159, 0.8234, 0.8246]
  policy_top5_acc : [0.6063, 0.6193, 0.6266, 0.6322, 0.6463, 0.6537, 0.6572, 0.6605, 0.6692, 0.6715]
  val_loss : [2.7552, 2.7402, 3.7837, 2.8074, 2.7840, 2.7000, 3.2671, 2.9061, 3.0519, 3.5203]
  val_policy_loss : [2.6564, 2.6420, 3.6843, 2.7077, 2.6841, 2.5995, 3.1669, 2.8068, 2.9525, 3.4201]
  val_policy_policy_acc : [0.2788, 0.2839, 0.2733, 0.2776, 0.2845, 0.2857, 0.2849, 0.2798, 0.2856, 0.2852]
  val_policy_top10_acc : [0.7797, 0.7812, 0.7741, 0.7749, 0.7808, 0.7775, 0.7798, 0.7777, 0.7787, 0.7769]
  val_policy_top5_acc : [0.6221, 0.6212, 0.6124, 0.6150, 0.6246, 0.6265, 0.6226, 0.6191, 0.6246, 0.6250]
  val_value_loss : [0.1973, 0.1961, 0.1974, 0.1993, 0.1995, 0.2009, 0.1994, 0.1981, 0.1981, 0.1990]
  val_value_value_mse : [0.1973, 0.1961, 0.1974, 0.1993, 0.1996, 0.2009, 0.1994, 0.1981, 0.1981, 0.1991]
  value_loss : [0.2005, 0.1973, 0.1969, 0.1954, 0.1916, 0.1895, 0.1895, 0.1878, 0.1860, 0.1848]
  value_value_mse : [0.2005, 0.1973, 0.1969, 0.1954, 0.1917, 0.1895, 0.1895, 0.1877, 0.1860, 0.1847]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:20:13.047117Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game87001_game88500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.431757   (epoch 10)   mode=min
  policy_loss: 2.339144   (epoch 10)   mode=min
  policy_policy_acc: 0.313877   (epoch 10)   mode=max
  policy_top10_acc: 0.819851   (epoch 10)   mode=max
  policy_top5_acc: 0.664328   (epoch 10)   mode=max
  val_loss: 2.617688   (epoch 6)   mode=min
  val_policy_loss: 2.518810   (epoch 6)   mode=min
  val_policy_policy_acc: 0.287712   (epoch 6)   mode=max
  val_policy_top10_acc: 0.782517   (epoch 9)   mode=max
  val_policy_top5_acc: 0.625974   (epoch 6)   mode=max
  val_value_loss: 0.197483   (epoch 9)   mode=min
  val_value_value_mse: 0.197491   (epoch 9)   mode=min
  value_loss: 0.185276   (epoch 10)   mode=min
  value_value_mse: 0.185276   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6684, 2.6232, 2.5936, 2.5370, 2.5131, 2.4807, 2.4667, 2.4581, 2.4395, 2.4318]
  policy_loss : [2.5685, 2.5249, 2.4956, 2.4413, 2.4180, 2.3866, 2.3734, 2.3649, 2.3467, 2.3391]
  policy_policy_acc : [0.2674, 0.2765, 0.2808, 0.2928, 0.2974, 0.3043, 0.3066, 0.3092, 0.3123, 0.3139]
  policy_top10_acc : [0.7699, 0.7798, 0.7888, 0.7981, 0.8035, 0.8106, 0.8124, 0.8153, 0.8190, 0.8199]
  policy_top5_acc : [0.6061, 0.6168, 0.6242, 0.6386, 0.6458, 0.6526, 0.6593, 0.6580, 0.6643, 0.6643]
  val_loss : [2.6227, 2.6343, 2.7108, 2.8507, 2.7698, 2.6177, 2.8198, 2.6403, 2.9000, 2.9347]
  val_policy_loss : [2.5214, 2.5348, 2.6115, 2.7515, 2.6698, 2.5188, 2.7207, 2.5413, 2.8010, 2.8355]
  val_policy_policy_acc : [0.2799, 0.2834, 0.2785, 0.2843, 0.2856, 0.2877, 0.2859, 0.2852, 0.2859, 0.2873]
  val_policy_top10_acc : [0.7763, 0.7755, 0.7758, 0.7790, 0.7818, 0.7817, 0.7814, 0.7815, 0.7825, 0.7819]
  val_policy_top5_acc : [0.6168, 0.6137, 0.6176, 0.6225, 0.6219, 0.6260, 0.6216, 0.6248, 0.6252, 0.6232]
  val_value_loss : [0.2025, 0.1989, 0.1984, 0.1978, 0.1996, 0.1976, 0.1977, 0.1978, 0.1975, 0.1979]
  val_value_value_mse : [0.2025, 0.1989, 0.1984, 0.1978, 0.1996, 0.1976, 0.1978, 0.1978, 0.1975, 0.1979]
  value_loss : [0.1998, 0.1965, 0.1961, 0.1913, 0.1899, 0.1879, 0.1868, 0.1862, 0.1857, 0.1853]
  value_value_mse : [0.1998, 0.1965, 0.1961, 0.1913, 0.1899, 0.1879, 0.1868, 0.1862, 0.1857, 0.1853]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:27:21.212567Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game88501_game90000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.467802   (epoch 7)   mode=min
  policy_loss: 2.373567   (epoch 7)   mode=min
  policy_policy_acc: 0.307060   (epoch 7)   mode=max
  policy_top10_acc: 0.812512   (epoch 7)   mode=max
  policy_top5_acc: 0.657776   (epoch 7)   mode=max
  val_loss: 2.610321   (epoch 1)   mode=min
  val_policy_loss: 2.510598   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287313   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783916   (epoch 6)   mode=max
  val_policy_top5_acc: 0.625774   (epoch 7)   mode=max
  val_value_loss: 0.196404   (epoch 7)   mode=min
  val_value_value_mse: 0.196406   (epoch 7)   mode=min
  value_loss: 0.188429   (epoch 7)   mode=min
  value_value_mse: 0.188430   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6668, 2.6235, 2.5918, 2.5356, 2.5123, 2.4767, 2.4678]
  policy_loss : [2.5666, 2.5244, 2.4940, 2.4395, 2.4169, 2.3820, 2.3736]
  policy_policy_acc : [0.2653, 0.2738, 0.2804, 0.2908, 0.2978, 0.3037, 0.3071]
  policy_top10_acc : [0.7696, 0.7797, 0.7871, 0.7997, 0.8030, 0.8111, 0.8125]
  policy_top5_acc : [0.6070, 0.6166, 0.6256, 0.6412, 0.6456, 0.6554, 0.6578]
  val_loss : [2.6103, 2.6181, 2.6523, 2.6115, 2.6481, 2.6664, 2.6404]
  val_policy_loss : [2.5106, 2.5193, 2.5532, 2.5131, 2.5484, 2.5681, 2.5421]
  val_policy_policy_acc : [0.2761, 0.2762, 0.2761, 0.2794, 0.2828, 0.2861, 0.2873]
  val_policy_top10_acc : [0.7741, 0.7792, 0.7794, 0.7777, 0.7766, 0.7839, 0.7825]
  val_policy_top5_acc : [0.6164, 0.6178, 0.6126, 0.6194, 0.6162, 0.6236, 0.6258]
  val_value_loss : [0.1994, 0.1975, 0.1982, 0.1969, 0.1993, 0.1965, 0.1964]
  val_value_value_mse : [0.1994, 0.1975, 0.1982, 0.1969, 0.1993, 0.1965, 0.1964]
  value_loss : [0.2002, 0.1982, 0.1956, 0.1922, 0.1908, 0.1894, 0.1884]
  value_value_mse : [0.2002, 0.1982, 0.1956, 0.1922, 0.1908, 0.1894, 0.1884]

================================================================================

History file: model_versions/chess_elo_model_V17_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:36:40.440353Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game90001_game91500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V16
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.414738   (epoch 9)   mode=min
  policy_loss: 2.319817   (epoch 9)   mode=min
  policy_policy_acc: 0.313909   (epoch 9)   mode=max
  policy_top10_acc: 0.826846   (epoch 9)   mode=max
  policy_top5_acc: 0.674318   (epoch 9)   mode=max
  val_loss: 2.608966   (epoch 3)   mode=min
  val_policy_loss: 2.508259   (epoch 3)   mode=min
  val_policy_policy_acc: 0.284316   (epoch 8)   mode=max
  val_policy_top10_acc: 0.782118   (epoch 3)   mode=max
  val_policy_top5_acc: 0.627173   (epoch 7)   mode=max
  val_value_loss: 0.197692   (epoch 1)   mode=min
  val_value_value_mse: 0.197698   (epoch 1)   mode=min
  value_loss: 0.189666   (epoch 9)   mode=min
  value_value_mse: 0.189661   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6549, 2.6162, 2.5884, 2.5598, 2.5375, 2.4817, 2.4609, 2.4272, 2.4147]
  policy_loss : [2.5525, 2.5150, 2.4881, 2.4605, 2.4387, 2.3848, 2.3645, 2.3317, 2.3198]
  policy_policy_acc : [0.2673, 0.2745, 0.2805, 0.2866, 0.2911, 0.3021, 0.3053, 0.3128, 0.3139]
  policy_top10_acc : [0.7739, 0.7839, 0.7907, 0.7963, 0.8021, 0.8134, 0.8172, 0.8235, 0.8268]
  policy_top5_acc : [0.6099, 0.6208, 0.6283, 0.6346, 0.6426, 0.6566, 0.6601, 0.6688, 0.6743]
  val_loss : [2.6132, 2.6104, 2.6090, 2.6199, 2.6192, 2.6104, 2.6226, 2.6337, 2.6235]
  val_policy_loss : [2.5143, 2.5100, 2.5083, 2.5188, 2.5181, 2.5094, 2.5218, 2.5320, 2.5223]
  val_policy_policy_acc : [0.2782, 0.2773, 0.2768, 0.2782, 0.2796, 0.2822, 0.2777, 0.2843, 0.2832]
  val_policy_top10_acc : [0.7764, 0.7793, 0.7821, 0.7719, 0.7757, 0.7806, 0.7772, 0.7785, 0.7778]
  val_policy_top5_acc : [0.6179, 0.6199, 0.6221, 0.6192, 0.6145, 0.6239, 0.6272, 0.6251, 0.6234]
  val_value_loss : [0.1977, 0.2007, 0.2014, 0.2023, 0.2021, 0.2020, 0.2016, 0.2033, 0.2021]
  val_value_value_mse : [0.1977, 0.2007, 0.2014, 0.2023, 0.2021, 0.2020, 0.2016, 0.2033, 0.2021]
  value_loss : [0.2045, 0.2022, 0.2008, 0.1989, 0.1975, 0.1941, 0.1930, 0.1907, 0.1897]
  value_value_mse : [0.2045, 0.2022, 0.2008, 0.1989, 0.1975, 0.1940, 0.1930, 0.1907, 0.1897]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:46:36.267538Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game91501_game93000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.413917   (epoch 10)   mode=min
  policy_loss: 2.317669   (epoch 10)   mode=min
  policy_policy_acc: 0.316273   (epoch 10)   mode=max
  policy_top10_acc: 0.825843   (epoch 10)   mode=max
  policy_top5_acc: 0.670984   (epoch 10)   mode=max
  val_loss: 2.598787   (epoch 4)   mode=min
  val_policy_loss: 2.499878   (epoch 4)   mode=min
  val_policy_policy_acc: 0.288611   (epoch 10)   mode=max
  val_policy_top10_acc: 0.787113   (epoch 9)   mode=max
  val_policy_top5_acc: 0.629371   (epoch 7)   mode=max
  val_value_loss: 0.197729   (epoch 4)   mode=min
  val_value_value_mse: 0.197727   (epoch 4)   mode=min
  value_loss: 0.192367   (epoch 9)   mode=min
  value_value_mse: 0.192367   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6523, 2.6094, 2.5818, 2.5173, 2.4970, 2.4806, 2.4490, 2.4368, 2.4188, 2.4139]
  policy_loss : [2.5486, 2.5069, 2.4806, 2.4178, 2.3983, 2.3827, 2.3520, 2.3399, 2.3226, 2.3177]
  policy_policy_acc : [0.2681, 0.2771, 0.2833, 0.2960, 0.2986, 0.3026, 0.3081, 0.3122, 0.3160, 0.3163]
  policy_top10_acc : [0.7744, 0.7846, 0.7908, 0.8037, 0.8089, 0.8112, 0.8171, 0.8212, 0.8238, 0.8258]
  policy_top5_acc : [0.6113, 0.6217, 0.6302, 0.6468, 0.6508, 0.6554, 0.6630, 0.6666, 0.6700, 0.6710]
  val_loss : [2.6073, 2.8903, 2.9916, 2.5988, 2.6625, 2.6226, 2.6338, 2.7761, 2.8612, 2.9867]
  val_policy_loss : [2.5059, 2.7904, 2.8906, 2.4999, 2.5627, 2.5222, 2.5343, 2.6764, 2.7620, 2.8872]
  val_policy_policy_acc : [0.2784, 0.2779, 0.2738, 0.2845, 0.2815, 0.2833, 0.2857, 0.2864, 0.2885, 0.2886]
  val_policy_top10_acc : [0.7815, 0.7825, 0.7762, 0.7837, 0.7808, 0.7811, 0.7848, 0.7812, 0.7871, 0.7850]
  val_policy_top5_acc : [0.6198, 0.6214, 0.6163, 0.6241, 0.6252, 0.6240, 0.6294, 0.6259, 0.6266, 0.6281]
  val_value_loss : [0.2028, 0.1993, 0.2014, 0.1977, 0.1995, 0.2007, 0.1988, 0.1989, 0.1981, 0.1984]
  val_value_value_mse : [0.2027, 0.1993, 0.2014, 0.1977, 0.1995, 0.2007, 0.1988, 0.1989, 0.1981, 0.1984]
  value_loss : [0.2076, 0.2050, 0.2024, 0.1990, 0.1974, 0.1958, 0.1941, 0.1938, 0.1924, 0.1925]
  value_value_mse : [0.2076, 0.2051, 0.2024, 0.1990, 0.1974, 0.1958, 0.1941, 0.1938, 0.1924, 0.1925]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T05:55:38.897532Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game93001_game94500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.400722   (epoch 9)   mode=min
  policy_loss: 2.304205   (epoch 9)   mode=min
  policy_policy_acc: 0.320386   (epoch 9)   mode=max
  policy_top10_acc: 0.825869   (epoch 9)   mode=max
  policy_top5_acc: 0.675306   (epoch 9)   mode=max
  val_loss: 2.596209   (epoch 3)   mode=min
  val_policy_loss: 2.495043   (epoch 3)   mode=min
  val_policy_policy_acc: 0.287213   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 7)   mode=max
  val_policy_top5_acc: 0.628472   (epoch 8)   mode=max
  val_value_loss: 0.197590   (epoch 6)   mode=min
  val_value_value_mse: 0.197588   (epoch 6)   mode=min
  value_loss: 0.192922   (epoch 9)   mode=min
  value_value_mse: 0.192922   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6425, 2.6050, 2.5735, 2.5449, 2.5245, 2.4684, 2.4472, 2.4105, 2.4007]
  policy_loss : [2.5391, 2.5025, 2.4723, 2.4442, 2.4247, 2.3699, 2.3498, 2.3137, 2.3042]
  policy_policy_acc : [0.2699, 0.2774, 0.2864, 0.2896, 0.2915, 0.3058, 0.3104, 0.3171, 0.3204]
  policy_top10_acc : [0.7764, 0.7847, 0.7911, 0.7986, 0.8029, 0.8132, 0.8188, 0.8239, 0.8259]
  policy_top5_acc : [0.6137, 0.6234, 0.6334, 0.6405, 0.6458, 0.6586, 0.6647, 0.6724, 0.6753]
  val_loss : [2.5980, 2.6157, 2.5962, 2.6016, 2.6620, 2.7413, 2.7686, 2.6153, 2.7604]
  val_policy_loss : [2.4974, 2.5143, 2.4950, 2.5024, 2.5585, 2.6424, 2.6687, 2.5159, 2.6598]
  val_policy_policy_acc : [0.2831, 0.2789, 0.2831, 0.2817, 0.2813, 0.2870, 0.2872, 0.2848, 0.2854]
  val_policy_top10_acc : [0.7814, 0.7771, 0.7801, 0.7802, 0.7754, 0.7813, 0.7830, 0.7813, 0.7815]
  val_policy_top5_acc : [0.6218, 0.6192, 0.6185, 0.6199, 0.6167, 0.6228, 0.6208, 0.6285, 0.6248]
  val_value_loss : [0.2010, 0.2028, 0.2023, 0.1981, 0.2068, 0.1976, 0.1996, 0.1988, 0.2010]
  val_value_value_mse : [0.2010, 0.2028, 0.2022, 0.1981, 0.2068, 0.1976, 0.1996, 0.1988, 0.2010]
  value_loss : [0.2067, 0.2050, 0.2024, 0.2016, 0.1997, 0.1969, 0.1949, 0.1935, 0.1929]
  value_value_mse : [0.2067, 0.2050, 0.2024, 0.2016, 0.1997, 0.1969, 0.1949, 0.1935, 0.1929]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:05:37.606510Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game94501_game96000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.424874   (epoch 10)   mode=min
  policy_loss: 2.328907   (epoch 10)   mode=min
  policy_policy_acc: 0.317082   (epoch 10)   mode=max
  policy_top10_acc: 0.822059   (epoch 10)   mode=max
  policy_top5_acc: 0.667752   (epoch 10)   mode=max
  val_loss: 2.596422   (epoch 4)   mode=min
  val_policy_loss: 2.496305   (epoch 4)   mode=min
  val_policy_policy_acc: 0.286613   (epoch 5)   mode=max
  val_policy_top10_acc: 0.784915   (epoch 7)   mode=max
  val_policy_top5_acc: 0.633666   (epoch 9)   mode=max
  val_value_loss: 0.199481   (epoch 7)   mode=min
  val_value_value_mse: 0.199487   (epoch 7)   mode=min
  value_loss: 0.191897   (epoch 10)   mode=min
  value_value_mse: 0.191895   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6605, 2.6203, 2.5928, 2.5311, 2.5125, 2.4952, 2.4606, 2.4498, 2.4319, 2.4249]
  policy_loss : [2.5577, 2.5183, 2.4919, 2.4322, 2.4141, 2.3971, 2.3636, 2.3530, 2.3359, 2.3289]
  policy_policy_acc : [0.2699, 0.2782, 0.2826, 0.2960, 0.3005, 0.3023, 0.3105, 0.3108, 0.3133, 0.3171]
  policy_top10_acc : [0.7719, 0.7828, 0.7881, 0.8016, 0.8055, 0.8093, 0.8150, 0.8175, 0.8212, 0.8221]
  policy_top5_acc : [0.6082, 0.6206, 0.6259, 0.6410, 0.6471, 0.6516, 0.6591, 0.6615, 0.6668, 0.6678]
  val_loss : [2.6146, 2.6930, 2.6701, 2.5964, 2.7055, 2.7787, 2.7694, 2.8585, 2.7920, 3.0629]
  val_policy_loss : [2.5129, 2.5925, 2.5694, 2.4963, 2.6050, 2.6781, 2.6695, 2.7582, 2.6913, 2.9624]
  val_policy_policy_acc : [0.2797, 0.2814, 0.2838, 0.2850, 0.2866, 0.2830, 0.2852, 0.2845, 0.2857, 0.2863]
  val_policy_top10_acc : [0.7758, 0.7841, 0.7822, 0.7786, 0.7814, 0.7831, 0.7849, 0.7849, 0.7844, 0.7849]
  val_policy_top5_acc : [0.6149, 0.6171, 0.6216, 0.6248, 0.6245, 0.6256, 0.6313, 0.6321, 0.6337, 0.6303]
  val_value_loss : [0.2034, 0.2010, 0.2014, 0.2002, 0.2007, 0.2011, 0.1995, 0.2003, 0.2011, 0.2004]
  val_value_value_mse : [0.2034, 0.2010, 0.2014, 0.2002, 0.2007, 0.2011, 0.1995, 0.2003, 0.2011, 0.2004]
  value_loss : [0.2054, 0.2038, 0.2021, 0.1977, 0.1968, 0.1960, 0.1940, 0.1934, 0.1922, 0.1919]
  value_value_mse : [0.2054, 0.2038, 0.2021, 0.1978, 0.1969, 0.1960, 0.1940, 0.1934, 0.1922, 0.1919]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:15:41.821564Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game96001_game97500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.409622   (epoch 10)   mode=min
  policy_loss: 2.314124   (epoch 10)   mode=min
  policy_policy_acc: 0.318639   (epoch 10)   mode=max
  policy_top10_acc: 0.825661   (epoch 10)   mode=max
  policy_top5_acc: 0.672633   (epoch 10)   mode=max
  val_loss: 2.649822   (epoch 5)   mode=min
  val_policy_loss: 2.548916   (epoch 5)   mode=min
  val_policy_policy_acc: 0.285015   (epoch 8)   mode=max
  val_policy_top10_acc: 0.787013   (epoch 8)   mode=max
  val_policy_top5_acc: 0.630170   (epoch 8)   mode=max
  val_value_loss: 0.197992   (epoch 1)   mode=min
  val_value_value_mse: 0.198010   (epoch 1)   mode=min
  value_loss: 0.190948   (epoch 10)   mode=min
  value_value_mse: 0.190952   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6649, 2.6236, 2.5924, 2.5675, 2.5098, 2.4896, 2.4737, 2.4394, 2.4280, 2.4096]
  policy_loss : [2.5621, 2.5219, 2.4918, 2.4675, 2.4117, 2.3923, 2.3769, 2.3432, 2.3322, 2.3141]
  policy_policy_acc : [0.2682, 0.2774, 0.2822, 0.2872, 0.2998, 0.3021, 0.3047, 0.3135, 0.3154, 0.3186]
  policy_top10_acc : [0.7708, 0.7806, 0.7889, 0.7947, 0.8068, 0.8093, 0.8143, 0.8199, 0.8238, 0.8257]
  policy_top5_acc : [0.6061, 0.6177, 0.6254, 0.6324, 0.6482, 0.6545, 0.6572, 0.6650, 0.6692, 0.6726]
  val_loss : [2.9770, 2.6751, 2.8236, 3.1300, 2.6498, 2.7990, 3.1053, 2.8111, 2.8007, 2.7318]
  val_policy_loss : [2.8777, 2.5738, 2.7236, 3.0292, 2.5489, 2.6988, 3.0050, 2.7106, 2.6998, 2.6319]
  val_policy_policy_acc : [0.2774, 0.2729, 0.2783, 0.2801, 0.2836, 0.2840, 0.2802, 0.2850, 0.2844, 0.2849]
  val_policy_top10_acc : [0.7822, 0.7792, 0.7823, 0.7830, 0.7852, 0.7813, 0.7811, 0.7870, 0.7807, 0.7847]
  val_policy_top5_acc : [0.6230, 0.6137, 0.6250, 0.6204, 0.6267, 0.6279, 0.6288, 0.6302, 0.6270, 0.6296]
  val_value_loss : [0.1980, 0.2025, 0.1997, 0.2010, 0.2018, 0.2001, 0.2001, 0.2006, 0.2016, 0.1998]
  val_value_value_mse : [0.1980, 0.2025, 0.1997, 0.2010, 0.2018, 0.2001, 0.2002, 0.2006, 0.2016, 0.1998]
  value_loss : [0.2058, 0.2035, 0.2013, 0.2000, 0.1962, 0.1945, 0.1936, 0.1924, 0.1916, 0.1909]
  value_value_mse : [0.2058, 0.2035, 0.2013, 0.2000, 0.1962, 0.1945, 0.1936, 0.1924, 0.1917, 0.1910]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:25:41.631997Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game97501_game99000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.406600   (epoch 10)   mode=min
  policy_loss: 2.315369   (epoch 10)   mode=min
  policy_policy_acc: 0.317283   (epoch 10)   mode=max
  policy_top10_acc: 0.825032   (epoch 10)   mode=max
  policy_top5_acc: 0.670785   (epoch 10)   mode=max
  val_loss: 2.611222   (epoch 6)   mode=min
  val_policy_loss: 2.511346   (epoch 6)   mode=min
  val_policy_policy_acc: 0.284915   (epoch 10)   mode=max
  val_policy_top10_acc: 0.784216   (epoch 6)   mode=max
  val_policy_top5_acc: 0.626773   (epoch 7)   mode=max
  val_value_loss: 0.199686   (epoch 6)   mode=min
  val_value_value_mse: 0.199707   (epoch 6)   mode=min
  value_loss: 0.182417   (epoch 10)   mode=min
  value_value_mse: 0.182416   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6727, 2.6303, 2.5987, 2.5723, 2.5478, 2.4877, 2.4710, 2.4483, 2.4194, 2.4066]
  policy_loss : [2.5721, 2.5314, 2.5015, 2.4758, 2.4521, 2.3942, 2.3782, 2.3562, 2.3280, 2.3154]
  policy_policy_acc : [0.2652, 0.2746, 0.2793, 0.2825, 0.2880, 0.3004, 0.3045, 0.3088, 0.3154, 0.3173]
  policy_top10_acc : [0.7690, 0.7793, 0.7872, 0.7922, 0.7963, 0.8091, 0.8122, 0.8179, 0.8227, 0.8250]
  policy_top5_acc : [0.6041, 0.6144, 0.6232, 0.6303, 0.6360, 0.6512, 0.6548, 0.6611, 0.6670, 0.6708]
  val_loss : [2.6224, 2.9885, 2.6138, 2.6404, 2.8832, 2.6112, 2.6754, 2.6323, 2.9721, 2.6416]
  val_policy_loss : [2.5215, 2.8880, 2.5133, 2.5405, 2.7814, 2.5113, 2.5738, 2.5318, 2.8713, 2.5405]
  val_policy_policy_acc : [0.2765, 0.2778, 0.2802, 0.2760, 0.2785, 0.2837, 0.2829, 0.2835, 0.2845, 0.2849]
  val_policy_top10_acc : [0.7799, 0.7798, 0.7818, 0.7785, 0.7821, 0.7842, 0.7836, 0.7834, 0.7806, 0.7785]
  val_policy_top5_acc : [0.6168, 0.6166, 0.6192, 0.6142, 0.6222, 0.6221, 0.6268, 0.6225, 0.6256, 0.6258]
  val_value_loss : [0.2017, 0.2006, 0.2007, 0.1997, 0.2032, 0.1997, 0.2029, 0.2010, 0.2010, 0.2019]
  val_value_value_mse : [0.2018, 0.2006, 0.2007, 0.1997, 0.2032, 0.1997, 0.2030, 0.2010, 0.2011, 0.2020]
  value_loss : [0.2012, 0.1977, 0.1945, 0.1930, 0.1915, 0.1869, 0.1856, 0.1842, 0.1829, 0.1824]
  value_value_mse : [0.2012, 0.1977, 0.1945, 0.1930, 0.1915, 0.1869, 0.1856, 0.1842, 0.1829, 0.1824]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:34:56.437341Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game99001_game100500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.403658   (epoch 9)   mode=min
  policy_loss: 2.314463   (epoch 9)   mode=min
  policy_policy_acc: 0.317606   (epoch 9)   mode=max
  policy_top10_acc: 0.825097   (epoch 9)   mode=max
  policy_top5_acc: 0.671270   (epoch 9)   mode=max
  val_loss: 2.625741   (epoch 3)   mode=min
  val_policy_loss: 2.521243   (epoch 3)   mode=min
  val_policy_policy_acc: 0.285315   (epoch 9)   mode=max
  val_policy_top10_acc: 0.784116   (epoch 6)   mode=max
  val_policy_top5_acc: 0.622977   (epoch 9)   mode=max
  val_value_loss: 0.198481   (epoch 1)   mode=min
  val_value_value_mse: 0.198499   (epoch 1)   mode=min
  value_loss: 0.179485   (epoch 9)   mode=min
  value_value_mse: 0.179448   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6526, 2.6133, 2.5814, 2.5501, 2.5278, 2.4727, 2.4476, 2.4151, 2.4037]
  policy_loss : [2.5551, 2.5180, 2.4867, 2.4558, 2.4351, 2.3811, 2.3563, 2.3252, 2.3145]
  policy_policy_acc : [0.2693, 0.2772, 0.2826, 0.2875, 0.2925, 0.3037, 0.3105, 0.3149, 0.3176]
  policy_top10_acc : [0.7730, 0.7819, 0.7895, 0.7960, 0.8005, 0.8102, 0.8167, 0.8233, 0.8251]
  policy_top5_acc : [0.6092, 0.6179, 0.6274, 0.6348, 0.6417, 0.6544, 0.6617, 0.6697, 0.6713]
  val_loss : [2.6384, 2.6285, 2.6257, 2.6365, 2.6391, 2.6383, 2.7170, 2.6477, 2.6644]
  val_policy_loss : [2.5391, 2.5272, 2.5212, 2.5360, 2.5366, 2.5383, 2.6161, 2.5478, 2.5645]
  val_policy_policy_acc : [0.2766, 0.2728, 0.2783, 0.2785, 0.2780, 0.2832, 0.2836, 0.2821, 0.2853]
  val_policy_top10_acc : [0.7788, 0.7747, 0.7811, 0.7778, 0.7759, 0.7841, 0.7783, 0.7805, 0.7794]
  val_policy_top5_acc : [0.6164, 0.6119, 0.6189, 0.6122, 0.6157, 0.6166, 0.6173, 0.6227, 0.6230]
  val_value_loss : [0.1985, 0.2027, 0.2090, 0.2008, 0.2049, 0.2000, 0.2016, 0.1999, 0.1998]
  val_value_value_mse : [0.1985, 0.2027, 0.2090, 0.2008, 0.2050, 0.2001, 0.2016, 0.1999, 0.1998]
  value_loss : [0.1949, 0.1918, 0.1890, 0.1880, 0.1863, 0.1830, 0.1822, 0.1795, 0.1795]
  value_value_mse : [0.1950, 0.1918, 0.1890, 0.1879, 0.1862, 0.1831, 0.1822, 0.1795, 0.1794]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:41:54.411001Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game100501_game102000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.434942   (epoch 7)   mode=min
  policy_loss: 2.339789   (epoch 7)   mode=min
  policy_policy_acc: 0.313368   (epoch 7)   mode=max
  policy_top10_acc: 0.819928   (epoch 7)   mode=max
  policy_top5_acc: 0.666066   (epoch 7)   mode=max
  val_loss: 2.628986   (epoch 1)   mode=min
  val_policy_loss: 2.525619   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289111   (epoch 6)   mode=max
  val_policy_top10_acc: 0.783017   (epoch 4)   mode=max
  val_policy_top5_acc: 0.626174   (epoch 7)   mode=max
  val_value_loss: 0.200592   (epoch 7)   mode=min
  val_value_value_mse: 0.200623   (epoch 7)   mode=min
  value_loss: 0.190359   (epoch 7)   mode=min
  value_value_mse: 0.190358   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6365, 2.5965, 2.5610, 2.5034, 2.4769, 2.4467, 2.4349]
  policy_loss : [2.5346, 2.4960, 2.4623, 2.4062, 2.3806, 2.3510, 2.3398]
  policy_policy_acc : [0.2746, 0.2798, 0.2883, 0.2991, 0.3046, 0.3122, 0.3134]
  policy_top10_acc : [0.7772, 0.7877, 0.7965, 0.8064, 0.8114, 0.8171, 0.8199]
  policy_top5_acc : [0.6160, 0.6259, 0.6357, 0.6489, 0.6574, 0.6637, 0.6661]
  val_loss : [2.6290, 2.6988, 6.3513, 4.3818, 4.2915, 4.2789, 3.6931]
  val_policy_loss : [2.5256, 2.5970, 6.2487, 4.2793, 4.1886, 4.1768, 3.5921]
  val_policy_policy_acc : [0.2784, 0.2790, 0.2779, 0.2848, 0.2875, 0.2891, 0.2883]
  val_policy_top10_acc : [0.7799, 0.7819, 0.7795, 0.7830, 0.7816, 0.7827, 0.7817]
  val_policy_top5_acc : [0.6153, 0.6172, 0.6175, 0.6260, 0.6218, 0.6248, 0.6262]
  val_value_loss : [0.2068, 0.2033, 0.2006, 0.2028, 0.2038, 0.2020, 0.2006]
  val_value_value_mse : [0.2068, 0.2034, 0.2006, 0.2028, 0.2038, 0.2020, 0.2006]
  value_loss : [0.2037, 0.2009, 0.1973, 0.1943, 0.1926, 0.1914, 0.1904]
  value_value_mse : [0.2037, 0.2009, 0.1973, 0.1943, 0.1926, 0.1914, 0.1904]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:48:45.367817Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game102001_game103500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.455382   (epoch 7)   mode=min
  policy_loss: 2.363863   (epoch 7)   mode=min
  policy_policy_acc: 0.309277   (epoch 7)   mode=max
  policy_top10_acc: 0.816118   (epoch 7)   mode=max
  policy_top5_acc: 0.662321   (epoch 7)   mode=max
  val_loss: 2.611814   (epoch 1)   mode=min
  val_policy_loss: 2.512706   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284316   (epoch 4)   mode=max
  val_policy_top10_acc: 0.786214   (epoch 4)   mode=max
  val_policy_top5_acc: 0.626773   (epoch 6)   mode=max
  val_value_loss: 0.197891   (epoch 3)   mode=min
  val_value_value_mse: 0.197920   (epoch 3)   mode=min
  value_loss: 0.183194   (epoch 7)   mode=min
  value_value_mse: 0.183195   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6531, 2.6142, 2.5793, 2.5213, 2.5021, 2.4662, 2.4554]
  policy_loss : [2.5552, 2.5173, 2.4835, 2.4276, 2.4091, 2.3742, 2.3639]
  policy_policy_acc : [0.2689, 0.2770, 0.2844, 0.2952, 0.2995, 0.3062, 0.3093]
  policy_top10_acc : [0.7718, 0.7827, 0.7898, 0.8022, 0.8058, 0.8149, 0.8161]
  policy_top5_acc : [0.6110, 0.6217, 0.6313, 0.6443, 0.6504, 0.6586, 0.6623]
  val_loss : [2.6118, 2.6855, 2.6315, 2.6663, 2.6161, 2.6417, 2.6190]
  val_policy_loss : [2.5127, 2.5858, 2.5326, 2.5669, 2.5163, 2.5424, 2.5193]
  val_policy_policy_acc : [0.2763, 0.2771, 0.2767, 0.2843, 0.2796, 0.2825, 0.2819]
  val_policy_top10_acc : [0.7796, 0.7767, 0.7793, 0.7862, 0.7833, 0.7840, 0.7852]
  val_policy_top5_acc : [0.6187, 0.6179, 0.6152, 0.6246, 0.6227, 0.6268, 0.6256]
  val_value_loss : [0.1981, 0.1993, 0.1979, 0.1988, 0.1997, 0.1988, 0.1995]
  val_value_value_mse : [0.1981, 0.1993, 0.1979, 0.1988, 0.1997, 0.1988, 0.1995]
  value_loss : [0.1959, 0.1939, 0.1917, 0.1874, 0.1860, 0.1839, 0.1832]
  value_value_mse : [0.1959, 0.1939, 0.1917, 0.1874, 0.1860, 0.1839, 0.1832]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T06:58:13.875247Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game103501_game105000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.431354   (epoch 10)   mode=min
  policy_loss: 2.335191   (epoch 10)   mode=min
  policy_policy_acc: 0.316431   (epoch 10)   mode=max
  policy_top10_acc: 0.821092   (epoch 10)   mode=max
  policy_top5_acc: 0.668595   (epoch 9)   mode=max
  val_loss: 2.604170   (epoch 4)   mode=min
  val_policy_loss: 2.503956   (epoch 4)   mode=min
  val_policy_policy_acc: 0.286114   (epoch 6)   mode=max
  val_policy_top10_acc: 0.784815   (epoch 7)   mode=max
  val_policy_top5_acc: 0.631968   (epoch 9)   mode=max
  val_value_loss: 0.198700   (epoch 9)   mode=min
  val_value_value_mse: 0.198722   (epoch 9)   mode=min
  value_loss: 0.192326   (epoch 10)   mode=min
  value_value_mse: 0.192319   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6707, 2.6314, 2.6004, 2.5416, 2.5219, 2.5035, 2.4698, 2.4593, 2.4388, 2.4314]
  policy_loss : [2.5684, 2.5297, 2.4998, 2.4427, 2.4236, 2.4058, 2.3731, 2.3627, 2.3426, 2.3352]
  policy_policy_acc : [0.2704, 0.2779, 0.2834, 0.2948, 0.2978, 0.3020, 0.3079, 0.3108, 0.3146, 0.3164]
  policy_top10_acc : [0.7698, 0.7796, 0.7859, 0.7993, 0.8040, 0.8086, 0.8136, 0.8169, 0.8198, 0.8211]
  policy_top5_acc : [0.6080, 0.6182, 0.6264, 0.6416, 0.6442, 0.6512, 0.6582, 0.6614, 0.6686, 0.6685]
  val_loss : [2.6127, 2.6256, 2.6127, 2.6042, 2.6480, 2.6751, 2.7161, 2.8955, 2.8440, 2.9701]
  val_policy_loss : [2.5115, 2.5209, 2.5118, 2.5040, 2.5475, 2.5751, 2.6164, 2.7949, 2.7445, 2.8705]
  val_policy_policy_acc : [0.2780, 0.2740, 0.2783, 0.2830, 0.2828, 0.2861, 0.2837, 0.2854, 0.2852, 0.2841]
  val_policy_top10_acc : [0.7792, 0.7767, 0.7819, 0.7815, 0.7837, 0.7845, 0.7848, 0.7834, 0.7831, 0.7837]
  val_policy_top5_acc : [0.6198, 0.6186, 0.6199, 0.6276, 0.6285, 0.6255, 0.6237, 0.6289, 0.6320, 0.6277]
  val_value_loss : [0.2025, 0.2093, 0.2017, 0.2004, 0.2009, 0.1999, 0.1992, 0.2008, 0.1987, 0.1988]
  val_value_value_mse : [0.2025, 0.2093, 0.2018, 0.2004, 0.2009, 0.1999, 0.1992, 0.2008, 0.1987, 0.1988]
  value_loss : [0.2049, 0.2033, 0.2010, 0.1978, 0.1965, 0.1956, 0.1936, 0.1933, 0.1925, 0.1923]
  value_value_mse : [0.2049, 0.2033, 0.2010, 0.1978, 0.1965, 0.1956, 0.1936, 0.1933, 0.1925, 0.1923]

================================================================================

History file: model_versions/chess_elo_model_V18_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:05:16.229088Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game105001_game106500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V17
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451680   (epoch 7)   mode=min
  policy_loss: 2.355254   (epoch 7)   mode=min
  policy_policy_acc: 0.309794   (epoch 7)   mode=max
  policy_top10_acc: 0.815201   (epoch 7)   mode=max
  policy_top5_acc: 0.663318   (epoch 7)   mode=max
  val_loss: 2.602756   (epoch 1)   mode=min
  val_policy_loss: 2.500920   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286314   (epoch 5)   mode=max
  val_policy_top10_acc: 0.783616   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626773   (epoch 6)   mode=max
  val_value_loss: 0.199303   (epoch 6)   mode=min
  val_value_value_mse: 0.199307   (epoch 6)   mode=min
  value_loss: 0.193365   (epoch 6)   mode=min
  value_value_mse: 0.193357   (epoch 6)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6497, 2.6116, 2.5763, 2.5167, 2.4943, 2.4649, 2.4517]
  policy_loss : [2.5478, 2.5103, 2.4756, 2.4183, 2.3964, 2.3681, 2.3553]
  policy_policy_acc : [0.2710, 0.2782, 0.2855, 0.2970, 0.2993, 0.3070, 0.3098]
  policy_top10_acc : [0.7735, 0.7834, 0.7901, 0.8018, 0.8069, 0.8137, 0.8152]
  policy_top5_acc : [0.6112, 0.6228, 0.6314, 0.6455, 0.6517, 0.6606, 0.6633]
  val_loss : [2.6028, 2.6047, 2.6307, 2.7624, 2.6763, 2.6191, 2.6404]
  val_policy_loss : [2.5009, 2.5031, 2.5300, 2.6611, 2.5755, 2.5195, 2.5406]
  val_policy_policy_acc : [0.2778, 0.2787, 0.2773, 0.2848, 0.2863, 0.2858, 0.2849]
  val_policy_top10_acc : [0.7836, 0.7786, 0.7728, 0.7816, 0.7798, 0.7806, 0.7819]
  val_policy_top5_acc : [0.6244, 0.6237, 0.6173, 0.6258, 0.6223, 0.6268, 0.6221]
  val_value_loss : [0.2037, 0.2031, 0.2014, 0.2024, 0.2015, 0.1993, 0.1994]
  val_value_value_mse : [0.2037, 0.2032, 0.2014, 0.2024, 0.2015, 0.1993, 0.1994]
  value_loss : [0.2042, 0.2028, 0.2007, 0.1965, 0.1952, 0.1934, 0.1934]
  value_value_mse : [0.2042, 0.2027, 0.2007, 0.1965, 0.1952, 0.1934, 0.1934]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:12:18.204515Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game106501_game108000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441571   (epoch 7)   mode=min
  policy_loss: 2.344342   (epoch 7)   mode=min
  policy_policy_acc: 0.312506   (epoch 7)   mode=max
  policy_top10_acc: 0.817938   (epoch 7)   mode=max
  policy_top5_acc: 0.663478   (epoch 7)   mode=max
  val_loss: 2.608098   (epoch 1)   mode=min
  val_policy_loss: 2.507494   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284316   (epoch 7)   mode=max
  val_policy_top10_acc: 0.783317   (epoch 5)   mode=max
  val_policy_top5_acc: 0.626573   (epoch 4)   mode=max
  val_value_loss: 0.196407   (epoch 6)   mode=min
  val_value_value_mse: 0.196423   (epoch 6)   mode=min
  value_loss: 0.194435   (epoch 6)   mode=min
  value_value_mse: 0.194434   (epoch 6)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6356, 2.5983, 2.5641, 2.5100, 2.4868, 2.4554, 2.4416]
  policy_loss : [2.5327, 2.4969, 2.4634, 2.4106, 2.3887, 2.3582, 2.3443]
  policy_policy_acc : [0.2749, 0.2817, 0.2884, 0.2999, 0.3043, 0.3121, 0.3125]
  policy_top10_acc : [0.7745, 0.7837, 0.7919, 0.8039, 0.8065, 0.8130, 0.8179]
  policy_top5_acc : [0.6144, 0.6257, 0.6341, 0.6473, 0.6531, 0.6615, 0.6635]
  val_loss : [2.6081, 2.6309, 2.6269, 2.6306, 2.6966, 2.6359, 2.6732]
  val_policy_loss : [2.5075, 2.5313, 2.5272, 2.5318, 2.5978, 2.5376, 2.5741]
  val_policy_policy_acc : [0.2783, 0.2796, 0.2774, 0.2839, 0.2806, 0.2841, 0.2843]
  val_policy_top10_acc : [0.7784, 0.7790, 0.7791, 0.7829, 0.7833, 0.7831, 0.7830]
  val_policy_top5_acc : [0.6225, 0.6172, 0.6174, 0.6266, 0.6213, 0.6248, 0.6227]
  val_value_loss : [0.2011, 0.1990, 0.1991, 0.1972, 0.1974, 0.1964, 0.1979]
  val_value_value_mse : [0.2011, 0.1990, 0.1991, 0.1972, 0.1974, 0.1964, 0.1979]
  value_loss : [0.2057, 0.2028, 0.2015, 0.1988, 0.1963, 0.1944, 0.1945]
  value_value_mse : [0.2057, 0.2028, 0.2015, 0.1988, 0.1963, 0.1944, 0.1945]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:19:15.314208Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game108001_game109500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.457190   (epoch 7)   mode=min
  policy_loss: 2.360092   (epoch 7)   mode=min
  policy_policy_acc: 0.308057   (epoch 7)   mode=max
  policy_top10_acc: 0.817696   (epoch 7)   mode=max
  policy_top5_acc: 0.663913   (epoch 7)   mode=max
  val_loss: 2.601130   (epoch 1)   mode=min
  val_policy_loss: 2.502208   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286813   (epoch 7)   mode=max
  val_policy_top10_acc: 0.786713   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626973   (epoch 6)   mode=max
  val_value_loss: 0.197741   (epoch 1)   mode=min
  val_value_value_mse: 0.197761   (epoch 1)   mode=min
  value_loss: 0.194200   (epoch 7)   mode=min
  value_value_mse: 0.194195   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6476, 2.6120, 2.5806, 2.5221, 2.5014, 2.4653, 2.4572]
  policy_loss : [2.5447, 2.5105, 2.4800, 2.4234, 2.4031, 2.3679, 2.3601]
  policy_policy_acc : [0.2710, 0.2774, 0.2836, 0.2946, 0.3009, 0.3060, 0.3081]
  policy_top10_acc : [0.7767, 0.7850, 0.7916, 0.8041, 0.8085, 0.8170, 0.8177]
  policy_top5_acc : [0.6131, 0.6243, 0.6328, 0.6479, 0.6537, 0.6611, 0.6639]
  val_loss : [2.6011, 3.1551, 3.0948, 3.0606, 3.3083, 3.8527, 2.6524]
  val_policy_loss : [2.5022, 3.0532, 2.9940, 2.9612, 3.2083, 3.7527, 2.5531]
  val_policy_policy_acc : [0.2808, 0.2857, 0.2835, 0.2832, 0.2857, 0.2861, 0.2868]
  val_policy_top10_acc : [0.7804, 0.7867, 0.7784, 0.7802, 0.7803, 0.7839, 0.7811]
  val_policy_top5_acc : [0.6197, 0.6199, 0.6186, 0.6226, 0.6231, 0.6270, 0.6238]
  val_value_loss : [0.1977, 0.2029, 0.2010, 0.1981, 0.1991, 0.1984, 0.1983]
  val_value_value_mse : [0.1978, 0.2029, 0.2010, 0.1982, 0.1992, 0.1984, 0.1984]
  value_loss : [0.2057, 0.2028, 0.2012, 0.1973, 0.1962, 0.1950, 0.1942]
  value_value_mse : [0.2057, 0.2028, 0.2012, 0.1973, 0.1962, 0.1950, 0.1942]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:27:09.857308Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game109501_game111000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.425870   (epoch 8)   mode=min
  policy_loss: 2.329618   (epoch 8)   mode=min
  policy_policy_acc: 0.316925   (epoch 8)   mode=max
  policy_top10_acc: 0.823521   (epoch 8)   mode=max
  policy_top5_acc: 0.667027   (epoch 8)   mode=max
  val_loss: 2.600851   (epoch 2)   mode=min
  val_policy_loss: 2.500263   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286813   (epoch 7)   mode=max
  val_policy_top10_acc: 0.786813   (epoch 7)   mode=max
  val_policy_top5_acc: 0.626973   (epoch 7)   mode=max
  val_value_loss: 0.197537   (epoch 7)   mode=min
  val_value_value_mse: 0.197551   (epoch 7)   mode=min
  value_loss: 0.192334   (epoch 7)   mode=min
  value_value_mse: 0.192337   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6415, 2.6066, 2.5731, 2.5472, 2.4917, 2.4705, 2.4338, 2.4259]
  policy_loss : [2.5396, 2.5052, 2.4729, 2.4472, 2.3938, 2.3732, 2.3376, 2.3296]
  policy_policy_acc : [0.2710, 0.2804, 0.2852, 0.2913, 0.3031, 0.3064, 0.3126, 0.3169]
  policy_top10_acc : [0.7783, 0.7866, 0.7923, 0.7996, 0.8117, 0.8144, 0.8226, 0.8235]
  policy_top5_acc : [0.6120, 0.6229, 0.6304, 0.6373, 0.6513, 0.6573, 0.6669, 0.6670]
  val_loss : [2.6337, 2.6009, 2.6076, 2.6113, 2.6577, 2.6434, 2.6514, 2.9862]
  val_policy_loss : [2.5320, 2.5003, 2.5076, 2.5100, 2.5582, 2.5439, 2.5526, 2.8871]
  val_policy_policy_acc : [0.2792, 0.2816, 0.2777, 0.2779, 0.2800, 0.2835, 0.2868, 0.2853]
  val_policy_top10_acc : [0.7776, 0.7793, 0.7814, 0.7808, 0.7810, 0.7807, 0.7868, 0.7822]
  val_policy_top5_acc : [0.6122, 0.6179, 0.6238, 0.6213, 0.6220, 0.6229, 0.6270, 0.6224]
  val_value_loss : [0.2035, 0.2012, 0.1999, 0.2024, 0.1988, 0.1988, 0.1975, 0.1977]
  val_value_value_mse : [0.2035, 0.2012, 0.1999, 0.2024, 0.1988, 0.1988, 0.1976, 0.1977]
  value_loss : [0.2040, 0.2027, 0.2004, 0.2000, 0.1958, 0.1945, 0.1923, 0.1924]
  value_value_mse : [0.2040, 0.2027, 0.2004, 0.2000, 0.1958, 0.1945, 0.1923, 0.1923]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:37:09.986893Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game111001_game112500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.399420   (epoch 10)   mode=min
  policy_loss: 2.305002   (epoch 10)   mode=min
  policy_policy_acc: 0.321250   (epoch 10)   mode=max
  policy_top10_acc: 0.826995   (epoch 10)   mode=max
  policy_top5_acc: 0.674881   (epoch 10)   mode=max
  val_loss: 2.602340   (epoch 5)   mode=min
  val_policy_loss: 2.503119   (epoch 5)   mode=min
  val_policy_policy_acc: 0.291009   (epoch 8)   mode=max
  val_policy_top10_acc: 0.784016   (epoch 9)   mode=max
  val_policy_top5_acc: 0.625774   (epoch 10)   mode=max
  val_value_loss: 0.196748   (epoch 4)   mode=min
  val_value_value_mse: 0.196762   (epoch 4)   mode=min
  value_loss: 0.188618   (epoch 10)   mode=min
  value_value_mse: 0.188615   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6395, 2.5989, 2.5718, 2.5147, 2.4927, 2.4748, 2.4589, 2.4265, 2.4156, 2.3994]
  policy_loss : [2.5378, 2.4986, 2.4728, 2.4174, 2.3959, 2.3783, 2.3631, 2.3318, 2.3210, 2.3050]
  policy_policy_acc : [0.2727, 0.2806, 0.2866, 0.2984, 0.3030, 0.3065, 0.3091, 0.3154, 0.3180, 0.3212]
  policy_top10_acc : [0.7760, 0.7872, 0.7927, 0.8047, 0.8093, 0.8138, 0.8162, 0.8219, 0.8252, 0.8270]
  policy_top5_acc : [0.6137, 0.6230, 0.6323, 0.6448, 0.6522, 0.6551, 0.6591, 0.6673, 0.6720, 0.6749]
  val_loss : [2.6099, 2.6250, 2.6124, 2.6878, 2.6023, 2.6109, 2.6361, 2.6068, 2.8193, 2.8968]
  val_policy_loss : [2.5114, 2.5248, 2.5125, 2.5894, 2.5031, 2.5121, 2.5373, 2.5082, 2.7203, 2.7982]
  val_policy_policy_acc : [0.2764, 0.2795, 0.2800, 0.2873, 0.2885, 0.2867, 0.2853, 0.2910, 0.2873, 0.2894]
  val_policy_top10_acc : [0.7777, 0.7750, 0.7802, 0.7809, 0.7826, 0.7827, 0.7786, 0.7809, 0.7840, 0.7817]
  val_policy_top5_acc : [0.6164, 0.6118, 0.6157, 0.6220, 0.6216, 0.6246, 0.6215, 0.6221, 0.6253, 0.6258]
  val_value_loss : [0.1970, 0.2004, 0.1999, 0.1967, 0.1986, 0.1977, 0.1976, 0.1972, 0.1979, 0.1969]
  val_value_value_mse : [0.1970, 0.2004, 0.1999, 0.1968, 0.1986, 0.1977, 0.1977, 0.1972, 0.1979, 0.1970]
  value_loss : [0.2036, 0.2007, 0.1978, 0.1949, 0.1934, 0.1927, 0.1915, 0.1896, 0.1891, 0.1886]
  value_value_mse : [0.2036, 0.2007, 0.1978, 0.1949, 0.1934, 0.1927, 0.1916, 0.1896, 0.1891, 0.1886]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:44:56.072928Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game112501_game114000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.420635   (epoch 8)   mode=min
  policy_loss: 2.328811   (epoch 8)   mode=min
  policy_policy_acc: 0.314199   (epoch 8)   mode=max
  policy_top10_acc: 0.823850   (epoch 8)   mode=max
  policy_top5_acc: 0.670650   (epoch 8)   mode=max
  val_loss: 2.607680   (epoch 2)   mode=min
  val_policy_loss: 2.507390   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285315   (epoch 6)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 7)   mode=max
  val_policy_top5_acc: 0.628771   (epoch 7)   mode=max
  val_value_loss: 0.197145   (epoch 3)   mode=min
  val_value_value_mse: 0.197176   (epoch 3)   mode=min
  value_loss: 0.184540   (epoch 8)   mode=min
  value_value_mse: 0.184598   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6488, 2.6043, 2.5725, 2.5443, 2.4873, 2.4686, 2.4369, 2.4206]
  policy_loss : [2.5488, 2.5064, 2.4760, 2.4483, 2.3942, 2.3751, 2.3458, 2.3288]
  policy_policy_acc : [0.2712, 0.2802, 0.2858, 0.2896, 0.3012, 0.3053, 0.3128, 0.3142]
  policy_top10_acc : [0.7771, 0.7859, 0.7948, 0.8001, 0.8105, 0.8150, 0.8222, 0.8238]
  policy_top5_acc : [0.6128, 0.6263, 0.6331, 0.6405, 0.6542, 0.6594, 0.6656, 0.6706]
  val_loss : [2.7779, 2.6077, 2.7222, 2.6564, 2.9695, 3.2277, 2.7685, 2.7001]
  val_policy_loss : [2.6769, 2.5074, 2.6236, 2.5552, 2.8692, 3.1276, 2.6685, 2.6002]
  val_policy_policy_acc : [0.2807, 0.2799, 0.2807, 0.2830, 0.2843, 0.2853, 0.2852, 0.2848]
  val_policy_top10_acc : [0.7829, 0.7795, 0.7808, 0.7802, 0.7840, 0.7820, 0.7851, 0.7837]
  val_policy_top5_acc : [0.6199, 0.6173, 0.6168, 0.6199, 0.6206, 0.6210, 0.6288, 0.6281]
  val_value_loss : [0.2018, 0.2004, 0.1971, 0.2021, 0.2002, 0.1994, 0.1997, 0.1994]
  val_value_value_mse : [0.2018, 0.2004, 0.1972, 0.2021, 0.2002, 0.1994, 0.1997, 0.1994]
  value_loss : [0.1994, 0.1969, 0.1940, 0.1927, 0.1887, 0.1877, 0.1854, 0.1845]
  value_value_mse : [0.1994, 0.1969, 0.1939, 0.1927, 0.1887, 0.1877, 0.1854, 0.1846]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T07:53:34.401749Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game114001_game115500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.395244   (epoch 9)   mode=min
  policy_loss: 2.300583   (epoch 9)   mode=min
  policy_policy_acc: 0.321639   (epoch 9)   mode=max
  policy_top10_acc: 0.829375   (epoch 9)   mode=max
  policy_top5_acc: 0.676158   (epoch 9)   mode=max
  val_loss: 2.602970   (epoch 3)   mode=min
  val_policy_loss: 2.501327   (epoch 3)   mode=min
  val_policy_policy_acc: 0.285215   (epoch 8)   mode=max
  val_policy_top10_acc: 0.785415   (epoch 3)   mode=max
  val_policy_top5_acc: 0.625275   (epoch 9)   mode=max
  val_value_loss: 0.199048   (epoch 5)   mode=min
  val_value_value_mse: 0.199065   (epoch 5)   mode=min
  value_loss: 0.189435   (epoch 9)   mode=min
  value_value_mse: 0.189445   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6379, 2.5984, 2.5693, 2.5441, 2.5179, 2.4609, 2.4410, 2.4119, 2.3952]
  policy_loss : [2.5362, 2.4978, 2.4694, 2.4447, 2.4193, 2.3646, 2.3454, 2.3167, 2.3006]
  policy_policy_acc : [0.2743, 0.2807, 0.2878, 0.2915, 0.2973, 0.3097, 0.3122, 0.3183, 0.3216]
  policy_top10_acc : [0.7790, 0.7869, 0.7943, 0.8001, 0.8045, 0.8164, 0.8205, 0.8249, 0.8294]
  policy_top5_acc : [0.6169, 0.6258, 0.6336, 0.6406, 0.6485, 0.6603, 0.6667, 0.6725, 0.6762]
  val_loss : [2.6067, 2.6050, 2.6030, 2.6219, 2.6162, 2.6137, 2.6137, 2.6112, 2.6203]
  val_policy_loss : [2.5066, 2.5048, 2.5013, 2.5211, 2.5166, 2.5120, 2.5122, 2.5112, 2.5191]
  val_policy_policy_acc : [0.2777, 0.2821, 0.2814, 0.2770, 0.2780, 0.2816, 0.2816, 0.2852, 0.2827]
  val_policy_top10_acc : [0.7815, 0.7846, 0.7854, 0.7779, 0.7809, 0.7831, 0.7845, 0.7830, 0.7837]
  val_policy_top5_acc : [0.6179, 0.6230, 0.6199, 0.6147, 0.6197, 0.6245, 0.6226, 0.6221, 0.6253]
  val_value_loss : [0.2000, 0.2004, 0.2032, 0.2014, 0.1990, 0.2033, 0.2029, 0.1998, 0.2022]
  val_value_value_mse : [0.2000, 0.2004, 0.2032, 0.2015, 0.1991, 0.2033, 0.2029, 0.1998, 0.2022]
  value_loss : [0.2036, 0.2013, 0.1999, 0.1988, 0.1973, 0.1926, 0.1910, 0.1900, 0.1894]
  value_value_mse : [0.2036, 0.2013, 0.1999, 0.1988, 0.1973, 0.1926, 0.1910, 0.1900, 0.1894]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:01:27.599258Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game115501_game117000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.438751   (epoch 8)   mode=min
  policy_loss: 2.342663   (epoch 8)   mode=min
  policy_policy_acc: 0.312698   (epoch 8)   mode=max
  policy_top10_acc: 0.821868   (epoch 8)   mode=max
  policy_top5_acc: 0.665267   (epoch 8)   mode=max
  val_loss: 2.600427   (epoch 2)   mode=min
  val_policy_loss: 2.500328   (epoch 2)   mode=min
  val_policy_policy_acc: 0.290010   (epoch 8)   mode=max
  val_policy_top10_acc: 0.786713   (epoch 6)   mode=max
  val_policy_top5_acc: 0.628971   (epoch 6)   mode=max
  val_value_loss: 0.198147   (epoch 3)   mode=min
  val_value_value_mse: 0.198163   (epoch 3)   mode=min
  value_loss: 0.192018   (epoch 8)   mode=min
  value_value_mse: 0.192023   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6534, 2.6153, 2.5812, 2.5594, 2.5001, 2.4816, 2.4506, 2.4388]
  policy_loss : [2.5519, 2.5145, 2.4812, 2.4603, 2.4026, 2.3846, 2.3545, 2.3427]
  policy_policy_acc : [0.2723, 0.2773, 0.2840, 0.2882, 0.2984, 0.3035, 0.3097, 0.3127]
  policy_top10_acc : [0.7731, 0.7830, 0.7906, 0.7954, 0.8074, 0.8113, 0.8178, 0.8219]
  policy_top5_acc : [0.6102, 0.6201, 0.6306, 0.6340, 0.6508, 0.6541, 0.6626, 0.6653]
  val_loss : [2.6165, 2.6004, 2.6108, 2.6281, 2.6799, 2.6371, 2.6241, 2.6121]
  val_policy_loss : [2.5157, 2.5003, 2.5117, 2.5265, 2.5804, 2.5367, 2.5243, 2.5127]
  val_policy_policy_acc : [0.2776, 0.2784, 0.2788, 0.2757, 0.2834, 0.2846, 0.2842, 0.2900]
  val_policy_top10_acc : [0.7779, 0.7811, 0.7793, 0.7755, 0.7800, 0.7867, 0.7835, 0.7844]
  val_policy_top5_acc : [0.6198, 0.6205, 0.6210, 0.6160, 0.6255, 0.6290, 0.6248, 0.6247]
  val_value_loss : [0.2015, 0.2000, 0.1981, 0.2032, 0.1988, 0.2006, 0.1994, 0.1988]
  val_value_value_mse : [0.2015, 0.2000, 0.1982, 0.2032, 0.1989, 0.2006, 0.1994, 0.1988]
  value_loss : [0.2032, 0.2017, 0.1999, 0.1983, 0.1951, 0.1938, 0.1924, 0.1920]
  value_value_mse : [0.2032, 0.2017, 0.1999, 0.1983, 0.1951, 0.1938, 0.1924, 0.1920]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:11:44.271379Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game117001_game118500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.416937   (epoch 10)   mode=min
  policy_loss: 2.323561   (epoch 10)   mode=min
  policy_policy_acc: 0.318195   (epoch 10)   mode=max
  policy_top10_acc: 0.825762   (epoch 10)   mode=max
  policy_top5_acc: 0.671079   (epoch 10)   mode=max
  val_loss: 2.598094   (epoch 4)   mode=min
  val_policy_loss: 2.496057   (epoch 4)   mode=min
  val_policy_policy_acc: 0.287712   (epoch 4)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 10)   mode=max
  val_policy_top5_acc: 0.628771   (epoch 9)   mode=max
  val_value_loss: 0.199661   (epoch 10)   mode=min
  val_value_value_mse: 0.199688   (epoch 10)   mode=min
  value_loss: 0.187337   (epoch 10)   mode=min
  value_value_mse: 0.187275   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6502, 2.6084, 2.5806, 2.5250, 2.5045, 2.4850, 2.4545, 2.4411, 2.4257, 2.4169]
  policy_loss : [2.5498, 2.5088, 2.4825, 2.4285, 2.4089, 2.3898, 2.3599, 2.3469, 2.3317, 2.3236]
  policy_policy_acc : [0.2725, 0.2790, 0.2841, 0.2966, 0.2993, 0.3039, 0.3095, 0.3137, 0.3167, 0.3182]
  policy_top10_acc : [0.7752, 0.7866, 0.7927, 0.8039, 0.8084, 0.8128, 0.8187, 0.8211, 0.8238, 0.8258]
  policy_top5_acc : [0.6110, 0.6225, 0.6294, 0.6436, 0.6487, 0.6539, 0.6607, 0.6660, 0.6691, 0.6711]
  val_loss : [2.6121, 2.6484, 2.6150, 2.5981, 2.6017, 2.6012, 2.6091, 2.6050, 2.6039, 2.6058]
  val_policy_loss : [2.5107, 2.5441, 2.5114, 2.4961, 2.5003, 2.4996, 2.5079, 2.5045, 2.5034, 2.5059]
  val_policy_policy_acc : [0.2763, 0.2676, 0.2805, 0.2877, 0.2826, 0.2838, 0.2870, 0.2867, 0.2861, 0.2876]
  val_policy_top10_acc : [0.7762, 0.7702, 0.7789, 0.7841, 0.7811, 0.7848, 0.7830, 0.7848, 0.7851, 0.7857]
  val_policy_top5_acc : [0.6184, 0.6112, 0.6193, 0.6272, 0.6258, 0.6254, 0.6227, 0.6273, 0.6288, 0.6266]
  val_value_loss : [0.2027, 0.2084, 0.2070, 0.2041, 0.2029, 0.2030, 0.2023, 0.2010, 0.2008, 0.1997]
  val_value_value_mse : [0.2027, 0.2084, 0.2070, 0.2041, 0.2029, 0.2030, 0.2023, 0.2010, 0.2008, 0.1997]
  value_loss : [0.2019, 0.1989, 0.1969, 0.1935, 0.1921, 0.1906, 0.1890, 0.1886, 0.1879, 0.1873]
  value_value_mse : [0.2019, 0.1989, 0.1968, 0.1935, 0.1921, 0.1906, 0.1889, 0.1886, 0.1879, 0.1873]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:18:29.990585Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game118501_game120000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.462152   (epoch 7)   mode=min
  policy_loss: 2.365194   (epoch 7)   mode=min
  policy_policy_acc: 0.306687   (epoch 7)   mode=max
  policy_top10_acc: 0.816916   (epoch 7)   mode=max
  policy_top5_acc: 0.660225   (epoch 7)   mode=max
  val_loss: 2.614543   (epoch 1)   mode=min
  val_policy_loss: 2.513596   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286513   (epoch 7)   mode=max
  val_policy_top10_acc: 0.784016   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624875   (epoch 6)   mode=max
  val_value_loss: 0.198918   (epoch 6)   mode=min
  val_value_value_mse: 0.198946   (epoch 6)   mode=min
  value_loss: 0.193921   (epoch 7)   mode=min
  value_value_mse: 0.193922   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6617, 2.6181, 2.5881, 2.5299, 2.5078, 2.4747, 2.4622]
  policy_loss : [2.5587, 2.5163, 2.4873, 2.4313, 2.4092, 2.3776, 2.3652]
  policy_policy_acc : [0.2673, 0.2766, 0.2807, 0.2934, 0.2979, 0.3057, 0.3067]
  policy_top10_acc : [0.7742, 0.7841, 0.7916, 0.8030, 0.8081, 0.8144, 0.8169]
  policy_top5_acc : [0.6093, 0.6200, 0.6284, 0.6435, 0.6487, 0.6570, 0.6602]
  val_loss : [2.6145, 2.7322, 2.6973, 2.6887, 2.6299, 2.6503, 2.6230]
  val_policy_loss : [2.5136, 2.6312, 2.5971, 2.5883, 2.5294, 2.5508, 2.5232]
  val_policy_policy_acc : [0.2797, 0.2810, 0.2821, 0.2824, 0.2819, 0.2858, 0.2865]
  val_policy_top10_acc : [0.7744, 0.7840, 0.7812, 0.7806, 0.7822, 0.7824, 0.7828]
  val_policy_top5_acc : [0.6190, 0.6232, 0.6209, 0.6205, 0.6217, 0.6249, 0.6230]
  val_value_loss : [0.2018, 0.2016, 0.2002, 0.2005, 0.2009, 0.1989, 0.1997]
  val_value_value_mse : [0.2019, 0.2016, 0.2003, 0.2005, 0.2009, 0.1989, 0.1997]
  value_loss : [0.2060, 0.2037, 0.2016, 0.1972, 0.1971, 0.1941, 0.1939]
  value_value_mse : [0.2060, 0.2037, 0.2016, 0.1972, 0.1971, 0.1941, 0.1939]

================================================================================

History file: model_versions/chess_elo_model_V19_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:26:42.343510Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game120001_game121500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V18
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.460659   (epoch 7)   mode=min
  policy_loss: 2.364378   (epoch 7)   mode=min
  policy_policy_acc: 0.307698   (epoch 7)   mode=max
  policy_top10_acc: 0.816010   (epoch 7)   mode=max
  policy_top5_acc: 0.660649   (epoch 7)   mode=max
  val_loss: 2.613969   (epoch 1)   mode=min
  val_policy_loss: 2.514277   (epoch 1)   mode=min
  val_policy_policy_acc: 0.294206   (epoch 6)   mode=max
  val_policy_top10_acc: 0.788012   (epoch 4)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 4)   mode=max
  val_value_loss: 0.196925   (epoch 5)   mode=min
  val_value_value_mse: 0.196954   (epoch 5)   mode=min
  value_loss: 0.192767   (epoch 7)   mode=min
  value_value_mse: 0.192762   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6517, 2.6125, 2.5842, 2.5263, 2.5066, 2.4752, 2.4607]
  policy_loss : [2.5501, 2.5117, 2.4837, 2.4276, 2.4085, 2.3779, 2.3644]
  policy_policy_acc : [0.2692, 0.2781, 0.2836, 0.2931, 0.2968, 0.3037, 0.3077]
  policy_top10_acc : [0.7747, 0.7840, 0.7904, 0.8023, 0.8066, 0.8123, 0.8160]
  policy_top5_acc : [0.6119, 0.6218, 0.6311, 0.6444, 0.6495, 0.6581, 0.6606]
  val_loss : [2.6140, 3.0770, 2.6580, 2.6414, 2.6844, 2.6560, 2.7321]
  val_policy_loss : [2.5143, 2.9754, 2.5574, 2.5421, 2.5858, 2.5572, 2.6331]
  val_policy_policy_acc : [0.2814, 0.2834, 0.2752, 0.2872, 0.2872, 0.2942, 0.2898]
  val_policy_top10_acc : [0.7756, 0.7818, 0.7715, 0.7880, 0.7833, 0.7859, 0.7871]
  val_policy_top5_acc : [0.6171, 0.6211, 0.6046, 0.6289, 0.6237, 0.6282, 0.6288]
  val_value_loss : [0.1992, 0.2024, 0.2009, 0.1987, 0.1969, 0.1974, 0.1977]
  val_value_value_mse : [0.1993, 0.2024, 0.2009, 0.1987, 0.1970, 0.1974, 0.1977]
  value_loss : [0.2035, 0.2018, 0.2008, 0.1964, 0.1953, 0.1933, 0.1928]
  value_value_mse : [0.2035, 0.2018, 0.2008, 0.1964, 0.1954, 0.1932, 0.1928]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:36:56.912748Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game121501_game123000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.420633   (epoch 10)   mode=min
  policy_loss: 2.325400   (epoch 10)   mode=min
  policy_policy_acc: 0.315661   (epoch 10)   mode=max
  policy_top10_acc: 0.824160   (epoch 10)   mode=max
  policy_top5_acc: 0.669966   (epoch 10)   mode=max
  val_loss: 2.586656   (epoch 4)   mode=min
  val_policy_loss: 2.485255   (epoch 4)   mode=min
  val_policy_policy_acc: 0.290110   (epoch 9)   mode=max
  val_policy_top10_acc: 0.786514   (epoch 4)   mode=max
  val_policy_top5_acc: 0.631169   (epoch 10)   mode=max
  val_value_loss: 0.199850   (epoch 10)   mode=min
  val_value_value_mse: 0.199871   (epoch 10)   mode=min
  value_loss: 0.190419   (epoch 10)   mode=min
  value_value_mse: 0.190421   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6521, 2.6133, 2.5820, 2.5260, 2.5044, 2.4896, 2.4584, 2.4461, 2.4298, 2.4206]
  policy_loss : [2.5497, 2.5124, 2.4819, 2.4278, 2.4067, 2.3926, 2.3622, 2.3502, 2.3343, 2.3254]
  policy_policy_acc : [0.2707, 0.2762, 0.2824, 0.2945, 0.2994, 0.3008, 0.3078, 0.3096, 0.3142, 0.3157]
  policy_top10_acc : [0.7734, 0.7834, 0.7897, 0.8013, 0.8063, 0.8079, 0.8163, 0.8183, 0.8202, 0.8242]
  policy_top5_acc : [0.6111, 0.6216, 0.6305, 0.6426, 0.6495, 0.6529, 0.6612, 0.6641, 0.6672, 0.6700]
  val_loss : [2.6033, 2.6164, 2.6150, 2.5867, 2.6649, 2.6176, 2.6225, 2.6713, 2.6258, 2.6309]
  val_policy_loss : [2.5001, 2.5161, 2.5138, 2.4853, 2.5636, 2.5176, 2.5218, 2.5709, 2.5240, 2.5309]
  val_policy_policy_acc : [0.2837, 0.2832, 0.2833, 0.2880, 0.2865, 0.2832, 0.2886, 0.2880, 0.2901, 0.2886]
  val_policy_top10_acc : [0.7845, 0.7831, 0.7796, 0.7865, 0.7855, 0.7830, 0.7853, 0.7823, 0.7843, 0.7846]
  val_policy_top5_acc : [0.6189, 0.6200, 0.6212, 0.6223, 0.6266, 0.6301, 0.6259, 0.6245, 0.6305, 0.6312]
  val_value_loss : [0.2063, 0.2006, 0.2022, 0.2026, 0.2022, 0.2000, 0.2014, 0.2007, 0.2036, 0.1999]
  val_value_value_mse : [0.2063, 0.2006, 0.2022, 0.2026, 0.2023, 0.2000, 0.2014, 0.2007, 0.2036, 0.1999]
  value_loss : [0.2048, 0.2017, 0.2000, 0.1964, 0.1955, 0.1940, 0.1923, 0.1917, 0.1910, 0.1904]
  value_value_mse : [0.2048, 0.2017, 0.2000, 0.1964, 0.1955, 0.1940, 0.1923, 0.1917, 0.1910, 0.1904]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:46:55.244276Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game123001_game124500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.382115   (epoch 10)   mode=min
  policy_loss: 2.289815   (epoch 10)   mode=min
  policy_policy_acc: 0.320597   (epoch 10)   mode=max
  policy_top10_acc: 0.830666   (epoch 10)   mode=max
  policy_top5_acc: 0.677904   (epoch 10)   mode=max
  val_loss: 2.604827   (epoch 4)   mode=min
  val_policy_loss: 2.504145   (epoch 4)   mode=min
  val_policy_policy_acc: 0.286414   (epoch 10)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627972   (epoch 3)   mode=max
  val_value_loss: 0.200349   (epoch 2)   mode=min
  val_value_value_mse: 0.200376   (epoch 2)   mode=min
  value_loss: 0.184725   (epoch 10)   mode=min
  value_value_mse: 0.184721   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6441, 2.6033, 2.5707, 2.5486, 2.5230, 2.5021, 2.4444, 2.4285, 2.3950, 2.3821]
  policy_loss : [2.5441, 2.5040, 2.4724, 2.4514, 2.4263, 2.4058, 2.3497, 2.3345, 2.3023, 2.2898]
  policy_policy_acc : [0.2692, 0.2788, 0.2849, 0.2888, 0.2934, 0.2975, 0.3089, 0.3120, 0.3183, 0.3206]
  policy_top10_acc : [0.7769, 0.7864, 0.7920, 0.7988, 0.8022, 0.8090, 0.8179, 0.8226, 0.8292, 0.8307]
  policy_top5_acc : [0.6121, 0.6236, 0.6316, 0.6388, 0.6446, 0.6483, 0.6639, 0.6669, 0.6766, 0.6779]
  val_loss : [2.6220, 2.6223, 2.6184, 2.6048, 2.6535, 2.6392, 2.7136, 2.7514, 2.6496, 2.9654]
  val_policy_loss : [2.5216, 2.5220, 2.5169, 2.5041, 2.5498, 2.5386, 2.6127, 2.6497, 2.5487, 2.8643]
  val_policy_policy_acc : [0.2793, 0.2848, 0.2810, 0.2828, 0.2807, 0.2778, 0.2829, 0.2826, 0.2862, 0.2864]
  val_policy_top10_acc : [0.7809, 0.7857, 0.7823, 0.7826, 0.7796, 0.7766, 0.7788, 0.7801, 0.7826, 0.7799]
  val_policy_top5_acc : [0.6190, 0.6210, 0.6280, 0.6238, 0.6195, 0.6213, 0.6215, 0.6220, 0.6263, 0.6229]
  val_value_loss : [0.2007, 0.2003, 0.2030, 0.2013, 0.2072, 0.2010, 0.2016, 0.2033, 0.2018, 0.2017]
  val_value_value_mse : [0.2007, 0.2004, 0.2030, 0.2013, 0.2072, 0.2010, 0.2016, 0.2033, 0.2018, 0.2017]
  value_loss : [0.2001, 0.1986, 0.1964, 0.1946, 0.1932, 0.1925, 0.1894, 0.1881, 0.1856, 0.1847]
  value_value_mse : [0.2001, 0.1986, 0.1964, 0.1946, 0.1932, 0.1925, 0.1894, 0.1881, 0.1856, 0.1847]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T08:56:12.218548Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game124501_game126000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.412823   (epoch 9)   mode=min
  policy_loss: 2.314981   (epoch 9)   mode=min
  policy_policy_acc: 0.317179   (epoch 9)   mode=max
  policy_top10_acc: 0.826895   (epoch 9)   mode=max
  policy_top5_acc: 0.673017   (epoch 9)   mode=max
  val_loss: 2.613873   (epoch 3)   mode=min
  val_policy_loss: 2.512828   (epoch 3)   mode=min
  val_policy_policy_acc: 0.289111   (epoch 9)   mode=max
  val_policy_top10_acc: 0.781718   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624975   (epoch 9)   mode=max
  val_value_loss: 0.201189   (epoch 8)   mode=min
  val_value_value_mse: 0.201191   (epoch 8)   mode=min
  value_loss: 0.195673   (epoch 9)   mode=min
  value_value_mse: 0.195674   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6520, 2.6103, 2.5809, 2.5553, 2.5323, 2.4760, 2.4542, 2.4224, 2.4128]
  policy_loss : [2.5476, 2.5071, 2.4781, 2.4535, 2.4310, 2.3764, 2.3549, 2.3244, 2.3150]
  policy_policy_acc : [0.2693, 0.2787, 0.2847, 0.2879, 0.2956, 0.3032, 0.3092, 0.3150, 0.3172]
  policy_top10_acc : [0.7777, 0.7868, 0.7936, 0.8005, 0.8048, 0.8160, 0.8188, 0.8255, 0.8269]
  policy_top5_acc : [0.6133, 0.6249, 0.6327, 0.6389, 0.6453, 0.6590, 0.6640, 0.6707, 0.6730]
  val_loss : [2.6181, 2.6777, 2.6139, 2.6743, 2.6975, 2.6232, 2.6393, 2.7146, 2.6459]
  val_policy_loss : [2.5174, 2.5764, 2.5128, 2.5703, 2.5966, 2.5216, 2.5384, 2.6139, 2.5452]
  val_policy_policy_acc : [0.2802, 0.2823, 0.2838, 0.2797, 0.2797, 0.2853, 0.2842, 0.2870, 0.2891]
  val_policy_top10_acc : [0.7812, 0.7817, 0.7773, 0.7735, 0.7757, 0.7808, 0.7789, 0.7792, 0.7774]
  val_policy_top5_acc : [0.6190, 0.6210, 0.6202, 0.6209, 0.6155, 0.6202, 0.6234, 0.6224, 0.6250]
  val_value_loss : [0.2014, 0.2025, 0.2019, 0.2076, 0.2017, 0.2030, 0.2018, 0.2012, 0.2014]
  val_value_value_mse : [0.2014, 0.2025, 0.2019, 0.2076, 0.2017, 0.2030, 0.2018, 0.2012, 0.2014]
  value_loss : [0.2087, 0.2063, 0.2056, 0.2037, 0.2025, 0.1992, 0.1984, 0.1961, 0.1957]
  value_value_mse : [0.2087, 0.2063, 0.2056, 0.2037, 0.2025, 0.1992, 0.1984, 0.1961, 0.1957]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:05:48.236747Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game126001_game127500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.430139   (epoch 9)   mode=min
  policy_loss: 2.334980   (epoch 9)   mode=min
  policy_policy_acc: 0.315256   (epoch 9)   mode=max
  policy_top10_acc: 0.820512   (epoch 9)   mode=max
  policy_top5_acc: 0.665638   (epoch 9)   mode=max
  val_loss: 2.603384   (epoch 3)   mode=min
  val_policy_loss: 2.500783   (epoch 3)   mode=min
  val_policy_policy_acc: 0.288611   (epoch 4)   mode=max
  val_policy_top10_acc: 0.782817   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624575   (epoch 9)   mode=max
  val_value_loss: 0.204224   (epoch 9)   mode=min
  val_value_value_mse: 0.204230   (epoch 9)   mode=min
  value_loss: 0.190353   (epoch 9)   mode=min
  value_value_mse: 0.190349   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6661, 2.6282, 2.5996, 2.5694, 2.5454, 2.4918, 2.4721, 2.4392, 2.4301]
  policy_loss : [2.5638, 2.5274, 2.4995, 2.4699, 2.4466, 2.3948, 2.3759, 2.3435, 2.3350]
  policy_policy_acc : [0.2680, 0.2742, 0.2823, 0.2862, 0.2914, 0.3019, 0.3046, 0.3122, 0.3153]
  policy_top10_acc : [0.7699, 0.7790, 0.7862, 0.7931, 0.7979, 0.8087, 0.8124, 0.8189, 0.8205]
  policy_top5_acc : [0.6063, 0.6182, 0.6240, 0.6312, 0.6372, 0.6508, 0.6566, 0.6640, 0.6656]
  val_loss : [2.6131, 2.6473, 2.6034, 2.6441, 2.6270, 2.6139, 2.6228, 2.7047, 2.6175]
  val_policy_loss : [2.5109, 2.5439, 2.5008, 2.5408, 2.5243, 2.5096, 2.5200, 2.6021, 2.5153]
  val_policy_policy_acc : [0.2844, 0.2857, 0.2867, 0.2886, 0.2826, 0.2874, 0.2857, 0.2859, 0.2882]
  val_policy_top10_acc : [0.7828, 0.7810, 0.7776, 0.7788, 0.7754, 0.7803, 0.7804, 0.7807, 0.7791]
  val_policy_top5_acc : [0.6244, 0.6213, 0.6164, 0.6170, 0.6161, 0.6219, 0.6186, 0.6234, 0.6246]
  val_value_loss : [0.2044, 0.2067, 0.2053, 0.2066, 0.2053, 0.2084, 0.2056, 0.2052, 0.2042]
  val_value_value_mse : [0.2044, 0.2067, 0.2053, 0.2066, 0.2053, 0.2084, 0.2057, 0.2052, 0.2042]
  value_loss : [0.2047, 0.2016, 0.2002, 0.1990, 0.1976, 0.1941, 0.1926, 0.1914, 0.1904]
  value_value_mse : [0.2047, 0.2016, 0.2002, 0.1990, 0.1976, 0.1941, 0.1925, 0.1914, 0.1903]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:12:56.398216Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game127501_game129000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.468230   (epoch 7)   mode=min
  policy_loss: 2.369710   (epoch 7)   mode=min
  policy_policy_acc: 0.306966   (epoch 7)   mode=max
  policy_top10_acc: 0.812127   (epoch 7)   mode=max
  policy_top5_acc: 0.658862   (epoch 7)   mode=max
  val_loss: 2.604429   (epoch 1)   mode=min
  val_policy_loss: 2.503073   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289311   (epoch 6)   mode=max
  val_policy_top10_acc: 0.784316   (epoch 1)   mode=max
  val_policy_top5_acc: 0.629870   (epoch 4)   mode=max
  val_value_loss: 0.199074   (epoch 4)   mode=min
  val_value_value_mse: 0.199085   (epoch 4)   mode=min
  value_loss: 0.197037   (epoch 7)   mode=min
  value_value_mse: 0.197036   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6561, 2.6197, 2.5879, 2.5313, 2.5070, 2.4777, 2.4682]
  policy_loss : [2.5519, 2.5165, 2.4858, 2.4310, 2.4075, 2.3790, 2.3697]
  policy_policy_acc : [0.2707, 0.2761, 0.2824, 0.2941, 0.2981, 0.3048, 0.3070]
  policy_top10_acc : [0.7727, 0.7804, 0.7876, 0.7995, 0.8051, 0.8104, 0.8121]
  policy_top5_acc : [0.6100, 0.6189, 0.6285, 0.6413, 0.6483, 0.6553, 0.6589]
  val_loss : [2.6044, 2.6060, 2.6176, 2.6131, 2.7897, 2.6730, 2.6830]
  val_policy_loss : [2.5031, 2.5052, 2.5166, 2.5136, 2.6883, 2.5729, 2.5832]
  val_policy_policy_acc : [0.2821, 0.2812, 0.2798, 0.2864, 0.2860, 0.2893, 0.2878]
  val_policy_top10_acc : [0.7843, 0.7802, 0.7780, 0.7830, 0.7777, 0.7819, 0.7822]
  val_policy_top5_acc : [0.6245, 0.6197, 0.6158, 0.6299, 0.6210, 0.6262, 0.6255]
  val_value_loss : [0.2026, 0.2016, 0.2018, 0.1991, 0.2026, 0.2001, 0.1995]
  val_value_value_mse : [0.2026, 0.2016, 0.2018, 0.1991, 0.2026, 0.2001, 0.1995]
  value_loss : [0.2084, 0.2063, 0.2043, 0.2006, 0.1992, 0.1974, 0.1970]
  value_value_mse : [0.2084, 0.2063, 0.2043, 0.2006, 0.1992, 0.1974, 0.1970]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:23:04.526317Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game129001_game130500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.414016   (epoch 10)   mode=min
  policy_loss: 2.321494   (epoch 10)   mode=min
  policy_policy_acc: 0.320100   (epoch 10)   mode=max
  policy_top10_acc: 0.823129   (epoch 10)   mode=max
  policy_top5_acc: 0.671152   (epoch 10)   mode=max
  val_loss: 2.596914   (epoch 4)   mode=min
  val_policy_loss: 2.496763   (epoch 4)   mode=min
  val_policy_policy_acc: 0.290509   (epoch 8)   mode=max
  val_policy_top10_acc: 0.786214   (epoch 8)   mode=max
  val_policy_top5_acc: 0.629870   (epoch 5)   mode=max
  val_value_loss: 0.198486   (epoch 9)   mode=min
  val_value_value_mse: 0.198501   (epoch 9)   mode=min
  value_loss: 0.185058   (epoch 10)   mode=min
  value_value_mse: 0.185062   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6423, 2.6061, 2.5763, 2.5185, 2.4957, 2.4787, 2.4494, 2.4368, 2.4205, 2.4140]
  policy_loss : [2.5428, 2.5079, 2.4788, 2.4233, 2.4005, 2.3843, 2.3559, 2.3434, 2.3276, 2.3215]
  policy_policy_acc : [0.2754, 0.2804, 0.2855, 0.2992, 0.3019, 0.3061, 0.3123, 0.3152, 0.3179, 0.3201]
  policy_top10_acc : [0.7759, 0.7843, 0.7906, 0.8032, 0.8070, 0.8114, 0.8174, 0.8194, 0.8226, 0.8231]
  policy_top5_acc : [0.6157, 0.6223, 0.6314, 0.6460, 0.6528, 0.6560, 0.6628, 0.6664, 0.6708, 0.6712]
  val_loss : [2.5990, 2.6003, 2.6156, 2.5969, 2.7900, 2.6132, 2.8117, 2.8805, 2.8907, 2.7394]
  val_policy_loss : [2.4983, 2.4993, 2.5149, 2.4968, 2.6901, 2.5131, 2.7121, 2.7806, 2.7912, 2.6396]
  val_policy_policy_acc : [0.2818, 0.2833, 0.2795, 0.2848, 0.2884, 0.2870, 0.2869, 0.2905, 0.2892, 0.2901]
  val_policy_top10_acc : [0.7832, 0.7791, 0.7824, 0.7852, 0.7848, 0.7823, 0.7841, 0.7862, 0.7838, 0.7847]
  val_policy_top5_acc : [0.6219, 0.6223, 0.6198, 0.6205, 0.6299, 0.6222, 0.6241, 0.6251, 0.6241, 0.6242]
  val_value_loss : [0.2015, 0.2019, 0.2012, 0.2002, 0.1994, 0.1999, 0.1987, 0.1994, 0.1985, 0.1993]
  val_value_value_mse : [0.2015, 0.2019, 0.2012, 0.2002, 0.1994, 0.2000, 0.1987, 0.1994, 0.1985, 0.1994]
  value_loss : [0.1990, 0.1963, 0.1951, 0.1906, 0.1902, 0.1888, 0.1872, 0.1870, 0.1860, 0.1851]
  value_value_mse : [0.1990, 0.1963, 0.1951, 0.1906, 0.1902, 0.1888, 0.1872, 0.1870, 0.1860, 0.1851]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:30:29.292639Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game130501_game132000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.451380   (epoch 7)   mode=min
  policy_loss: 2.358772   (epoch 7)   mode=min
  policy_policy_acc: 0.310533   (epoch 7)   mode=max
  policy_top10_acc: 0.813854   (epoch 7)   mode=max
  policy_top5_acc: 0.660671   (epoch 7)   mode=max
  val_loss: 2.603454   (epoch 1)   mode=min
  val_policy_loss: 2.503298   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289910   (epoch 5)   mode=max
  val_policy_top10_acc: 0.787912   (epoch 6)   mode=max
  val_policy_top5_acc: 0.628272   (epoch 5)   mode=max
  val_value_loss: 0.198056   (epoch 6)   mode=min
  val_value_value_mse: 0.198077   (epoch 6)   mode=min
  value_loss: 0.185200   (epoch 6)   mode=min
  value_value_mse: 0.185200   (epoch 6)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6466, 2.6012, 2.5724, 2.5156, 2.4936, 2.4618, 2.4514]
  policy_loss : [2.5477, 2.5036, 2.4756, 2.4210, 2.3998, 2.3692, 2.3588]
  policy_policy_acc : [0.2719, 0.2790, 0.2859, 0.2966, 0.3020, 0.3084, 0.3105]
  policy_top10_acc : [0.7741, 0.7844, 0.7911, 0.8026, 0.8072, 0.8138, 0.8139]
  policy_top5_acc : [0.6142, 0.6240, 0.6317, 0.6468, 0.6521, 0.6597, 0.6607]
  val_loss : [2.6035, 2.6166, 2.6235, 2.6336, 2.6311, 2.6084, 2.6091]
  val_policy_loss : [2.5033, 2.5173, 2.5217, 2.5322, 2.5315, 2.5092, 2.5094]
  val_policy_policy_acc : [0.2823, 0.2833, 0.2832, 0.2836, 0.2899, 0.2882, 0.2869]
  val_policy_top10_acc : [0.7830, 0.7847, 0.7851, 0.7828, 0.7845, 0.7879, 0.7861]
  val_policy_top5_acc : [0.6236, 0.6254, 0.6229, 0.6266, 0.6283, 0.6275, 0.6275]
  val_value_loss : [0.2002, 0.1983, 0.2035, 0.2026, 0.1990, 0.1981, 0.1994]
  val_value_value_mse : [0.2002, 0.1983, 0.2035, 0.2026, 0.1990, 0.1981, 0.1994]
  value_loss : [0.1979, 0.1952, 0.1934, 0.1892, 0.1878, 0.1852, 0.1852]
  value_value_mse : [0.1979, 0.1952, 0.1934, 0.1892, 0.1878, 0.1852, 0.1852]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:37:44.235349Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game132001_game133500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.467860   (epoch 7)   mode=min
  policy_loss: 2.371221   (epoch 7)   mode=min
  policy_policy_acc: 0.306972   (epoch 7)   mode=max
  policy_top10_acc: 0.813240   (epoch 6)   mode=max
  policy_top5_acc: 0.658529   (epoch 7)   mode=max
  val_loss: 2.601360   (epoch 1)   mode=min
  val_policy_loss: 2.499982   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288911   (epoch 4)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 6)   mode=max
  val_policy_top5_acc: 0.626873   (epoch 6)   mode=max
  val_value_loss: 0.199544   (epoch 7)   mode=min
  val_value_value_mse: 0.199563   (epoch 7)   mode=min
  value_loss: 0.193214   (epoch 7)   mode=min
  value_value_mse: 0.193214   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6553, 2.6160, 2.5883, 2.5290, 2.5081, 2.4756, 2.4679]
  policy_loss : [2.5526, 2.5142, 2.4876, 2.4305, 2.4101, 2.3787, 2.3712]
  policy_policy_acc : [0.2694, 0.2777, 0.2816, 0.2944, 0.2994, 0.3058, 0.3070]
  policy_top10_acc : [0.7737, 0.7833, 0.7904, 0.8020, 0.8058, 0.8132, 0.8132]
  policy_top5_acc : [0.6106, 0.6207, 0.6290, 0.6412, 0.6478, 0.6565, 0.6585]
  val_loss : [2.6014, 2.6133, 2.6151, 2.6017, 2.6050, 2.6021, 2.6019]
  val_policy_loss : [2.5000, 2.5114, 2.5145, 2.5018, 2.5041, 2.5015, 2.5021]
  val_policy_policy_acc : [0.2831, 0.2783, 0.2801, 0.2889, 0.2835, 0.2875, 0.2871]
  val_policy_top10_acc : [0.7814, 0.7825, 0.7782, 0.7840, 0.7821, 0.7853, 0.7829]
  val_policy_top5_acc : [0.6210, 0.6195, 0.6155, 0.6231, 0.6242, 0.6269, 0.6241]
  val_value_loss : [0.2027, 0.2037, 0.2011, 0.1998, 0.2017, 0.2011, 0.1995]
  val_value_value_mse : [0.2027, 0.2037, 0.2011, 0.1998, 0.2017, 0.2012, 0.1996]
  value_loss : [0.2053, 0.2036, 0.2013, 0.1972, 0.1959, 0.1938, 0.1932]
  value_value_mse : [0.2053, 0.2036, 0.2013, 0.1972, 0.1959, 0.1938, 0.1932]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:44:53.969127Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game133501_game135000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.461831   (epoch 7)   mode=min
  policy_loss: 2.365404   (epoch 7)   mode=min
  policy_policy_acc: 0.308075   (epoch 7)   mode=max
  policy_top10_acc: 0.813459   (epoch 7)   mode=max
  policy_top5_acc: 0.659760   (epoch 7)   mode=max
  val_loss: 2.597073   (epoch 1)   mode=min
  val_policy_loss: 2.498182   (epoch 1)   mode=min
  val_policy_policy_acc: 0.294306   (epoch 4)   mode=max
  val_policy_top10_acc: 0.786414   (epoch 7)   mode=max
  val_policy_top5_acc: 0.629970   (epoch 7)   mode=max
  val_value_loss: 0.197578   (epoch 6)   mode=min
  val_value_value_mse: 0.197601   (epoch 6)   mode=min
  value_loss: 0.193012   (epoch 7)   mode=min
  value_value_mse: 0.193012   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6554, 2.6181, 2.5901, 2.5296, 2.5097, 2.4782, 2.4618]
  policy_loss : [2.5528, 2.5167, 2.4894, 2.4311, 2.4121, 2.3810, 2.3654]
  policy_policy_acc : [0.2702, 0.2769, 0.2824, 0.2955, 0.2986, 0.3052, 0.3081]
  policy_top10_acc : [0.7703, 0.7802, 0.7868, 0.8001, 0.8040, 0.8112, 0.8135]
  policy_top5_acc : [0.6093, 0.6181, 0.6257, 0.6428, 0.6468, 0.6543, 0.6598]
  val_loss : [2.5971, 2.6174, 2.6259, 2.7307, 2.6367, 2.7487, 2.6650]
  val_policy_loss : [2.4982, 2.5180, 2.5232, 2.6315, 2.5368, 2.6498, 2.5658]
  val_policy_policy_acc : [0.2821, 0.2826, 0.2874, 0.2943, 0.2915, 0.2910, 0.2941]
  val_policy_top10_acc : [0.7842, 0.7784, 0.7826, 0.7833, 0.7835, 0.7842, 0.7864]
  val_policy_top5_acc : [0.6169, 0.6184, 0.6252, 0.6265, 0.6257, 0.6264, 0.6300]
  val_value_loss : [0.1976, 0.1988, 0.2052, 0.1981, 0.1997, 0.1976, 0.1982]
  val_value_value_mse : [0.1976, 0.1988, 0.2052, 0.1981, 0.1997, 0.1976, 0.1983]
  value_loss : [0.2054, 0.2033, 0.2009, 0.1969, 0.1958, 0.1941, 0.1930]
  value_value_mse : [0.2054, 0.2033, 0.2009, 0.1968, 0.1958, 0.1940, 0.1930]

================================================================================

History file: model_versions/chess_elo_model_V20_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T09:53:10.205094Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game135001_game136500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V19
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.434966   (epoch 8)   mode=min
  policy_loss: 2.345972   (epoch 8)   mode=min
  policy_policy_acc: 0.313492   (epoch 8)   mode=max
  policy_top10_acc: 0.819229   (epoch 8)   mode=max
  policy_top5_acc: 0.665359   (epoch 8)   mode=max
  val_loss: 2.596915   (epoch 2)   mode=min
  val_policy_loss: 2.497174   (epoch 2)   mode=min
  val_policy_policy_acc: 0.293107   (epoch 7)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 3)   mode=max
  val_policy_top5_acc: 0.630270   (epoch 7)   mode=max
  val_value_loss: 0.198466   (epoch 1)   mode=min
  val_value_value_mse: 0.198487   (epoch 1)   mode=min
  value_loss: 0.178144   (epoch 8)   mode=min
  value_value_mse: 0.178146   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6471, 2.6127, 2.5801, 2.5546, 2.4984, 2.4799, 2.4476, 2.4350]
  policy_loss : [2.5505, 2.5171, 2.4860, 2.4616, 2.4072, 2.3893, 2.3582, 2.3460]
  policy_policy_acc : [0.2716, 0.2778, 0.2838, 0.2891, 0.3000, 0.3029, 0.3096, 0.3135]
  policy_top10_acc : [0.7746, 0.7828, 0.7889, 0.7957, 0.8073, 0.8114, 0.8169, 0.8192]
  policy_top5_acc : [0.6121, 0.6212, 0.6293, 0.6364, 0.6497, 0.6545, 0.6628, 0.6654]
  val_loss : [2.6991, 2.5969, 2.6432, 2.8390, 2.6731, 2.6790, 2.7933, 2.7179]
  val_policy_loss : [2.5997, 2.4972, 2.5416, 2.7369, 2.5696, 2.5777, 2.6920, 2.6167]
  val_policy_policy_acc : [0.2793, 0.2877, 0.2862, 0.2824, 0.2915, 0.2889, 0.2931, 0.2921]
  val_policy_top10_acc : [0.7799, 0.7823, 0.7853, 0.7805, 0.7807, 0.7815, 0.7807, 0.7816]
  val_policy_top5_acc : [0.6208, 0.6255, 0.6208, 0.6234, 0.6300, 0.6270, 0.6303, 0.6287]
  val_value_loss : [0.1985, 0.1994, 0.2032, 0.2037, 0.2069, 0.2025, 0.2023, 0.2023]
  val_value_value_mse : [0.1985, 0.1994, 0.2032, 0.2037, 0.2069, 0.2025, 0.2023, 0.2023]
  value_loss : [0.1933, 0.1913, 0.1887, 0.1862, 0.1821, 0.1811, 0.1789, 0.1781]
  value_value_mse : [0.1933, 0.1913, 0.1887, 0.1862, 0.1821, 0.1811, 0.1789, 0.1781]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:03:22.322563Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game136501_game138000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.412010   (epoch 10)   mode=min
  policy_loss: 2.314561   (epoch 10)   mode=min
  policy_policy_acc: 0.320389   (epoch 10)   mode=max
  policy_top10_acc: 0.825060   (epoch 10)   mode=max
  policy_top5_acc: 0.674101   (epoch 10)   mode=max
  val_loss: 2.597670   (epoch 5)   mode=min
  val_policy_loss: 2.497469   (epoch 5)   mode=min
  val_policy_policy_acc: 0.289810   (epoch 9)   mode=max
  val_policy_top10_acc: 0.789610   (epoch 4)   mode=max
  val_policy_top5_acc: 0.625475   (epoch 5)   mode=max
  val_value_loss: 0.200345   (epoch 5)   mode=min
  val_value_value_mse: 0.200352   (epoch 5)   mode=min
  value_loss: 0.194848   (epoch 10)   mode=min
  value_value_mse: 0.194849   (epoch 10)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0003, 0.0003, 0.0001]
  loss : [2.6519, 2.6099, 2.5842, 2.5194, 2.5018, 2.4878, 2.4689, 2.4419, 2.4271, 2.4120]
  policy_loss : [2.5475, 2.5064, 2.4815, 2.4186, 2.4018, 2.3881, 2.3699, 2.3437, 2.3291, 2.3146]
  policy_policy_acc : [0.2722, 0.2803, 0.2855, 0.2993, 0.3024, 0.3054, 0.3084, 0.3137, 0.3177, 0.3204]
  policy_top10_acc : [0.7758, 0.7844, 0.7915, 0.8042, 0.8075, 0.8104, 0.8132, 0.8203, 0.8218, 0.8251]
  policy_top5_acc : [0.6118, 0.6231, 0.6314, 0.6464, 0.6496, 0.6543, 0.6594, 0.6659, 0.6703, 0.6741]
  val_loss : [2.6054, 2.6050, 2.6099, 2.6015, 2.5977, 2.6113, 2.6077, 2.6575, 2.6057, 2.6537]
  val_policy_loss : [2.5004, 2.5041, 2.5068, 2.5004, 2.4975, 2.5092, 2.5071, 2.5570, 2.5052, 2.5532]
  val_policy_policy_acc : [0.2846, 0.2853, 0.2834, 0.2854, 0.2857, 0.2890, 0.2891, 0.2868, 0.2898, 0.2898]
  val_policy_top10_acc : [0.7841, 0.7851, 0.7829, 0.7896, 0.7872, 0.7806, 0.7800, 0.7819, 0.7827, 0.7808]
  val_policy_top5_acc : [0.6195, 0.6225, 0.6206, 0.6230, 0.6255, 0.6210, 0.6255, 0.6230, 0.6253, 0.6240]
  val_value_loss : [0.2098, 0.2018, 0.2059, 0.2021, 0.2003, 0.2041, 0.2009, 0.2008, 0.2008, 0.2009]
  val_value_value_mse : [0.2099, 0.2019, 0.2059, 0.2021, 0.2004, 0.2041, 0.2009, 0.2008, 0.2008, 0.2009]
  value_loss : [0.2087, 0.2070, 0.2055, 0.2016, 0.1998, 0.1993, 0.1979, 0.1965, 0.1961, 0.1948]
  value_value_mse : [0.2087, 0.2070, 0.2055, 0.2016, 0.1998, 0.1993, 0.1979, 0.1965, 0.1961, 0.1948]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:11:42.827385Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game138001_game139500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.432138   (epoch 8)   mode=min
  policy_loss: 2.337451   (epoch 8)   mode=min
  policy_policy_acc: 0.315237   (epoch 8)   mode=max
  policy_top10_acc: 0.819634   (epoch 8)   mode=max
  policy_top5_acc: 0.665505   (epoch 8)   mode=max
  val_loss: 2.607302   (epoch 2)   mode=min
  val_policy_loss: 2.505869   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287213   (epoch 5)   mode=max
  val_policy_top10_acc: 0.784116   (epoch 6)   mode=max
  val_policy_top5_acc: 0.623477   (epoch 7)   mode=max
  val_value_loss: 0.200549   (epoch 7)   mode=min
  val_value_value_mse: 0.200553   (epoch 7)   mode=min
  value_loss: 0.189387   (epoch 8)   mode=min
  value_value_mse: 0.189386   (epoch 8)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6461, 2.6092, 2.5776, 2.5508, 2.4998, 2.4785, 2.4442, 2.4321]
  policy_loss : [2.5438, 2.5083, 2.4781, 2.4517, 2.4026, 2.3822, 2.3491, 2.3375]
  policy_policy_acc : [0.2730, 0.2797, 0.2857, 0.2914, 0.3009, 0.3060, 0.3123, 0.3152]
  policy_top10_acc : [0.7746, 0.7821, 0.7902, 0.7957, 0.8071, 0.8108, 0.8173, 0.8196]
  policy_top5_acc : [0.6125, 0.6222, 0.6316, 0.6381, 0.6507, 0.6556, 0.6637, 0.6655]
  val_loss : [2.6179, 2.6073, 2.6212, 2.6178, 2.6544, 2.6206, 2.6356, 2.7258]
  val_policy_loss : [2.5157, 2.5059, 2.5205, 2.5170, 2.5537, 2.5197, 2.5352, 2.6246]
  val_policy_policy_acc : [0.2836, 0.2854, 0.2804, 0.2817, 0.2872, 0.2809, 0.2837, 0.2807]
  val_policy_top10_acc : [0.7751, 0.7793, 0.7821, 0.7795, 0.7817, 0.7841, 0.7838, 0.7832]
  val_policy_top5_acc : [0.6134, 0.6185, 0.6186, 0.6197, 0.6205, 0.6212, 0.6235, 0.6196]
  val_value_loss : [0.2043, 0.2027, 0.2013, 0.2013, 0.2012, 0.2015, 0.2005, 0.2020]
  val_value_value_mse : [0.2043, 0.2027, 0.2013, 0.2013, 0.2012, 0.2015, 0.2006, 0.2020]
  value_loss : [0.2046, 0.2019, 0.1989, 0.1983, 0.1943, 0.1926, 0.1903, 0.1894]
  value_value_mse : [0.2046, 0.2019, 0.1989, 0.1983, 0.1943, 0.1926, 0.1903, 0.1894]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:18:42.993525Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game139501_game141000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.457519   (epoch 7)   mode=min
  policy_loss: 2.362401   (epoch 7)   mode=min
  policy_policy_acc: 0.310825   (epoch 7)   mode=max
  policy_top10_acc: 0.814609   (epoch 7)   mode=max
  policy_top5_acc: 0.660892   (epoch 7)   mode=max
  val_loss: 2.597796   (epoch 1)   mode=min
  val_policy_loss: 2.494752   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288811   (epoch 7)   mode=max
  val_policy_top10_acc: 0.784216   (epoch 4)   mode=max
  val_policy_top5_acc: 0.626873   (epoch 6)   mode=max
  val_value_loss: 0.198968   (epoch 7)   mode=min
  val_value_value_mse: 0.198992   (epoch 7)   mode=min
  value_loss: 0.190709   (epoch 7)   mode=min
  value_value_mse: 0.190714   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6524, 2.6106, 2.5801, 2.5224, 2.5020, 2.4721, 2.4575]
  policy_loss : [2.5518, 2.5110, 2.4814, 2.4255, 2.4057, 2.3766, 2.3624]
  policy_policy_acc : [0.2734, 0.2796, 0.2846, 0.2983, 0.3026, 0.3083, 0.3108]
  policy_top10_acc : [0.7731, 0.7820, 0.7902, 0.8012, 0.8061, 0.8117, 0.8146]
  policy_top5_acc : [0.6095, 0.6210, 0.6294, 0.6435, 0.6485, 0.6560, 0.6609]
  val_loss : [2.5978, 2.6356, 2.7453, 2.6909, 2.7276, 2.8113, 2.9118]
  val_policy_loss : [2.4948, 2.5328, 2.6443, 2.5910, 2.6276, 2.7113, 2.8121]
  val_policy_policy_acc : [0.2859, 0.2789, 0.2872, 0.2884, 0.2878, 0.2875, 0.2888]
  val_policy_top10_acc : [0.7826, 0.7783, 0.7818, 0.7842, 0.7836, 0.7834, 0.7834]
  val_policy_top5_acc : [0.6252, 0.6122, 0.6197, 0.6261, 0.6261, 0.6269, 0.6257]
  val_value_loss : [0.2060, 0.2056, 0.2019, 0.1995, 0.1996, 0.1998, 0.1990]
  val_value_value_mse : [0.2060, 0.2056, 0.2019, 0.1995, 0.1997, 0.1998, 0.1990]
  value_loss : [0.2017, 0.1994, 0.1973, 0.1939, 0.1923, 0.1910, 0.1907]
  value_value_mse : [0.2017, 0.1994, 0.1973, 0.1940, 0.1923, 0.1910, 0.1907]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:26:00.508480Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game141001_game142500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.460252   (epoch 7)   mode=min
  policy_loss: 2.363657   (epoch 7)   mode=min
  policy_policy_acc: 0.309617   (epoch 7)   mode=max
  policy_top10_acc: 0.812577   (epoch 7)   mode=max
  policy_top5_acc: 0.659025   (epoch 7)   mode=max
  val_loss: 2.595232   (epoch 1)   mode=min
  val_policy_loss: 2.494190   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289011   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784715   (epoch 6)   mode=max
  val_policy_top5_acc: 0.623976   (epoch 1)   mode=max
  val_value_loss: 0.199149   (epoch 7)   mode=min
  val_value_value_mse: 0.199172   (epoch 7)   mode=min
  value_loss: 0.192793   (epoch 7)   mode=min
  value_value_mse: 0.192793   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6465, 2.6102, 2.5795, 2.5262, 2.5018, 2.4711, 2.4603]
  policy_loss : [2.5432, 2.5088, 2.4791, 2.4274, 2.4036, 2.3741, 2.3637]
  policy_policy_acc : [0.2738, 0.2789, 0.2847, 0.2968, 0.3014, 0.3075, 0.3096]
  policy_top10_acc : [0.7734, 0.7830, 0.7874, 0.8007, 0.8050, 0.8110, 0.8126]
  policy_top5_acc : [0.6110, 0.6196, 0.6275, 0.6429, 0.6478, 0.6572, 0.6590]
  val_loss : [2.5952, 2.6092, 2.6305, 2.5978, 2.6005, 2.5959, 2.5971]
  val_policy_loss : [2.4942, 2.5083, 2.5284, 2.4963, 2.4992, 2.4954, 2.4975]
  val_policy_policy_acc : [0.2890, 0.2837, 0.2827, 0.2861, 0.2857, 0.2873, 0.2819]
  val_policy_top10_acc : [0.7822, 0.7803, 0.7785, 0.7823, 0.7824, 0.7847, 0.7814]
  val_policy_top5_acc : [0.6240, 0.6208, 0.6148, 0.6198, 0.6223, 0.6232, 0.6213]
  val_value_loss : [0.2020, 0.2016, 0.2040, 0.2028, 0.2025, 0.2010, 0.1991]
  val_value_value_mse : [0.2020, 0.2017, 0.2040, 0.2028, 0.2025, 0.2010, 0.1992]
  value_loss : [0.2063, 0.2029, 0.2009, 0.1974, 0.1959, 0.1940, 0.1928]
  value_value_mse : [0.2063, 0.2029, 0.2009, 0.1974, 0.1959, 0.1939, 0.1928]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:33:26.863476Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game142501_game144000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.441975   (epoch 7)   mode=min
  policy_loss: 2.348759   (epoch 7)   mode=min
  policy_policy_acc: 0.311308   (epoch 7)   mode=max
  policy_top10_acc: 0.818688   (epoch 7)   mode=max
  policy_top5_acc: 0.663678   (epoch 7)   mode=max
  val_loss: 2.605703   (epoch 1)   mode=min
  val_policy_loss: 2.506212   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289311   (epoch 6)   mode=max
  val_policy_top10_acc: 0.791508   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630769   (epoch 6)   mode=max
  val_value_loss: 0.197562   (epoch 5)   mode=min
  val_value_value_mse: 0.197582   (epoch 5)   mode=min
  value_loss: 0.186241   (epoch 7)   mode=min
  value_value_mse: 0.186249   (epoch 7)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6285, 2.5909, 2.5642, 2.5074, 2.4838, 2.4553, 2.4420]
  policy_loss : [2.5285, 2.4928, 2.4671, 2.4122, 2.3892, 2.3618, 2.3488]
  policy_policy_acc : [0.2738, 0.2812, 0.2883, 0.2977, 0.3045, 0.3095, 0.3113]
  policy_top10_acc : [0.7790, 0.7878, 0.7921, 0.8053, 0.8108, 0.8159, 0.8187]
  policy_top5_acc : [0.6152, 0.6270, 0.6332, 0.6479, 0.6537, 0.6605, 0.6637]
  val_loss : [2.6057, 2.6099, 2.6340, 2.6058, 2.6189, 2.6184, 2.6088]
  val_policy_loss : [2.5062, 2.5091, 2.5346, 2.5064, 2.5201, 2.5195, 2.5089]
  val_policy_policy_acc : [0.2828, 0.2802, 0.2814, 0.2866, 0.2836, 0.2893, 0.2892]
  val_policy_top10_acc : [0.7915, 0.7821, 0.7829, 0.7825, 0.7828, 0.7851, 0.7848]
  val_policy_top5_acc : [0.6249, 0.6178, 0.6196, 0.6222, 0.6244, 0.6308, 0.6291]
  val_value_loss : [0.1990, 0.2015, 0.1987, 0.1988, 0.1976, 0.1978, 0.1998]
  val_value_value_mse : [0.1990, 0.2015, 0.1987, 0.1988, 0.1976, 0.1978, 0.1998]
  value_loss : [0.1998, 0.1965, 0.1946, 0.1904, 0.1887, 0.1870, 0.1862]
  value_value_mse : [0.1998, 0.1965, 0.1946, 0.1905, 0.1887, 0.1870, 0.1862]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:42:44.326771Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game144001_game145500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.408636   (epoch 9)   mode=min
  policy_loss: 2.311462   (epoch 9)   mode=min
  policy_policy_acc: 0.322671   (epoch 9)   mode=max
  policy_top10_acc: 0.823605   (epoch 9)   mode=max
  policy_top5_acc: 0.671299   (epoch 9)   mode=max
  val_loss: 2.605563   (epoch 3)   mode=min
  val_policy_loss: 2.505717   (epoch 3)   mode=min
  val_policy_policy_acc: 0.288911   (epoch 8)   mode=max
  val_policy_top10_acc: 0.783716   (epoch 6)   mode=max
  val_policy_top5_acc: 0.625874   (epoch 8)   mode=max
  val_value_loss: 0.198431   (epoch 8)   mode=min
  val_value_value_mse: 0.198445   (epoch 8)   mode=min
  value_loss: 0.194639   (epoch 9)   mode=min
  value_value_mse: 0.194658   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6395, 2.6051, 2.5769, 2.5483, 2.5260, 2.4705, 2.4511, 2.4212, 2.4086]
  policy_loss : [2.5349, 2.5016, 2.4744, 2.4472, 2.4257, 2.3714, 2.3532, 2.3230, 2.3115]
  policy_policy_acc : [0.2777, 0.2845, 0.2873, 0.2935, 0.2980, 0.3100, 0.3132, 0.3191, 0.3227]
  policy_top10_acc : [0.7781, 0.7844, 0.7914, 0.7977, 0.8012, 0.8133, 0.8163, 0.8228, 0.8236]
  policy_top5_acc : [0.6155, 0.6247, 0.6304, 0.6373, 0.6443, 0.6574, 0.6609, 0.6696, 0.6713]
  val_loss : [2.6335, 2.6313, 2.6056, 2.6205, 2.6791, 2.6593, 2.6817, 2.6459, 2.9051]
  val_policy_loss : [2.5329, 2.5295, 2.5057, 2.5206, 2.5779, 2.5598, 2.5811, 2.5467, 2.8047]
  val_policy_policy_acc : [0.2784, 0.2819, 0.2811, 0.2829, 0.2808, 0.2853, 0.2850, 0.2889, 0.2869]
  val_policy_top10_acc : [0.7772, 0.7808, 0.7772, 0.7830, 0.7796, 0.7837, 0.7806, 0.7821, 0.7816]
  val_policy_top5_acc : [0.6166, 0.6168, 0.6186, 0.6221, 0.6204, 0.6223, 0.6219, 0.6259, 0.6228]
  val_value_loss : [0.2010, 0.2036, 0.1997, 0.1999, 0.2023, 0.1989, 0.2011, 0.1984, 0.2006]
  val_value_value_mse : [0.2010, 0.2036, 0.1998, 0.1999, 0.2023, 0.1989, 0.2011, 0.1984, 0.2007]
  value_loss : [0.2094, 0.2066, 0.2051, 0.2029, 0.2013, 0.1991, 0.1972, 0.1959, 0.1946]
  value_value_mse : [0.2094, 0.2066, 0.2050, 0.2030, 0.2012, 0.1991, 0.1972, 0.1959, 0.1947]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T10:53:28.101259Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game145501_game147000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.437170   (epoch 10)   mode=min
  policy_loss: 2.341095   (epoch 10)   mode=min
  policy_policy_acc: 0.312337   (epoch 10)   mode=max
  policy_top10_acc: 0.820300   (epoch 9)   mode=max
  policy_top5_acc: 0.664022   (epoch 10)   mode=max
  val_loss: 2.589262   (epoch 6)   mode=min
  val_policy_loss: 2.490913   (epoch 6)   mode=min
  val_policy_policy_acc: 0.290809   (epoch 10)   mode=max
  val_policy_top10_acc: 0.785015   (epoch 4)   mode=max
  val_policy_top5_acc: 0.629870   (epoch 10)   mode=max
  val_value_loss: 0.196442   (epoch 6)   mode=min
  val_value_value_mse: 0.196466   (epoch 6)   mode=min
  value_loss: 0.191930   (epoch 9)   mode=min
  value_value_mse: 0.191931   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003, 0.0003, 0.0001, 0.0001]
  loss : [2.6589, 2.6180, 2.5862, 2.5311, 2.5116, 2.4786, 2.4680, 2.4593, 2.4419, 2.4372]
  policy_loss : [2.5558, 2.5162, 2.4855, 2.4326, 2.4136, 2.3817, 2.3711, 2.3625, 2.3459, 2.3411]
  policy_policy_acc : [0.2683, 0.2774, 0.2829, 0.2950, 0.2973, 0.3041, 0.3058, 0.3083, 0.3112, 0.3123]
  policy_top10_acc : [0.7721, 0.7823, 0.7891, 0.8011, 0.8045, 0.8106, 0.8128, 0.8152, 0.8203, 0.8196]
  policy_top5_acc : [0.6084, 0.6187, 0.6270, 0.6419, 0.6459, 0.6544, 0.6570, 0.6596, 0.6640, 0.6640]
  val_loss : [2.5988, 2.6170, 2.6164, 2.6540, 2.5955, 2.5893, 2.5923, 2.5943, 2.6926, 2.6009]
  val_policy_loss : [2.4958, 2.5157, 2.5166, 2.5554, 2.4965, 2.4909, 2.4939, 2.4957, 2.5942, 2.5025]
  val_policy_policy_acc : [0.2837, 0.2819, 0.2810, 0.2883, 0.2847, 0.2875, 0.2899, 0.2876, 0.2876, 0.2908]
  val_policy_top10_acc : [0.7816, 0.7770, 0.7814, 0.7850, 0.7807, 0.7817, 0.7810, 0.7833, 0.7810, 0.7825]
  val_policy_top5_acc : [0.6155, 0.6170, 0.6179, 0.6266, 0.6237, 0.6274, 0.6271, 0.6280, 0.6267, 0.6299]
  val_value_loss : [0.2058, 0.2024, 0.1994, 0.1970, 0.1979, 0.1964, 0.1966, 0.1971, 0.1965, 0.1967]
  val_value_value_mse : [0.2058, 0.2025, 0.1994, 0.1970, 0.1979, 0.1965, 0.1967, 0.1971, 0.1965, 0.1967]
  value_loss : [0.2062, 0.2037, 0.2014, 0.1971, 0.1960, 0.1938, 0.1938, 0.1934, 0.1919, 0.1921]
  value_value_mse : [0.2062, 0.2037, 0.2014, 0.1971, 0.1960, 0.1938, 0.1938, 0.1934, 0.1919, 0.1921]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T11:02:51.569347Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game147001_game148500.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.395860   (epoch 9)   mode=min
  policy_loss: 2.303990   (epoch 9)   mode=min
  policy_policy_acc: 0.320683   (epoch 9)   mode=max
  policy_top10_acc: 0.826854   (epoch 9)   mode=max
  policy_top5_acc: 0.675213   (epoch 9)   mode=max
  val_loss: 2.608480   (epoch 3)   mode=min
  val_policy_loss: 2.506881   (epoch 3)   mode=min
  val_policy_policy_acc: 0.287912   (epoch 8)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 9)   mode=max
  val_policy_top5_acc: 0.624376   (epoch 8)   mode=max
  val_value_loss: 0.197206   (epoch 1)   mode=min
  val_value_value_mse: 0.197242   (epoch 1)   mode=min
  value_loss: 0.183967   (epoch 9)   mode=min
  value_value_mse: 0.183980   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6308, 2.5893, 2.5605, 2.5345, 2.5143, 2.4605, 2.4379, 2.4069, 2.3959]
  policy_loss : [2.5313, 2.4914, 2.4630, 2.4385, 2.4186, 2.3666, 2.3446, 2.3150, 2.3040]
  policy_policy_acc : [0.2750, 0.2830, 0.2881, 0.2942, 0.2973, 0.3091, 0.3134, 0.3195, 0.3207]
  policy_top10_acc : [0.7800, 0.7890, 0.7955, 0.7994, 0.8041, 0.8151, 0.8188, 0.8251, 0.8269]
  policy_top5_acc : [0.6166, 0.6266, 0.6360, 0.6401, 0.6458, 0.6603, 0.6628, 0.6725, 0.6752]
  val_loss : [2.6097, 2.6146, 2.6085, 2.6275, 2.6267, 2.6792, 2.6219, 2.7577, 2.6323]
  val_policy_loss : [2.5111, 2.5126, 2.5069, 2.5266, 2.5267, 2.5767, 2.5225, 2.6572, 2.5312]
  val_policy_policy_acc : [0.2793, 0.2824, 0.2804, 0.2840, 0.2781, 0.2851, 0.2838, 0.2879, 0.2861]
  val_policy_top10_acc : [0.7760, 0.7785, 0.7793, 0.7805, 0.7754, 0.7811, 0.7800, 0.7783, 0.7814]
  val_policy_top5_acc : [0.6183, 0.6226, 0.6175, 0.6189, 0.6142, 0.6204, 0.6214, 0.6244, 0.6211]
  val_value_loss : [0.1972, 0.2039, 0.2031, 0.2017, 0.2000, 0.2046, 0.1986, 0.2006, 0.2020]
  val_value_value_mse : [0.1972, 0.2039, 0.2032, 0.2017, 0.2001, 0.2046, 0.1987, 0.2007, 0.2020]
  value_loss : [0.1991, 0.1963, 0.1950, 0.1924, 0.1908, 0.1881, 0.1871, 0.1847, 0.1840]
  value_value_mse : [0.1991, 0.1963, 0.1950, 0.1924, 0.1908, 0.1881, 0.1871, 0.1847, 0.1840]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T11:12:20.527861Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game148501_game150000.npz
  epochs: 10
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.397687   (epoch 9)   mode=min
  policy_loss: 2.305782   (epoch 9)   mode=min
  policy_policy_acc: 0.320644   (epoch 9)   mode=max
  policy_top10_acc: 0.826706   (epoch 9)   mode=max
  policy_top5_acc: 0.675920   (epoch 9)   mode=max
  val_loss: 2.605828   (epoch 3)   mode=min
  val_policy_loss: 2.505437   (epoch 3)   mode=min
  val_policy_policy_acc: 0.287612   (epoch 8)   mode=max
  val_policy_top10_acc: 0.784216   (epoch 6)   mode=max
  val_policy_top5_acc: 0.627872   (epoch 6)   mode=max
  val_value_loss: 0.199175   (epoch 4)   mode=min
  val_value_value_mse: 0.199198   (epoch 4)   mode=min
  value_loss: 0.183778   (epoch 9)   mode=min
  value_value_mse: 0.183789   (epoch 9)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0005, 0.0005, 0.0003, 0.0003]
  loss : [2.6329, 2.5942, 2.5653, 2.5367, 2.5142, 2.4598, 2.4400, 2.4102, 2.3977]
  policy_loss : [2.5340, 2.4963, 2.4688, 2.4408, 2.4189, 2.3658, 2.3470, 2.3181, 2.3058]
  policy_policy_acc : [0.2759, 0.2819, 0.2878, 0.2923, 0.2971, 0.3094, 0.3130, 0.3186, 0.3206]
  policy_top10_acc : [0.7773, 0.7872, 0.7927, 0.7985, 0.8044, 0.8147, 0.8187, 0.8239, 0.8267]
  policy_top5_acc : [0.6142, 0.6247, 0.6335, 0.6421, 0.6473, 0.6599, 0.6639, 0.6717, 0.6759]
  val_loss : [2.6088, 2.6255, 2.6058, 2.6216, 2.7796, 2.6580, 2.7203, 2.7123, 2.7981]
  val_policy_loss : [2.5089, 2.5236, 2.5054, 2.5220, 2.6761, 2.5578, 2.6199, 2.6125, 2.6977]
  val_policy_policy_acc : [0.2813, 0.2802, 0.2818, 0.2816, 0.2814, 0.2836, 0.2830, 0.2876, 0.2864]
  val_policy_top10_acc : [0.7786, 0.7782, 0.7819, 0.7796, 0.7752, 0.7842, 0.7803, 0.7826, 0.7818]
  val_policy_top5_acc : [0.6178, 0.6217, 0.6252, 0.6185, 0.6196, 0.6279, 0.6224, 0.6264, 0.6243]
  val_value_loss : [0.1997, 0.2038, 0.2007, 0.1992, 0.2067, 0.2005, 0.2004, 0.1993, 0.2003]
  val_value_value_mse : [0.1997, 0.2038, 0.2007, 0.1992, 0.2067, 0.2005, 0.2005, 0.1994, 0.2003]
  value_loss : [0.1978, 0.1960, 0.1931, 0.1918, 0.1904, 0.1881, 0.1859, 0.1841, 0.1838]
  value_value_mse : [0.1978, 0.1960, 0.1931, 0.1918, 0.1904, 0.1881, 0.1859, 0.1841, 0.1838]

================================================================================

History file: model_versions/chess_elo_model_V21_history.npy
Total runs: 12
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T17:39:38.146464Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.596996   (epoch 2)   mode=min
  policy_loss: 2.499063   (epoch 2)   mode=min
  policy_policy_acc: 0.282833   (epoch 2)   mode=max
  policy_top10_acc: 0.785520   (epoch 2)   mode=max
  policy_top5_acc: 0.625121   (epoch 2)   mode=max
  val_loss: 2.595948   (epoch 1)   mode=min
  val_policy_loss: 2.497213   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283217   (epoch 1)   mode=max
  val_policy_top10_acc: 0.781618   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622178   (epoch 1)   mode=max
  val_value_loss: 0.197306   (epoch 1)   mode=min
  val_value_value_mse: 0.197322   (epoch 1)   mode=min
  value_loss: 0.195920   (epoch 2)   mode=min
  value_value_mse: 0.195922   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6348, 2.5970]
  policy_loss : [2.5358, 2.4991]
  policy_policy_acc : [0.2758, 0.2828]
  policy_top10_acc : [0.7774, 0.7855]
  policy_top5_acc : [0.6164, 0.6251]
  val_loss : [2.5959, 2.6056]
  val_policy_loss : [2.4972, 2.5037]
  val_policy_policy_acc : [0.2832, 0.2812]
  val_policy_top10_acc : [0.7816, 0.7815]
  val_policy_top5_acc : [0.6222, 0.6221]
  val_value_loss : [0.1973, 0.2036]
  val_value_value_mse : [0.1973, 0.2036]
  value_loss : [0.1980, 0.1959]
  value_value_mse : [0.1980, 0.1959]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-17T17:52:12.784135Z
Args (salvati):
  alpha: 2.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.602794   (epoch 2)   mode=min
  policy_loss: 2.504896   (epoch 2)   mode=min
  policy_policy_acc: 0.281424   (epoch 2)   mode=max
  policy_top10_acc: 0.784655   (epoch 2)   mode=max
  policy_top5_acc: 0.624688   (epoch 2)   mode=max
  val_loss: 2.613402   (epoch 1)   mode=min
  val_policy_loss: 2.514322   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283017   (epoch 2)   mode=max
  val_policy_top10_acc: 0.779820   (epoch 2)   mode=max
  val_policy_top5_acc: 0.622378   (epoch 2)   mode=max
  val_value_loss: 0.198057   (epoch 1)   mode=min
  val_value_value_mse: 0.198083   (epoch 1)   mode=min
  value_loss: 0.195768   (epoch 2)   mode=min
  value_value_mse: 0.195771   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6485, 2.6028]
  policy_loss : [2.5492, 2.5049]
  policy_policy_acc : [0.2737, 0.2814]
  policy_top10_acc : [0.7754, 0.7847]
  policy_top5_acc : [0.6153, 0.6247]
  val_loss : [2.6134, 2.6151]
  val_policy_loss : [2.5143, 2.5149]
  val_policy_policy_acc : [0.2810, 0.2830]
  val_policy_top10_acc : [0.7789, 0.7798]
  val_policy_top5_acc : [0.6206, 0.6224]
  val_value_loss : [0.1981, 0.2004]
  val_value_value_mse : [0.1981, 0.2004]
  value_loss : [0.1986, 0.1958]
  value_value_mse : [0.1986, 0.1958]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T12:57:39.637700Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V20
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592963   (epoch 2)   mode=min
  policy_loss: 2.495249   (epoch 2)   mode=min
  policy_policy_acc: 0.282138   (epoch 2)   mode=max
  policy_top10_acc: 0.787241   (epoch 2)   mode=max
  policy_top5_acc: 0.627083   (epoch 2)   mode=max
  val_loss: 2.616061   (epoch 1)   mode=min
  val_policy_loss: 2.517263   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290210   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786014   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623976   (epoch 2)   mode=max
  val_value_loss: 0.197353   (epoch 1)   mode=min
  val_value_value_mse: 0.197389   (epoch 1)   mode=min
  value_loss: 0.195434   (epoch 2)   mode=min
  value_value_mse: 0.195437   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6358, 2.5930]
  policy_loss : [2.5366, 2.4952]
  policy_policy_acc : [0.2754, 0.2821]
  policy_top10_acc : [0.7773, 0.7872]
  policy_top5_acc : [0.6154, 0.6271]
  val_loss : [2.6161, 2.6260]
  val_policy_loss : [2.5173, 2.5259]
  val_policy_policy_acc : [0.2808, 0.2902]
  val_policy_top10_acc : [0.7811, 0.7860]
  val_policy_top5_acc : [0.6227, 0.6240]
  val_value_loss : [0.1974, 0.2000]
  val_value_value_mse : [0.1974, 0.2001]
  value_loss : [0.1984, 0.1954]
  value_value_mse : [0.1984, 0.1954]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:00:16.730970Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game151501_game153000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.607950   (epoch 2)   mode=min
  policy_loss: 2.506564   (epoch 2)   mode=min
  policy_policy_acc: 0.277803   (epoch 2)   mode=max
  policy_top10_acc: 0.786092   (epoch 2)   mode=max
  policy_top5_acc: 0.622455   (epoch 2)   mode=max
  val_loss: 2.598886   (epoch 2)   mode=min
  val_policy_loss: 2.498596   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284216   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 2)   mode=max
  val_policy_top5_acc: 0.622478   (epoch 2)   mode=max
  val_value_loss: 0.199063   (epoch 1)   mode=min
  val_value_value_mse: 0.199079   (epoch 1)   mode=min
  value_loss: 0.202611   (epoch 2)   mode=min
  value_value_mse: 0.202616   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6460, 2.6079]
  policy_loss : [2.5438, 2.5066]
  policy_policy_acc : [0.2698, 0.2778]
  policy_top10_acc : [0.7765, 0.7861]
  policy_top5_acc : [0.6135, 0.6225]
  val_loss : [2.6156, 2.5989]
  val_policy_loss : [2.5159, 2.4986]
  val_policy_policy_acc : [0.2813, 0.2842]
  val_policy_top10_acc : [0.7784, 0.7832]
  val_policy_top5_acc : [0.6186, 0.6225]
  val_value_loss : [0.1991, 0.2004]
  val_value_value_mse : [0.1991, 0.2004]
  value_loss : [0.2042, 0.2026]
  value_value_mse : [0.2042, 0.2026]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:03:12.587068Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game153001_game154500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.613730   (epoch 2)   mode=min
  policy_loss: 2.513496   (epoch 2)   mode=min
  policy_policy_acc: 0.275359   (epoch 2)   mode=max
  policy_top10_acc: 0.785211   (epoch 2)   mode=max
  policy_top5_acc: 0.622380   (epoch 2)   mode=max
  val_loss: 2.599768   (epoch 1)   mode=min
  val_policy_loss: 2.499130   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286114   (epoch 1)   mode=max
  val_policy_top10_acc: 0.781219   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624875   (epoch 1)   mode=max
  val_value_loss: 0.201049   (epoch 1)   mode=min
  val_value_value_mse: 0.201075   (epoch 1)   mode=min
  value_loss: 0.200648   (epoch 2)   mode=min
  value_value_mse: 0.200645   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6581, 2.6137]
  policy_loss : [2.5565, 2.5135]
  policy_policy_acc : [0.2655, 0.2754]
  policy_top10_acc : [0.7732, 0.7852]
  policy_top5_acc : [0.6093, 0.6224]
  val_loss : [2.5998, 2.7090]
  val_policy_loss : [2.4991, 2.6041]
  val_policy_policy_acc : [0.2861, 0.2808]
  val_policy_top10_acc : [0.7812, 0.7759]
  val_policy_top5_acc : [0.6249, 0.6133]
  val_value_loss : [0.2010, 0.2096]
  val_value_value_mse : [0.2011, 0.2096]
  value_loss : [0.2034, 0.2006]
  value_value_mse : [0.2034, 0.2006]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:06:19.123690Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game154501_game156000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601316   (epoch 2)   mode=min
  policy_loss: 2.501826   (epoch 2)   mode=min
  policy_policy_acc: 0.279726   (epoch 2)   mode=max
  policy_top10_acc: 0.786092   (epoch 2)   mode=max
  policy_top5_acc: 0.623000   (epoch 2)   mode=max
  val_loss: 2.602362   (epoch 1)   mode=min
  val_policy_loss: 2.500602   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285215   (epoch 1)   mode=max
  val_policy_top10_acc: 0.781718   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624875   (epoch 1)   mode=max
  val_value_loss: 0.198681   (epoch 2)   mode=min
  val_value_value_mse: 0.198712   (epoch 2)   mode=min
  value_loss: 0.198982   (epoch 2)   mode=min
  value_value_mse: 0.198988   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6395, 2.6013]
  policy_loss : [2.5387, 2.5018]
  policy_policy_acc : [0.2700, 0.2797]
  policy_top10_acc : [0.7768, 0.7861]
  policy_top5_acc : [0.6142, 0.6230]
  val_loss : [2.6024, 2.6435]
  val_policy_loss : [2.5006, 2.5441]
  val_policy_policy_acc : [0.2852, 0.2761]
  val_policy_top10_acc : [0.7800, 0.7817]
  val_policy_top5_acc : [0.6249, 0.6201]
  val_value_loss : [0.2034, 0.1987]
  val_value_value_mse : [0.2034, 0.1987]
  value_loss : [0.2016, 0.1990]
  value_value_mse : [0.2016, 0.1990]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:09:02.537585Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game156001_game157500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.626167   (epoch 2)   mode=min
  policy_loss: 2.527423   (epoch 2)   mode=min
  policy_policy_acc: 0.271986   (epoch 2)   mode=max
  policy_top10_acc: 0.780083   (epoch 2)   mode=max
  policy_top5_acc: 0.617721   (epoch 2)   mode=max
  val_loss: 2.597788   (epoch 2)   mode=min
  val_policy_loss: 2.497147   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285315   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784016   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624376   (epoch 2)   mode=max
  val_value_loss: 0.199653   (epoch 1)   mode=min
  val_value_value_mse: 0.199671   (epoch 1)   mode=min
  value_loss: 0.203468   (epoch 2)   mode=min
  value_value_mse: 0.203471   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6639, 2.6262]
  policy_loss : [2.5610, 2.5274]
  policy_policy_acc : [0.2665, 0.2720]
  policy_top10_acc : [0.7723, 0.7801]
  policy_top5_acc : [0.6064, 0.6177]
  val_loss : [2.6381, 2.5978]
  val_policy_loss : [2.5381, 2.4971]
  val_policy_policy_acc : [0.2811, 0.2853]
  val_policy_top10_acc : [0.7764, 0.7840]
  val_policy_top5_acc : [0.6162, 0.6244]
  val_value_loss : [0.1997, 0.2011]
  val_value_value_mse : [0.1997, 0.2011]
  value_loss : [0.2061, 0.2035]
  value_value_mse : [0.2060, 0.2035]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:11:31.373068Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game157501_game159000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.616462   (epoch 2)   mode=min
  policy_loss: 2.513509   (epoch 2)   mode=min
  policy_policy_acc: 0.278577   (epoch 2)   mode=max
  policy_top10_acc: 0.784428   (epoch 2)   mode=max
  policy_top5_acc: 0.622641   (epoch 2)   mode=max
  val_loss: 2.617700   (epoch 1)   mode=min
  val_policy_loss: 2.516427   (epoch 1)   mode=min
  val_policy_policy_acc: 0.279121   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782517   (epoch 2)   mode=max
  val_policy_top5_acc: 0.620779   (epoch 2)   mode=max
  val_value_loss: 0.198463   (epoch 2)   mode=min
  val_value_value_mse: 0.198470   (epoch 2)   mode=min
  value_loss: 0.205863   (epoch 2)   mode=min
  value_value_mse: 0.205811   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6590, 2.6165]
  policy_loss : [2.5555, 2.5135]
  policy_policy_acc : [0.2714, 0.2786]
  policy_top10_acc : [0.7732, 0.7844]
  policy_top5_acc : [0.6093, 0.6226]
  val_loss : [2.6177, 2.6264]
  val_policy_loss : [2.5164, 2.5271]
  val_policy_policy_acc : [0.2791, 0.2787]
  val_policy_top10_acc : [0.7753, 0.7825]
  val_policy_top5_acc : [0.6146, 0.6208]
  val_value_loss : [0.2024, 0.1985]
  val_value_value_mse : [0.2024, 0.1985]
  value_loss : [0.2085, 0.2059]
  value_value_mse : [0.2085, 0.2058]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:13:53.772432Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game159001_game160500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.610530   (epoch 2)   mode=min
  policy_loss: 2.508166   (epoch 2)   mode=min
  policy_policy_acc: 0.280041   (epoch 2)   mode=max
  policy_top10_acc: 0.784229   (epoch 2)   mode=max
  policy_top5_acc: 0.621361   (epoch 2)   mode=max
  val_loss: 2.618096   (epoch 1)   mode=min
  val_policy_loss: 2.514433   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783816   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622577   (epoch 2)   mode=max
  val_value_loss: 0.204435   (epoch 2)   mode=min
  val_value_value_mse: 0.204448   (epoch 2)   mode=min
  value_loss: 0.204887   (epoch 2)   mode=min
  value_value_mse: 0.204866   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6492, 2.6105]
  policy_loss : [2.5456, 2.5082]
  policy_policy_acc : [0.2721, 0.2800]
  policy_top10_acc : [0.7755, 0.7842]
  policy_top5_acc : [0.6114, 0.6214]
  val_loss : [2.6181, 2.6834]
  val_policy_loss : [2.5144, 2.5812]
  val_policy_policy_acc : [0.2851, 0.2807]
  val_policy_top10_acc : [0.7838, 0.7836]
  val_policy_top5_acc : [0.6194, 0.6226]
  val_value_loss : [0.2072, 0.2044]
  val_value_value_mse : [0.2072, 0.2044]
  value_loss : [0.2068, 0.2049]
  value_value_mse : [0.2068, 0.2049]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:16:07.306824Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game160501_game162000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.599939   (epoch 2)   mode=min
  policy_loss: 2.501082   (epoch 2)   mode=min
  policy_policy_acc: 0.278976   (epoch 2)   mode=max
  policy_top10_acc: 0.784800   (epoch 2)   mode=max
  policy_top5_acc: 0.623155   (epoch 2)   mode=max
  val_loss: 2.627819   (epoch 1)   mode=min
  val_policy_loss: 2.527664   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286114   (epoch 2)   mode=max
  val_policy_top10_acc: 0.780320   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623876   (epoch 2)   mode=max
  val_value_loss: 0.200342   (epoch 1)   mode=min
  val_value_value_mse: 0.200357   (epoch 1)   mode=min
  value_loss: 0.197567   (epoch 2)   mode=min
  value_value_mse: 0.197549   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6381, 2.5999]
  policy_loss : [2.5385, 2.5011]
  policy_policy_acc : [0.2714, 0.2790]
  policy_top10_acc : [0.7770, 0.7848]
  policy_top5_acc : [0.6158, 0.6232]
  val_loss : [2.6278, 2.6652]
  val_policy_loss : [2.5277, 2.5645]
  val_policy_policy_acc : [0.2796, 0.2861]
  val_policy_top10_acc : [0.7803, 0.7802]
  val_policy_top5_acc : [0.6184, 0.6239]
  val_value_loss : [0.2003, 0.2011]
  val_value_value_mse : [0.2004, 0.2012]
  value_loss : [0.1990, 0.1976]
  value_value_mse : [0.1990, 0.1975]

================================================================================

RUN #11
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:18:14.500833Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game162001_game163500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.597152   (epoch 2)   mode=min
  policy_loss: 2.494479   (epoch 2)   mode=min
  policy_policy_acc: 0.285865   (epoch 2)   mode=max
  policy_top10_acc: 0.786774   (epoch 2)   mode=max
  policy_top5_acc: 0.626670   (epoch 2)   mode=max
  val_loss: 2.642118   (epoch 2)   mode=min
  val_policy_loss: 2.539668   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284416   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788512   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628771   (epoch 1)   mode=max
  val_value_loss: 0.202430   (epoch 1)   mode=min
  val_value_value_mse: 0.202443   (epoch 1)   mode=min
  value_loss: 0.205032   (epoch 2)   mode=min
  value_value_mse: 0.205011   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6356, 2.5972]
  policy_loss : [2.5323, 2.4945]
  policy_policy_acc : [0.2782, 0.2859]
  policy_top10_acc : [0.7772, 0.7868]
  policy_top5_acc : [0.6168, 0.6267]
  val_loss : [3.1553, 2.6421]
  val_policy_loss : [3.0537, 2.5397]
  val_policy_policy_acc : [0.2844, 0.2824]
  val_policy_top10_acc : [0.7885, 0.7780]
  val_policy_top5_acc : [0.6288, 0.6234]
  val_value_loss : [0.2024, 0.2047]
  val_value_value_mse : [0.2024, 0.2047]
  value_loss : [0.2065, 0.2050]
  value_value_mse : [0.2065, 0.2050]

================================================================================

RUN #12
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:20:21.269967Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game163501_game165000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585832   (epoch 2)   mode=min
  policy_loss: 2.483083   (epoch 2)   mode=min
  policy_policy_acc: 0.285991   (epoch 2)   mode=max
  policy_top10_acc: 0.788211   (epoch 2)   mode=max
  policy_top5_acc: 0.630008   (epoch 2)   mode=max
  val_loss: 2.628251   (epoch 1)   mode=min
  val_policy_loss: 2.527642   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283317   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785215   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627173   (epoch 2)   mode=max
  val_value_loss: 0.200677   (epoch 2)   mode=min
  val_value_value_mse: 0.200694   (epoch 2)   mode=min
  value_loss: 0.205574   (epoch 2)   mode=min
  value_value_mse: 0.205570   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6258, 2.5858]
  policy_loss : [2.5219, 2.4831]
  policy_policy_acc : [0.2779, 0.2860]
  policy_top10_acc : [0.7800, 0.7882]
  policy_top5_acc : [0.6184, 0.6300]
  val_loss : [2.6283, 2.7227]
  val_policy_loss : [2.5276, 2.6222]
  val_policy_policy_acc : [0.2833, 0.2829]
  val_policy_top10_acc : [0.7827, 0.7852]
  val_policy_top5_acc : [0.6215, 0.6272]
  val_value_loss : [0.2011, 0.2007]
  val_value_value_mse : [0.2012, 0.2007]
  value_loss : [0.2078, 0.2056]
  value_value_mse : [0.2078, 0.2056]

================================================================================

History file: model_versions/chess_elo_model_V22_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:22:33.540357Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game165001_game166500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V21
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.599385   (epoch 2)   mode=min
  policy_loss: 2.495838   (epoch 2)   mode=min
  policy_policy_acc: 0.282174   (epoch 2)   mode=max
  policy_top10_acc: 0.786796   (epoch 2)   mode=max
  policy_top5_acc: 0.627606   (epoch 2)   mode=max
  val_loss: 2.633189   (epoch 2)   mode=min
  val_policy_loss: 2.530542   (epoch 2)   mode=min
  val_policy_policy_acc: 0.282717   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785614   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626873   (epoch 1)   mode=max
  val_value_loss: 0.200832   (epoch 1)   mode=min
  val_value_value_mse: 0.200859   (epoch 1)   mode=min
  value_loss: 0.207181   (epoch 2)   mode=min
  value_value_mse: 0.207166   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6340, 2.5994]
  policy_loss : [2.5298, 2.4958]
  policy_policy_acc : [0.2758, 0.2822]
  policy_top10_acc : [0.7780, 0.7868]
  policy_top5_acc : [0.6173, 0.6276]
  val_loss : [2.6443, 2.6332]
  val_policy_loss : [2.5437, 2.5305]
  val_policy_policy_acc : [0.2827, 0.2792]
  val_policy_top10_acc : [0.7856, 0.7758]
  val_policy_top5_acc : [0.6269, 0.6155]
  val_value_loss : [0.2008, 0.2051]
  val_value_value_mse : [0.2009, 0.2051]
  value_loss : [0.2086, 0.2072]
  value_value_mse : [0.2086, 0.2072]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:24:36.962602Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game166501_game168000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.608224   (epoch 2)   mode=min
  policy_loss: 2.508361   (epoch 2)   mode=min
  policy_policy_acc: 0.284466   (epoch 2)   mode=max
  policy_top10_acc: 0.783750   (epoch 2)   mode=max
  policy_top5_acc: 0.624227   (epoch 2)   mode=max
  val_loss: 4.490805   (epoch 2)   mode=min
  val_policy_loss: 4.388593   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286713   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783716   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626873   (epoch 2)   mode=max
  val_value_loss: 0.202127   (epoch 2)   mode=min
  val_value_value_mse: 0.202136   (epoch 2)   mode=min
  value_loss: 0.199768   (epoch 2)   mode=min
  value_value_mse: 0.199769   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6445, 2.6082]
  policy_loss : [2.5430, 2.5084]
  policy_policy_acc : [0.2751, 0.2845]
  policy_top10_acc : [0.7743, 0.7837]
  policy_top5_acc : [0.6133, 0.6242]
  val_loss : [5.1974, 4.4908]
  val_policy_loss : [5.0939, 4.3886]
  val_policy_policy_acc : [0.2867, 0.2813]
  val_policy_top10_acc : [0.7832, 0.7837]
  val_policy_top5_acc : [0.6243, 0.6269]
  val_value_loss : [0.2036, 0.2021]
  val_value_value_mse : [0.2036, 0.2021]
  value_loss : [0.2029, 0.1998]
  value_value_mse : [0.2029, 0.1998]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:26:47.518895Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game168001_game169500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.623652   (epoch 2)   mode=min
  policy_loss: 2.521971   (epoch 2)   mode=min
  policy_policy_acc: 0.278765   (epoch 2)   mode=max
  policy_top10_acc: 0.781247   (epoch 2)   mode=max
  policy_top5_acc: 0.619915   (epoch 2)   mode=max
  val_loss: 2.904599   (epoch 1)   mode=min
  val_policy_loss: 2.804200   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287812   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786613   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623177   (epoch 2)   mode=max
  val_value_loss: 0.198151   (epoch 2)   mode=min
  val_value_value_mse: 0.198181   (epoch 2)   mode=min
  value_loss: 0.203436   (epoch 2)   mode=min
  value_value_mse: 0.203439   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6648, 2.6237]
  policy_loss : [2.5614, 2.5220]
  policy_policy_acc : [0.2699, 0.2788]
  policy_top10_acc : [0.7707, 0.7812]
  policy_top5_acc : [0.6073, 0.6199]
  val_loss : [2.9046, 3.1778]
  val_policy_loss : [2.8042, 3.0783]
  val_policy_policy_acc : [0.2865, 0.2878]
  val_policy_top10_acc : [0.7769, 0.7866]
  val_policy_top5_acc : [0.6213, 0.6232]
  val_value_loss : [0.2004, 0.1982]
  val_value_value_mse : [0.2004, 0.1982]
  value_loss : [0.2068, 0.2034]
  value_value_mse : [0.2068, 0.2034]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:28:53.327469Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game169501_game171000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598638   (epoch 2)   mode=min
  policy_loss: 2.495543   (epoch 2)   mode=min
  policy_policy_acc: 0.281329   (epoch 2)   mode=max
  policy_top10_acc: 0.785720   (epoch 2)   mode=max
  policy_top5_acc: 0.625504   (epoch 2)   mode=max
  val_loss: 2.600555   (epoch 1)   mode=min
  val_policy_loss: 2.500287   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289411   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786514   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622677   (epoch 1)   mode=max
  val_value_loss: 0.199137   (epoch 2)   mode=min
  val_value_value_mse: 0.199153   (epoch 2)   mode=min
  value_loss: 0.206183   (epoch 2)   mode=min
  value_value_mse: 0.206182   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6367, 2.5986]
  policy_loss : [2.5326, 2.4955]
  policy_policy_acc : [0.2733, 0.2813]
  policy_top10_acc : [0.7779, 0.7857]
  policy_top5_acc : [0.6148, 0.6255]
  val_loss : [2.6006, 2.6006]
  val_policy_loss : [2.5003, 2.5009]
  val_policy_policy_acc : [0.2820, 0.2894]
  val_policy_top10_acc : [0.7865, 0.7828]
  val_policy_top5_acc : [0.6227, 0.6206]
  val_value_loss : [0.2004, 0.1991]
  val_value_value_mse : [0.2004, 0.1992]
  value_loss : [0.2081, 0.2062]
  value_value_mse : [0.2081, 0.2062]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:31:08.702302Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game171001_game172500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.604417   (epoch 2)   mode=min
  policy_loss: 2.506165   (epoch 2)   mode=min
  policy_policy_acc: 0.279522   (epoch 2)   mode=max
  policy_top10_acc: 0.784287   (epoch 2)   mode=max
  policy_top5_acc: 0.621864   (epoch 2)   mode=max
  val_loss: 2.621752   (epoch 1)   mode=min
  val_policy_loss: 2.522390   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286414   (epoch 1)   mode=max
  val_policy_top10_acc: 0.781818   (epoch 1)   mode=max
  val_policy_top5_acc: 0.621978   (epoch 2)   mode=max
  val_value_loss: 0.196567   (epoch 2)   mode=min
  val_value_value_mse: 0.196591   (epoch 2)   mode=min
  value_loss: 0.196268   (epoch 2)   mode=min
  value_value_mse: 0.196284   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6380, 2.6044]
  policy_loss : [2.5384, 2.5062]
  policy_policy_acc : [0.2725, 0.2795]
  policy_top10_acc : [0.7755, 0.7843]
  policy_top5_acc : [0.6121, 0.6219]
  val_loss : [2.6218, 2.6572]
  val_policy_loss : [2.5224, 2.5588]
  val_policy_policy_acc : [0.2864, 0.2862]
  val_policy_top10_acc : [0.7818, 0.7766]
  val_policy_top5_acc : [0.6190, 0.6220]
  val_value_loss : [0.1985, 0.1966]
  val_value_value_mse : [0.1985, 0.1966]
  value_loss : [0.1992, 0.1963]
  value_value_mse : [0.1992, 0.1963]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:33:56.049979Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game172501_game174000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603482   (epoch 2)   mode=min
  policy_loss: 2.505142   (epoch 2)   mode=min
  policy_policy_acc: 0.280960   (epoch 2)   mode=max
  policy_top10_acc: 0.783541   (epoch 2)   mode=max
  policy_top5_acc: 0.623930   (epoch 2)   mode=max
  val_loss: 2.605736   (epoch 1)   mode=min
  val_policy_loss: 2.505890   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285614   (epoch 2)   mode=max
  val_policy_top10_acc: 0.782418   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623477   (epoch 2)   mode=max
  val_value_loss: 0.199095   (epoch 2)   mode=min
  val_value_value_mse: 0.199116   (epoch 2)   mode=min
  value_loss: 0.196894   (epoch 2)   mode=min
  value_value_mse: 0.196898   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6384, 2.6035]
  policy_loss : [2.5384, 2.5051]
  policy_policy_acc : [0.2750, 0.2810]
  policy_top10_acc : [0.7774, 0.7835]
  policy_top5_acc : [0.6142, 0.6239]
  val_loss : [2.6057, 2.6249]
  val_policy_loss : [2.5059, 2.5253]
  val_policy_policy_acc : [0.2846, 0.2856]
  val_policy_top10_acc : [0.7824, 0.7806]
  val_policy_top5_acc : [0.6194, 0.6235]
  val_value_loss : [0.1997, 0.1991]
  val_value_value_mse : [0.1997, 0.1991]
  value_loss : [0.2003, 0.1969]
  value_value_mse : [0.2003, 0.1969]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:36:16.939181Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game174001_game175500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.608374   (epoch 2)   mode=min
  policy_loss: 2.512442   (epoch 2)   mode=min
  policy_policy_acc: 0.277875   (epoch 2)   mode=max
  policy_top10_acc: 0.782866   (epoch 2)   mode=max
  policy_top5_acc: 0.619522   (epoch 2)   mode=max
  val_loss: 2.586403   (epoch 1)   mode=min
  val_policy_loss: 2.486701   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288112   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786414   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622777   (epoch 1)   mode=max
  val_value_loss: 0.197279   (epoch 2)   mode=min
  val_value_value_mse: 0.197292   (epoch 2)   mode=min
  value_loss: 0.192033   (epoch 2)   mode=min
  value_value_mse: 0.192037   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6441, 2.6084]
  policy_loss : [2.5462, 2.5124]
  policy_policy_acc : [0.2720, 0.2779]
  policy_top10_acc : [0.7737, 0.7829]
  policy_top5_acc : [0.6100, 0.6195]
  val_loss : [2.5864, 2.5980]
  val_policy_loss : [2.4867, 2.4993]
  val_policy_policy_acc : [0.2881, 0.2829]
  val_policy_top10_acc : [0.7864, 0.7807]
  val_policy_top5_acc : [0.6228, 0.6184]
  val_value_loss : [0.1992, 0.1973]
  val_value_value_mse : [0.1992, 0.1973]
  value_loss : [0.1957, 0.1920]
  value_value_mse : [0.1957, 0.1920]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:38:35.479420Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game175501_game177000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585357   (epoch 2)   mode=min
  policy_loss: 2.485872   (epoch 2)   mode=min
  policy_policy_acc: 0.282879   (epoch 2)   mode=max
  policy_top10_acc: 0.788162   (epoch 2)   mode=max
  policy_top5_acc: 0.626924   (epoch 2)   mode=max
  val_loss: 2.605120   (epoch 2)   mode=min
  val_policy_loss: 2.504972   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284116   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787612   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626274   (epoch 2)   mode=max
  val_value_loss: 0.197615   (epoch 1)   mode=min
  val_value_value_mse: 0.197624   (epoch 1)   mode=min
  value_loss: 0.198948   (epoch 2)   mode=min
  value_value_mse: 0.198958   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6259, 2.5854]
  policy_loss : [2.5249, 2.4859]
  policy_policy_acc : [0.2768, 0.2829]
  policy_top10_acc : [0.7802, 0.7882]
  policy_top5_acc : [0.6162, 0.6269]
  val_loss : [2.6108, 2.6051]
  val_policy_loss : [2.5119, 2.5050]
  val_policy_policy_acc : [0.2770, 0.2841]
  val_policy_top10_acc : [0.7869, 0.7876]
  val_policy_top5_acc : [0.6224, 0.6263]
  val_value_loss : [0.1976, 0.2003]
  val_value_value_mse : [0.1976, 0.2003]
  value_loss : [0.2019, 0.1989]
  value_value_mse : [0.2019, 0.1990]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:40:50.374324Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game177001_game178500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.596867   (epoch 2)   mode=min
  policy_loss: 2.495414   (epoch 2)   mode=min
  policy_policy_acc: 0.281436   (epoch 2)   mode=max
  policy_top10_acc: 0.789418   (epoch 2)   mode=max
  policy_top5_acc: 0.625637   (epoch 2)   mode=max
  val_loss: 2.592283   (epoch 2)   mode=min
  val_policy_loss: 2.494469   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283317   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784915   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624376   (epoch 2)   mode=max
  val_value_loss: 0.195528   (epoch 2)   mode=min
  val_value_value_mse: 0.195546   (epoch 2)   mode=min
  value_loss: 0.202891   (epoch 2)   mode=min
  value_value_mse: 0.202897   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6344, 2.5969]
  policy_loss : [2.5322, 2.4954]
  policy_policy_acc : [0.2741, 0.2814]
  policy_top10_acc : [0.7792, 0.7894]
  policy_top5_acc : [0.6155, 0.6256]
  val_loss : [2.6182, 2.5923]
  val_policy_loss : [2.5189, 2.4945]
  val_policy_policy_acc : [0.2833, 0.2799]
  val_policy_top10_acc : [0.7849, 0.7827]
  val_policy_top5_acc : [0.6222, 0.6244]
  val_value_loss : [0.1985, 0.1955]
  val_value_value_mse : [0.1986, 0.1955]
  value_loss : [0.2043, 0.2029]
  value_value_mse : [0.2043, 0.2029]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:42:59.464075Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game178501_game180000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.593797   (epoch 2)   mode=min
  policy_loss: 2.495564   (epoch 2)   mode=min
  policy_policy_acc: 0.280463   (epoch 2)   mode=max
  policy_top10_acc: 0.786808   (epoch 2)   mode=max
  policy_top5_acc: 0.625391   (epoch 2)   mode=max
  val_loss: 2.579571   (epoch 2)   mode=min
  val_policy_loss: 2.481384   (epoch 2)   mode=min
  val_policy_policy_acc: 0.291009   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623277   (epoch 2)   mode=max
  val_value_loss: 0.196382   (epoch 2)   mode=min
  val_value_value_mse: 0.196390   (epoch 2)   mode=min
  value_loss: 0.196464   (epoch 2)   mode=min
  value_value_mse: 0.196465   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6280, 2.5938]
  policy_loss : [2.5288, 2.4956]
  policy_policy_acc : [0.2739, 0.2805]
  policy_top10_acc : [0.7785, 0.7868]
  policy_top5_acc : [0.6164, 0.6254]
  val_loss : [2.6035, 2.5796]
  val_policy_loss : [2.5038, 2.4814]
  val_policy_policy_acc : [0.2807, 0.2910]
  val_policy_top10_acc : [0.7832, 0.7857]
  val_policy_top5_acc : [0.6230, 0.6233]
  val_value_loss : [0.1995, 0.1964]
  val_value_value_mse : [0.1995, 0.1964]
  value_loss : [0.1984, 0.1965]
  value_value_mse : [0.1984, 0.1965]

================================================================================

History file: model_versions/chess_elo_model_V23_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:45:07.797088Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game180001_game181500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V22
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.600206   (epoch 2)   mode=min
  policy_loss: 2.500745   (epoch 2)   mode=min
  policy_policy_acc: 0.282266   (epoch 2)   mode=max
  policy_top10_acc: 0.786895   (epoch 2)   mode=max
  policy_top5_acc: 0.626155   (epoch 2)   mode=max
  val_loss: 2.598786   (epoch 1)   mode=min
  val_policy_loss: 2.499193   (epoch 1)   mode=min
  val_policy_policy_acc: 0.280020   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782218   (epoch 1)   mode=max
  val_policy_top5_acc: 0.619880   (epoch 1)   mode=max
  val_value_loss: 0.196796   (epoch 2)   mode=min
  val_value_value_mse: 0.196811   (epoch 2)   mode=min
  value_loss: 0.198998   (epoch 2)   mode=min
  value_value_mse: 0.198997   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6352, 2.6002]
  policy_loss : [2.5341, 2.5007]
  policy_policy_acc : [0.2752, 0.2823]
  policy_top10_acc : [0.7792, 0.7869]
  policy_top5_acc : [0.6156, 0.6262]
  val_loss : [2.5988, 2.8096]
  val_policy_loss : [2.4992, 2.7110]
  val_policy_policy_acc : [0.2800, 0.2780]
  val_policy_top10_acc : [0.7822, 0.7743]
  val_policy_top5_acc : [0.6199, 0.6160]
  val_value_loss : [0.1990, 0.1968]
  val_value_value_mse : [0.1991, 0.1968]
  value_loss : [0.2023, 0.1990]
  value_value_mse : [0.2023, 0.1990]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:47:22.099973Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game181501_game183000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.605249   (epoch 2)   mode=min
  policy_loss: 2.504043   (epoch 2)   mode=min
  policy_policy_acc: 0.280258   (epoch 2)   mode=max
  policy_top10_acc: 0.786000   (epoch 2)   mode=max
  policy_top5_acc: 0.625152   (epoch 2)   mode=max
  val_loss: 2.596117   (epoch 1)   mode=min
  val_policy_loss: 2.494603   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284815   (epoch 2)   mode=max
  val_policy_top10_acc: 0.782318   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624276   (epoch 2)   mode=max
  val_value_loss: 0.199782   (epoch 2)   mode=min
  val_value_value_mse: 0.199795   (epoch 2)   mode=min
  value_loss: 0.202381   (epoch 2)   mode=min
  value_value_mse: 0.202381   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6406, 2.6052]
  policy_loss : [2.5382, 2.5040]
  policy_policy_acc : [0.2745, 0.2803]
  policy_top10_acc : [0.7792, 0.7860]
  policy_top5_acc : [0.6168, 0.6252]
  val_loss : [2.5961, 2.6018]
  val_policy_loss : [2.4946, 2.5019]
  val_policy_policy_acc : [0.2835, 0.2848]
  val_policy_top10_acc : [0.7823, 0.7794]
  val_policy_top5_acc : [0.6222, 0.6243]
  val_value_loss : [0.2029, 0.1998]
  val_value_value_mse : [0.2029, 0.1998]
  value_loss : [0.2049, 0.2024]
  value_value_mse : [0.2049, 0.2024]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:49:44.893975Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game183001_game184500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601759   (epoch 2)   mode=min
  policy_loss: 2.500778   (epoch 2)   mode=min
  policy_policy_acc: 0.281774   (epoch 2)   mode=max
  policy_top10_acc: 0.782813   (epoch 2)   mode=max
  policy_top5_acc: 0.624057   (epoch 2)   mode=max
  val_loss: 2.582305   (epoch 2)   mode=min
  val_policy_loss: 2.484207   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288112   (epoch 2)   mode=max
  val_policy_top10_acc: 0.780719   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627772   (epoch 2)   mode=max
  val_value_loss: 0.196160   (epoch 2)   mode=min
  val_value_value_mse: 0.196175   (epoch 2)   mode=min
  value_loss: 0.200948   (epoch 2)   mode=min
  value_value_mse: 0.200909   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6342, 2.6018]
  policy_loss : [2.5335, 2.5008]
  policy_policy_acc : [0.2755, 0.2818]
  policy_top10_acc : [0.7762, 0.7828]
  policy_top5_acc : [0.6151, 0.6241]
  val_loss : [2.6553, 2.5823]
  val_policy_loss : [2.5546, 2.4842]
  val_policy_policy_acc : [0.2810, 0.2881]
  val_policy_top10_acc : [0.7737, 0.7807]
  val_policy_top5_acc : [0.6138, 0.6278]
  val_value_loss : [0.2013, 0.1962]
  val_value_value_mse : [0.2013, 0.1962]
  value_loss : [0.2025, 0.2009]
  value_value_mse : [0.2025, 0.2009]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:51:59.997676Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game184501_game186000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595134   (epoch 2)   mode=min
  policy_loss: 2.495428   (epoch 2)   mode=min
  policy_policy_acc: 0.283523   (epoch 2)   mode=max
  policy_top10_acc: 0.785699   (epoch 2)   mode=max
  policy_top5_acc: 0.625930   (epoch 2)   mode=max
  val_loss: 2.595625   (epoch 1)   mode=min
  val_policy_loss: 2.495932   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286613   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784715   (epoch 2)   mode=max
  val_policy_top5_acc: 0.629670   (epoch 2)   mode=max
  val_value_loss: 0.197246   (epoch 2)   mode=min
  val_value_value_mse: 0.197269   (epoch 2)   mode=min
  value_loss: 0.199249   (epoch 2)   mode=min
  value_value_mse: 0.199240   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6320, 2.5951]
  policy_loss : [2.5309, 2.4954]
  policy_policy_acc : [0.2757, 0.2835]
  policy_top10_acc : [0.7771, 0.7857]
  policy_top5_acc : [0.6157, 0.6259]
  val_loss : [2.5956, 2.6296]
  val_policy_loss : [2.4959, 2.5309]
  val_policy_policy_acc : [0.2837, 0.2866]
  val_policy_top10_acc : [0.7806, 0.7847]
  val_policy_top5_acc : [0.6250, 0.6297]
  val_value_loss : [0.1993, 0.1972]
  val_value_value_mse : [0.1993, 0.1973]
  value_loss : [0.2020, 0.1992]
  value_value_mse : [0.2020, 0.1992]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:54:17.269577Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game186001_game187500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.593693   (epoch 2)   mode=min
  policy_loss: 2.492652   (epoch 2)   mode=min
  policy_policy_acc: 0.282075   (epoch 2)   mode=max
  policy_top10_acc: 0.786238   (epoch 2)   mode=max
  policy_top5_acc: 0.627917   (epoch 2)   mode=max
  val_loss: 2.592257   (epoch 2)   mode=min
  val_policy_loss: 2.493281   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283217   (epoch 2)   mode=max
  val_policy_top10_acc: 0.781419   (epoch 2)   mode=max
  val_policy_top5_acc: 0.622677   (epoch 2)   mode=max
  val_value_loss: 0.197886   (epoch 2)   mode=min
  val_value_value_mse: 0.197897   (epoch 2)   mode=min
  value_loss: 0.202126   (epoch 2)   mode=min
  value_value_mse: 0.202118   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6301, 2.5937]
  policy_loss : [2.5282, 2.4927]
  policy_policy_acc : [0.2755, 0.2821]
  policy_top10_acc : [0.7788, 0.7862]
  policy_top5_acc : [0.6170, 0.6279]
  val_loss : [2.6070, 2.5923]
  val_policy_loss : [2.5075, 2.4933]
  val_policy_policy_acc : [0.2758, 0.2832]
  val_policy_top10_acc : [0.7790, 0.7814]
  val_policy_top5_acc : [0.6147, 0.6227]
  val_value_loss : [0.1990, 0.1979]
  val_value_value_mse : [0.1990, 0.1979]
  value_loss : [0.2039, 0.2021]
  value_value_mse : [0.2039, 0.2021]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:56:23.248444Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game187501_game189000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594047   (epoch 2)   mode=min
  policy_loss: 2.497252   (epoch 2)   mode=min
  policy_policy_acc: 0.281764   (epoch 2)   mode=max
  policy_top10_acc: 0.785586   (epoch 2)   mode=max
  policy_top5_acc: 0.625832   (epoch 2)   mode=max
  val_loss: 2.598295   (epoch 2)   mode=min
  val_policy_loss: 2.499279   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287413   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783117   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623776   (epoch 2)   mode=max
  val_value_loss: 0.197879   (epoch 2)   mode=min
  val_value_value_mse: 0.197881   (epoch 2)   mode=min
  value_loss: 0.193619   (epoch 2)   mode=min
  value_value_mse: 0.193616   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6334, 2.5940]
  policy_loss : [2.5350, 2.4973]
  policy_policy_acc : [0.2740, 0.2818]
  policy_top10_acc : [0.7782, 0.7856]
  policy_top5_acc : [0.6157, 0.6258]
  val_loss : [2.6230, 2.5983]
  val_policy_loss : [2.5232, 2.4993]
  val_policy_policy_acc : [0.2874, 0.2864]
  val_policy_top10_acc : [0.7831, 0.7817]
  val_policy_top5_acc : [0.6237, 0.6238]
  val_value_loss : [0.1994, 0.1979]
  val_value_value_mse : [0.1994, 0.1979]
  value_loss : [0.1968, 0.1936]
  value_value_mse : [0.1968, 0.1936]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T13:58:38.643613Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game189001_game190500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598160   (epoch 2)   mode=min
  policy_loss: 2.498755   (epoch 2)   mode=min
  policy_policy_acc: 0.280609   (epoch 2)   mode=max
  policy_top10_acc: 0.784980   (epoch 2)   mode=max
  policy_top5_acc: 0.622423   (epoch 2)   mode=max
  val_loss: 2.594308   (epoch 2)   mode=min
  val_policy_loss: 2.496270   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284416   (epoch 2)   mode=max
  val_policy_top10_acc: 0.782318   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623576   (epoch 2)   mode=max
  val_value_loss: 0.195887   (epoch 2)   mode=min
  val_value_value_mse: 0.195905   (epoch 2)   mode=min
  value_loss: 0.198694   (epoch 2)   mode=min
  value_value_mse: 0.198667   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6316, 2.5982]
  policy_loss : [2.5315, 2.4988]
  policy_policy_acc : [0.2744, 0.2806]
  policy_top10_acc : [0.7775, 0.7850]
  policy_top5_acc : [0.6147, 0.6224]
  val_loss : [2.5961, 2.5943]
  val_policy_loss : [2.4972, 2.4963]
  val_policy_policy_acc : [0.2829, 0.2844]
  val_policy_top10_acc : [0.7823, 0.7785]
  val_policy_top5_acc : [0.6207, 0.6236]
  val_value_loss : [0.1977, 0.1959]
  val_value_value_mse : [0.1977, 0.1959]
  value_loss : [0.2003, 0.1987]
  value_value_mse : [0.2003, 0.1987]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:00:55.223057Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game190501_game192000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601394   (epoch 2)   mode=min
  policy_loss: 2.501215   (epoch 2)   mode=min
  policy_policy_acc: 0.279226   (epoch 2)   mode=max
  policy_top10_acc: 0.784300   (epoch 2)   mode=max
  policy_top5_acc: 0.622715   (epoch 2)   mode=max
  val_loss: 3.976304   (epoch 1)   mode=min
  val_policy_loss: 3.877757   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289910   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786913   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623277   (epoch 2)   mode=max
  val_value_loss: 0.195217   (epoch 1)   mode=min
  val_value_value_mse: 0.195242   (epoch 1)   mode=min
  value_loss: 0.200300   (epoch 2)   mode=min
  value_value_mse: 0.200298   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6383, 2.6014]
  policy_loss : [2.5360, 2.5012]
  policy_policy_acc : [0.2736, 0.2792]
  policy_top10_acc : [0.7749, 0.7843]
  policy_top5_acc : [0.6141, 0.6227]
  val_loss : [3.9763, 5.0601]
  val_policy_loss : [3.8778, 4.9597]
  val_policy_policy_acc : [0.2899, 0.2886]
  val_policy_top10_acc : [0.7834, 0.7869]
  val_policy_top5_acc : [0.6225, 0.6233]
  val_value_loss : [0.1952, 0.1975]
  val_value_value_mse : [0.1952, 0.1975]
  value_loss : [0.2045, 0.2003]
  value_value_mse : [0.2045, 0.2003]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:03:10.350753Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game192001_game193500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.602146   (epoch 2)   mode=min
  policy_loss: 2.502383   (epoch 2)   mode=min
  policy_policy_acc: 0.281812   (epoch 2)   mode=max
  policy_top10_acc: 0.784957   (epoch 2)   mode=max
  policy_top5_acc: 0.622461   (epoch 2)   mode=max
  val_loss: 4.812915   (epoch 1)   mode=min
  val_policy_loss: 4.711205   (epoch 1)   mode=min
  val_policy_policy_acc: 0.291209   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787912   (epoch 1)   mode=max
  val_policy_top5_acc: 0.629670   (epoch 1)   mode=max
  val_value_loss: 0.198114   (epoch 2)   mode=min
  val_value_value_mse: 0.198130   (epoch 2)   mode=min
  value_loss: 0.199531   (epoch 2)   mode=min
  value_value_mse: 0.199535   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6399, 2.6021]
  policy_loss : [2.5390, 2.5024]
  policy_policy_acc : [0.2756, 0.2818]
  policy_top10_acc : [0.7743, 0.7850]
  policy_top5_acc : [0.6141, 0.6225]
  val_loss : [4.8129, 5.5215]
  val_policy_loss : [4.7112, 5.4207]
  val_policy_policy_acc : [0.2912, 0.2845]
  val_policy_top10_acc : [0.7879, 0.7778]
  val_policy_top5_acc : [0.6297, 0.6173]
  val_value_loss : [0.2007, 0.1981]
  val_value_value_mse : [0.2007, 0.1981]
  value_loss : [0.2019, 0.1995]
  value_value_mse : [0.2019, 0.1995]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:05:24.168628Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game193501_game195000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files.npz
  validation_indices: validation_selected_indices.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.605347   (epoch 2)   mode=min
  policy_loss: 2.506578   (epoch 2)   mode=min
  policy_policy_acc: 0.281734   (epoch 2)   mode=max
  policy_top10_acc: 0.784558   (epoch 2)   mode=max
  policy_top5_acc: 0.622850   (epoch 2)   mode=max
  val_loss: 2.613288   (epoch 2)   mode=min
  val_policy_loss: 2.515074   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288312   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785215   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626274   (epoch 2)   mode=max
  val_value_loss: 0.196186   (epoch 2)   mode=min
  val_value_value_mse: 0.196192   (epoch 2)   mode=min
  value_loss: 0.197534   (epoch 2)   mode=min
  value_value_mse: 0.197530   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6388, 2.6053]
  policy_loss : [2.5387, 2.5066]
  policy_policy_acc : [0.2750, 0.2817]
  policy_top10_acc : [0.7767, 0.7846]
  policy_top5_acc : [0.6136, 0.6228]
  val_loss : [2.6870, 2.6133]
  val_policy_loss : [2.5871, 2.5151]
  val_policy_policy_acc : [0.2851, 0.2883]
  val_policy_top10_acc : [0.7852, 0.7838]
  val_policy_top5_acc : [0.6233, 0.6263]
  val_value_loss : [0.1996, 0.1962]
  val_value_value_mse : [0.1996, 0.1962]
  value_loss : [0.2001, 0.1975]
  value_value_mse : [0.2001, 0.1975]

================================================================================

History file: model_versions/chess_elo_model_V24_history.npy
Total runs: 20
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:07:55.759690Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.580988   (epoch 2)   mode=min
  policy_loss: 2.484183   (epoch 2)   mode=min
  policy_policy_acc: 0.286868   (epoch 2)   mode=max
  policy_top10_acc: 0.789528   (epoch 2)   mode=max
  policy_top5_acc: 0.629987   (epoch 2)   mode=max
  val_loss: 3.142375   (epoch 1)   mode=min
  val_policy_loss: 3.043499   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286513   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786014   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628671   (epoch 1)   mode=max
  val_value_loss: 0.193857   (epoch 2)   mode=min
  val_value_value_mse: 0.193824   (epoch 2)   mode=min
  value_loss: 0.194087   (epoch 2)   mode=min
  value_value_mse: 0.194088   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6185, 2.5810]
  policy_loss : [2.5203, 2.4842]
  policy_policy_acc : [0.2794, 0.2869]
  policy_top10_acc : [0.7805, 0.7895]
  policy_top5_acc : [0.6194, 0.6300]
  val_loss : [3.1424, 3.3308]
  val_policy_loss : [3.0435, 3.2332]
  val_policy_policy_acc : [0.2856, 0.2865]
  val_policy_top10_acc : [0.7860, 0.7848]
  val_policy_top5_acc : [0.6287, 0.6262]
  val_value_loss : [0.1967, 0.1939]
  val_value_value_mse : [0.1967, 0.1938]
  value_loss : [0.1966, 0.1941]
  value_value_mse : [0.1966, 0.1941]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:10:23.923780Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game151501_game153000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.588295   (epoch 2)   mode=min
  policy_loss: 2.487791   (epoch 2)   mode=min
  policy_policy_acc: 0.283769   (epoch 2)   mode=max
  policy_top10_acc: 0.790382   (epoch 2)   mode=max
  policy_top5_acc: 0.629175   (epoch 2)   mode=max
  val_loss: 2.775443   (epoch 1)   mode=min
  val_policy_loss: 2.677414   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290909   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788512   (epoch 2)   mode=max
  val_policy_top5_acc: 0.636563   (epoch 2)   mode=max
  val_value_loss: 0.195317   (epoch 1)   mode=min
  val_value_value_mse: 0.195303   (epoch 1)   mode=min
  value_loss: 0.201023   (epoch 2)   mode=min
  value_value_mse: 0.201019   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6246, 2.5883]
  policy_loss : [2.5232, 2.4878]
  policy_policy_acc : [0.2748, 0.2838]
  policy_top10_acc : [0.7811, 0.7904]
  policy_top5_acc : [0.6201, 0.6292]
  val_loss : [2.7754, 2.8931]
  val_policy_loss : [2.6774, 2.7941]
  val_policy_policy_acc : [0.2876, 0.2909]
  val_policy_top10_acc : [0.7833, 0.7885]
  val_policy_top5_acc : [0.6307, 0.6366]
  val_value_loss : [0.1953, 0.1971]
  val_value_value_mse : [0.1953, 0.1971]
  value_loss : [0.2027, 0.2010]
  value_value_mse : [0.2027, 0.2010]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:12:41.322133Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game153001_game154500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603697   (epoch 2)   mode=min
  policy_loss: 2.504289   (epoch 2)   mode=min
  policy_policy_acc: 0.278571   (epoch 2)   mode=max
  policy_top10_acc: 0.785758   (epoch 2)   mode=max
  policy_top5_acc: 0.623803   (epoch 2)   mode=max
  val_loss: 3.023839   (epoch 1)   mode=min
  val_policy_loss: 2.922930   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288511   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786913   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632068   (epoch 1)   mode=max
  val_value_loss: 0.200666   (epoch 2)   mode=min
  val_value_value_mse: 0.200649   (epoch 2)   mode=min
  value_loss: 0.198965   (epoch 2)   mode=min
  value_value_mse: 0.198964   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6407, 2.6037]
  policy_loss : [2.5404, 2.5043]
  policy_policy_acc : [0.2711, 0.2786]
  policy_top10_acc : [0.7774, 0.7858]
  policy_top5_acc : [0.6148, 0.6238]
  val_loss : [3.0238, 7.0646]
  val_policy_loss : [2.9229, 6.9613]
  val_policy_policy_acc : [0.2856, 0.2885]
  val_policy_top10_acc : [0.7862, 0.7869]
  val_policy_top5_acc : [0.6321, 0.6308]
  val_value_loss : [0.2008, 0.2007]
  val_value_value_mse : [0.2008, 0.2006]
  value_loss : [0.2005, 0.1990]
  value_value_mse : [0.2005, 0.1990]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:15:23.588827Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game154501_game156000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.590415   (epoch 2)   mode=min
  policy_loss: 2.492116   (epoch 2)   mode=min
  policy_policy_acc: 0.280385   (epoch 2)   mode=max
  policy_top10_acc: 0.788904   (epoch 2)   mode=max
  policy_top5_acc: 0.627221   (epoch 2)   mode=max
  val_loss: 3.020257   (epoch 2)   mode=min
  val_policy_loss: 2.922570   (epoch 2)   mode=min
  val_policy_policy_acc: 0.283916   (epoch 2)   mode=max
  val_policy_top10_acc: 0.786014   (epoch 1)   mode=max
  val_policy_top5_acc: 0.632967   (epoch 1)   mode=max
  val_value_loss: 0.193392   (epoch 1)   mode=min
  val_value_value_mse: 0.193360   (epoch 1)   mode=min
  value_loss: 0.196597   (epoch 2)   mode=min
  value_value_mse: 0.196597   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6253, 2.5904]
  policy_loss : [2.5263, 2.4921]
  policy_policy_acc : [0.2747, 0.2804]
  policy_top10_acc : [0.7797, 0.7889]
  policy_top5_acc : [0.6166, 0.6272]
  val_loss : [4.0615, 3.0203]
  val_policy_loss : [3.9637, 2.9226]
  val_policy_policy_acc : [0.2837, 0.2839]
  val_policy_top10_acc : [0.7860, 0.7791]
  val_policy_top5_acc : [0.6330, 0.6257]
  val_value_loss : [0.1934, 0.1942]
  val_value_value_mse : [0.1934, 0.1942]
  value_loss : [0.1981, 0.1966]
  value_value_mse : [0.1981, 0.1966]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:18:21.055052Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game156001_game157500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.612583   (epoch 2)   mode=min
  policy_loss: 2.511900   (epoch 2)   mode=min
  policy_policy_acc: 0.275980   (epoch 2)   mode=max
  policy_top10_acc: 0.784355   (epoch 2)   mode=max
  policy_top5_acc: 0.621958   (epoch 2)   mode=max
  val_loss: 2.649925   (epoch 1)   mode=min
  val_policy_loss: 2.551059   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285215   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631069   (epoch 1)   mode=max
  val_value_loss: 0.193543   (epoch 2)   mode=min
  val_value_value_mse: 0.193512   (epoch 2)   mode=min
  value_loss: 0.201318   (epoch 2)   mode=min
  value_value_mse: 0.201329   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6445, 2.6126]
  policy_loss : [2.5427, 2.5119]
  policy_policy_acc : [0.2690, 0.2760]
  policy_top10_acc : [0.7749, 0.7844]
  policy_top5_acc : [0.6110, 0.6220]
  val_loss : [2.6499, 7.2978]
  val_policy_loss : [2.5511, 7.1979]
  val_policy_policy_acc : [0.2852, 0.2845]
  val_policy_top10_acc : [0.7851, 0.7844]
  val_policy_top5_acc : [0.6311, 0.6305]
  val_value_loss : [0.1972, 0.1935]
  val_value_value_mse : [0.1972, 0.1935]
  value_loss : [0.2035, 0.2013]
  value_value_mse : [0.2035, 0.2013]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:20:56.555551Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game157501_game159000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.606965   (epoch 2)   mode=min
  policy_loss: 2.505122   (epoch 2)   mode=min
  policy_policy_acc: 0.281186   (epoch 2)   mode=max
  policy_top10_acc: 0.785808   (epoch 2)   mode=max
  policy_top5_acc: 0.625399   (epoch 2)   mode=max
  val_loss: 2.590831   (epoch 1)   mode=min
  val_policy_loss: 2.493261   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287612   (epoch 2)   mode=max
  val_policy_top10_acc: 0.793307   (epoch 1)   mode=max
  val_policy_top5_acc: 0.636464   (epoch 1)   mode=max
  val_value_loss: 0.194393   (epoch 2)   mode=min
  val_value_value_mse: 0.194380   (epoch 2)   mode=min
  value_loss: 0.203856   (epoch 2)   mode=min
  value_value_mse: 0.203859   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6450, 2.6070]
  policy_loss : [2.5421, 2.5051]
  policy_policy_acc : [0.2742, 0.2812]
  policy_top10_acc : [0.7780, 0.7858]
  policy_top5_acc : [0.6128, 0.6254]
  val_loss : [2.5908, 2.7397]
  val_policy_loss : [2.4933, 2.6422]
  val_policy_policy_acc : [0.2856, 0.2876]
  val_policy_top10_acc : [0.7933, 0.7855]
  val_policy_top5_acc : [0.6365, 0.6338]
  val_value_loss : [0.1948, 0.1944]
  val_value_value_mse : [0.1947, 0.1944]
  value_loss : [0.2060, 0.2039]
  value_value_mse : [0.2060, 0.2039]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:23:42.308471Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game159001_game160500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603305   (epoch 2)   mode=min
  policy_loss: 2.501873   (epoch 2)   mode=min
  policy_policy_acc: 0.281540   (epoch 2)   mode=max
  policy_top10_acc: 0.785948   (epoch 2)   mode=max
  policy_top5_acc: 0.626174   (epoch 2)   mode=max
  val_loss: 2.875113   (epoch 2)   mode=min
  val_policy_loss: 2.771153   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785015   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628272   (epoch 2)   mode=max
  val_value_loss: 0.201056   (epoch 1)   mode=min
  val_value_value_mse: 0.201032   (epoch 1)   mode=min
  value_loss: 0.202958   (epoch 2)   mode=min
  value_value_mse: 0.202958   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6387, 2.6033]
  policy_loss : [2.5359, 2.5019]
  policy_policy_acc : [0.2764, 0.2815]
  policy_top10_acc : [0.7766, 0.7859]
  policy_top5_acc : [0.6151, 0.6262]
  val_loss : [3.5353, 2.8751]
  val_policy_loss : [3.4340, 2.7712]
  val_policy_policy_acc : [0.2851, 0.2832]
  val_policy_top10_acc : [0.7850, 0.7840]
  val_policy_top5_acc : [0.6276, 0.6283]
  val_value_loss : [0.2011, 0.2071]
  val_value_value_mse : [0.2010, 0.2071]
  value_loss : [0.2054, 0.2030]
  value_value_mse : [0.2054, 0.2030]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:26:20.390382Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game160501_game162000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.590575   (epoch 2)   mode=min
  policy_loss: 2.493173   (epoch 2)   mode=min
  policy_policy_acc: 0.282636   (epoch 2)   mode=max
  policy_top10_acc: 0.787292   (epoch 2)   mode=max
  policy_top5_acc: 0.627068   (epoch 2)   mode=max
  val_loss: 2.625786   (epoch 2)   mode=min
  val_policy_loss: 2.526960   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286713   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785914   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630370   (epoch 1)   mode=max
  val_value_loss: 0.193197   (epoch 1)   mode=min
  val_value_value_mse: 0.193191   (epoch 1)   mode=min
  value_loss: 0.194747   (epoch 2)   mode=min
  value_value_mse: 0.194745   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6266, 2.5906]
  policy_loss : [2.5275, 2.4932]
  policy_policy_acc : [0.2742, 0.2826]
  policy_top10_acc : [0.7800, 0.7873]
  policy_top5_acc : [0.6174, 0.6271]
  val_loss : [3.1533, 2.6258]
  val_policy_loss : [3.0563, 2.5270]
  val_policy_policy_acc : [0.2802, 0.2867]
  val_policy_top10_acc : [0.7859, 0.7854]
  val_policy_top5_acc : [0.6304, 0.6272]
  val_value_loss : [0.1932, 0.1973]
  val_value_value_mse : [0.1932, 0.1973]
  value_loss : [0.1981, 0.1947]
  value_value_mse : [0.1981, 0.1947]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:28:39.199676Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game162001_game163500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.583312   (epoch 2)   mode=min
  policy_loss: 2.481637   (epoch 2)   mode=min
  policy_policy_acc: 0.288429   (epoch 2)   mode=max
  policy_top10_acc: 0.788434   (epoch 2)   mode=max
  policy_top5_acc: 0.629939   (epoch 2)   mode=max
  val_loss: 3.777264   (epoch 1)   mode=min
  val_policy_loss: 3.675708   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284416   (epoch 2)   mode=max
  val_policy_top10_acc: 0.791009   (epoch 1)   mode=max
  val_policy_top5_acc: 0.635964   (epoch 1)   mode=max
  val_value_loss: 0.199275   (epoch 2)   mode=min
  val_value_value_mse: 0.199270   (epoch 2)   mode=min
  value_loss: 0.203271   (epoch 2)   mode=min
  value_value_mse: 0.203271   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6240, 2.5833]
  policy_loss : [2.5216, 2.4816]
  policy_policy_acc : [0.2804, 0.2884]
  policy_top10_acc : [0.7795, 0.7884]
  policy_top5_acc : [0.6196, 0.6299]
  val_loss : [3.7773, 5.2116]
  val_policy_loss : [3.6757, 5.1102]
  val_policy_policy_acc : [0.2832, 0.2844]
  val_policy_top10_acc : [0.7910, 0.7884]
  val_policy_top5_acc : [0.6360, 0.6333]
  val_value_loss : [0.2012, 0.1993]
  val_value_value_mse : [0.2012, 0.1993]
  value_loss : [0.2047, 0.2033]
  value_value_mse : [0.2047, 0.2033]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:30:49.401915Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game163501_game165000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.580779   (epoch 2)   mode=min
  policy_loss: 2.479275   (epoch 2)   mode=min
  policy_policy_acc: 0.287646   (epoch 2)   mode=max
  policy_top10_acc: 0.791146   (epoch 2)   mode=max
  policy_top5_acc: 0.631160   (epoch 2)   mode=max
  val_loss: 2.612852   (epoch 1)   mode=min
  val_policy_loss: 2.513938   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290709   (epoch 1)   mode=max
  val_policy_top10_acc: 0.791409   (epoch 1)   mode=max
  val_policy_top5_acc: 0.632168   (epoch 1)   mode=max
  val_value_loss: 0.197407   (epoch 1)   mode=min
  val_value_value_mse: 0.197389   (epoch 1)   mode=min
  value_loss: 0.203824   (epoch 2)   mode=min
  value_value_mse: 0.203861   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6149, 2.5808]
  policy_loss : [2.5121, 2.4793]
  policy_policy_acc : [0.2803, 0.2876]
  policy_top10_acc : [0.7815, 0.7911]
  policy_top5_acc : [0.6211, 0.6312]
  val_loss : [2.6129, 3.3979]
  val_policy_loss : [2.5139, 3.2969]
  val_policy_policy_acc : [0.2907, 0.2889]
  val_policy_top10_acc : [0.7914, 0.7899]
  val_policy_top5_acc : [0.6322, 0.6321]
  val_value_loss : [0.1974, 0.2009]
  val_value_value_mse : [0.1974, 0.2008]
  value_loss : [0.2061, 0.2038]
  value_value_mse : [0.2061, 0.2039]

================================================================================

RUN #11
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:39:35.158602Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V23
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.582654   (epoch 2)   mode=min
  policy_loss: 2.485597   (epoch 2)   mode=min
  policy_policy_acc: 0.284223   (epoch 2)   mode=max
  policy_top10_acc: 0.788422   (epoch 2)   mode=max
  policy_top5_acc: 0.627664   (epoch 2)   mode=max
  val_loss: 2.843103   (epoch 1)   mode=min
  val_policy_loss: 2.744729   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286014   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787712   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630869   (epoch 1)   mode=max
  val_value_loss: 0.195991   (epoch 1)   mode=min
  val_value_value_mse: 0.195968   (epoch 1)   mode=min
  value_loss: 0.194285   (epoch 2)   mode=min
  value_value_mse: 0.194253   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6181, 2.5827]
  policy_loss : [2.5198, 2.4856]
  policy_policy_acc : [0.2778, 0.2842]
  policy_top10_acc : [0.7822, 0.7884]
  policy_top5_acc : [0.6211, 0.6277]
  val_loss : [2.8431, 2.8765]
  val_policy_loss : [2.7447, 2.7775]
  val_policy_policy_acc : [0.2860, 0.2841]
  val_policy_top10_acc : [0.7877, 0.7834]
  val_policy_top5_acc : [0.6309, 0.6251]
  val_value_loss : [0.1960, 0.1972]
  val_value_value_mse : [0.1960, 0.1971]
  value_loss : [0.1968, 0.1943]
  value_value_mse : [0.1968, 0.1943]

================================================================================

RUN #12
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:42:23.148825Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game151501_game153000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591513   (epoch 2)   mode=min
  policy_loss: 2.491252   (epoch 2)   mode=min
  policy_policy_acc: 0.281461   (epoch 2)   mode=max
  policy_top10_acc: 0.788269   (epoch 2)   mode=max
  policy_top5_acc: 0.628465   (epoch 2)   mode=max
  val_loss: 2.777328   (epoch 2)   mode=min
  val_policy_loss: 2.677963   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285814   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786514   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631568   (epoch 1)   mode=max
  val_value_loss: 0.198082   (epoch 2)   mode=min
  val_value_value_mse: 0.198063   (epoch 2)   mode=min
  value_loss: 0.200485   (epoch 2)   mode=min
  value_value_mse: 0.200487   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6262, 2.5915]
  policy_loss : [2.5250, 2.4913]
  policy_policy_acc : [0.2744, 0.2815]
  policy_top10_acc : [0.7807, 0.7883]
  policy_top5_acc : [0.6188, 0.6285]
  val_loss : [2.7889, 2.7773]
  val_policy_loss : [2.6885, 2.6780]
  val_policy_policy_acc : [0.2858, 0.2855]
  val_policy_top10_acc : [0.7865, 0.7834]
  val_policy_top5_acc : [0.6316, 0.6312]
  val_value_loss : [0.2002, 0.1981]
  val_value_value_mse : [0.2002, 0.1981]
  value_loss : [0.2024, 0.2005]
  value_value_mse : [0.2024, 0.2005]

================================================================================

RUN #13
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:44:41.654725Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game153001_game154500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.606631   (epoch 2)   mode=min
  policy_loss: 2.506856   (epoch 2)   mode=min
  policy_policy_acc: 0.276829   (epoch 2)   mode=max
  policy_top10_acc: 0.785187   (epoch 2)   mode=max
  policy_top5_acc: 0.622003   (epoch 2)   mode=max
  val_loss: 4.003081   (epoch 2)   mode=min
  val_policy_loss: 3.903275   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287912   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788212   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631968   (epoch 1)   mode=max
  val_value_loss: 0.197438   (epoch 2)   mode=min
  val_value_value_mse: 0.197423   (epoch 2)   mode=min
  value_loss: 0.199552   (epoch 2)   mode=min
  value_value_mse: 0.199546   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6456, 2.6066]
  policy_loss : [2.5447, 2.5069]
  policy_policy_acc : [0.2690, 0.2768]
  policy_top10_acc : [0.7763, 0.7852]
  policy_top5_acc : [0.6134, 0.6220]
  val_loss : [6.5811, 4.0031]
  val_policy_loss : [6.4788, 3.9033]
  val_policy_policy_acc : [0.2879, 0.2877]
  val_policy_top10_acc : [0.7882, 0.7879]
  val_policy_top5_acc : [0.6320, 0.6318]
  val_value_loss : [0.1994, 0.1974]
  val_value_value_mse : [0.1994, 0.1974]
  value_loss : [0.2018, 0.1996]
  value_value_mse : [0.2018, 0.1995]

================================================================================

RUN #14
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:46:58.073005Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game154501_game156000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594868   (epoch 2)   mode=min
  policy_loss: 2.496618   (epoch 2)   mode=min
  policy_policy_acc: 0.281942   (epoch 2)   mode=max
  policy_top10_acc: 0.787865   (epoch 2)   mode=max
  policy_top5_acc: 0.626452   (epoch 2)   mode=max
  val_loss: 2.583897   (epoch 1)   mode=min
  val_policy_loss: 2.484874   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290310   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789810   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630869   (epoch 1)   mode=max
  val_value_loss: 0.195128   (epoch 2)   mode=min
  val_value_value_mse: 0.195117   (epoch 2)   mode=min
  value_loss: 0.196498   (epoch 2)   mode=min
  value_value_mse: 0.196498   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6294, 2.5949]
  policy_loss : [2.5300, 2.4966]
  policy_policy_acc : [0.2730, 0.2819]
  policy_top10_acc : [0.7783, 0.7879]
  policy_top5_acc : [0.6156, 0.6265]
  val_loss : [2.5839, 2.8456]
  val_policy_loss : [2.4849, 2.7477]
  val_policy_policy_acc : [0.2843, 0.2903]
  val_policy_top10_acc : [0.7869, 0.7898]
  val_policy_top5_acc : [0.6309, 0.6285]
  val_value_loss : [0.1977, 0.1951]
  val_value_value_mse : [0.1977, 0.1951]
  value_loss : [0.1987, 0.1965]
  value_value_mse : [0.1987, 0.1965]

================================================================================

RUN #15
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:49:08.599433Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game156001_game157500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.611683   (epoch 2)   mode=min
  policy_loss: 2.510956   (epoch 2)   mode=min
  policy_policy_acc: 0.275830   (epoch 2)   mode=max
  policy_top10_acc: 0.784285   (epoch 2)   mode=max
  policy_top5_acc: 0.621768   (epoch 2)   mode=max
  val_loss: 2.601504   (epoch 1)   mode=min
  val_policy_loss: 2.503413   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288511   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630769   (epoch 1)   mode=max
  val_value_loss: 0.193881   (epoch 2)   mode=min
  val_value_value_mse: 0.193858   (epoch 2)   mode=min
  value_loss: 0.201433   (epoch 2)   mode=min
  value_value_mse: 0.201433   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6428, 2.6117]
  policy_loss : [2.5414, 2.5110]
  policy_policy_acc : [0.2694, 0.2758]
  policy_top10_acc : [0.7772, 0.7843]
  policy_top5_acc : [0.6139, 0.6218]
  val_loss : [2.6015, 2.6788]
  val_policy_loss : [2.5034, 2.5816]
  val_policy_policy_acc : [0.2874, 0.2885]
  val_policy_top10_acc : [0.7857, 0.7850]
  val_policy_top5_acc : [0.6308, 0.6305]
  val_value_loss : [0.1959, 0.1939]
  val_value_value_mse : [0.1959, 0.1939]
  value_loss : [0.2031, 0.2014]
  value_value_mse : [0.2031, 0.2014]

================================================================================

RUN #16
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:51:15.978231Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game157501_game159000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.608293   (epoch 2)   mode=min
  policy_loss: 2.505703   (epoch 2)   mode=min
  policy_policy_acc: 0.281599   (epoch 2)   mode=max
  policy_top10_acc: 0.785292   (epoch 2)   mode=max
  policy_top5_acc: 0.623705   (epoch 2)   mode=max
  val_loss: 3.363679   (epoch 1)   mode=min
  val_policy_loss: 3.265301   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284016   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784516   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631768   (epoch 1)   mode=max
  val_value_loss: 0.195440   (epoch 1)   mode=min
  val_value_value_mse: 0.195428   (epoch 1)   mode=min
  value_loss: 0.205020   (epoch 2)   mode=min
  value_value_mse: 0.205021   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6449, 2.6083]
  policy_loss : [2.5417, 2.5057]
  policy_policy_acc : [0.2746, 0.2816]
  policy_top10_acc : [0.7764, 0.7853]
  policy_top5_acc : [0.6129, 0.6237]
  val_loss : [3.3637, 13.0067]
  val_policy_loss : [3.2653, 12.9015]
  val_policy_policy_acc : [0.2840, 0.2812]
  val_policy_top10_acc : [0.7827, 0.7845]
  val_policy_top5_acc : [0.6318, 0.6280]
  val_value_loss : [0.1954, 0.1978]
  val_value_value_mse : [0.1954, 0.1977]
  value_loss : [0.2066, 0.2050]
  value_value_mse : [0.2065, 0.2050]

================================================================================

RUN #17
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:53:26.813593Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game159001_game160500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601521   (epoch 2)   mode=min
  policy_loss: 2.500096   (epoch 2)   mode=min
  policy_policy_acc: 0.281799   (epoch 2)   mode=max
  policy_top10_acc: 0.785928   (epoch 2)   mode=max
  policy_top5_acc: 0.624869   (epoch 2)   mode=max
  val_loss: 2.767898   (epoch 2)   mode=min
  val_policy_loss: 2.666106   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284515   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787413   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630270   (epoch 2)   mode=max
  val_value_loss: 0.200756   (epoch 1)   mode=min
  val_value_value_mse: 0.200739   (epoch 1)   mode=min
  value_loss: 0.202713   (epoch 2)   mode=min
  value_value_mse: 0.202710   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6375, 2.6015]
  policy_loss : [2.5354, 2.5001]
  policy_policy_acc : [0.2755, 0.2818]
  policy_top10_acc : [0.7769, 0.7859]
  policy_top5_acc : [0.6165, 0.6249]
  val_loss : [4.9536, 2.7679]
  val_policy_loss : [4.8517, 2.6661]
  val_policy_policy_acc : [0.2845, 0.2828]
  val_policy_top10_acc : [0.7850, 0.7874]
  val_policy_top5_acc : [0.6297, 0.6303]
  val_value_loss : [0.2008, 0.2029]
  val_value_value_mse : [0.2007, 0.2029]
  value_loss : [0.2044, 0.2027]
  value_value_mse : [0.2044, 0.2027]

================================================================================

RUN #18
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:55:37.364996Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game160501_game162000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591901   (epoch 2)   mode=min
  policy_loss: 2.493898   (epoch 2)   mode=min
  policy_policy_acc: 0.280338   (epoch 2)   mode=max
  policy_top10_acc: 0.787471   (epoch 2)   mode=max
  policy_top5_acc: 0.627524   (epoch 2)   mode=max
  val_loss: 2.608333   (epoch 1)   mode=min
  val_policy_loss: 2.508732   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788512   (epoch 2)   mode=max
  val_policy_top5_acc: 0.629770   (epoch 2)   mode=max
  val_value_loss: 0.195504   (epoch 2)   mode=min
  val_value_value_mse: 0.195496   (epoch 2)   mode=min
  value_loss: 0.196071   (epoch 2)   mode=min
  value_value_mse: 0.196076   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6298, 2.5919]
  policy_loss : [2.5306, 2.4939]
  policy_policy_acc : [0.2754, 0.2803]
  policy_top10_acc : [0.7775, 0.7875]
  policy_top5_acc : [0.6147, 0.6275]
  val_loss : [2.6083, 2.9007]
  val_policy_loss : [2.5087, 2.8026]
  val_policy_policy_acc : [0.2826, 0.2857]
  val_policy_top10_acc : [0.7843, 0.7885]
  val_policy_top5_acc : [0.6282, 0.6298]
  val_value_loss : [0.1989, 0.1955]
  val_value_value_mse : [0.1989, 0.1955]
  value_loss : [0.1986, 0.1961]
  value_value_mse : [0.1986, 0.1961]

================================================================================

RUN #19
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:57:47.236426Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game162001_game163500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584757   (epoch 2)   mode=min
  policy_loss: 2.483480   (epoch 2)   mode=min
  policy_policy_acc: 0.287132   (epoch 2)   mode=max
  policy_top10_acc: 0.787566   (epoch 2)   mode=max
  policy_top5_acc: 0.630987   (epoch 2)   mode=max
  val_loss: 2.910557   (epoch 1)   mode=min
  val_policy_loss: 2.811395   (epoch 1)   mode=min
  val_policy_policy_acc: 0.291309   (epoch 1)   mode=max
  val_policy_top10_acc: 0.790909   (epoch 1)   mode=max
  val_policy_top5_acc: 0.635365   (epoch 2)   mode=max
  val_value_loss: 0.197539   (epoch 1)   mode=min
  val_value_value_mse: 0.197530   (epoch 1)   mode=min
  value_loss: 0.202556   (epoch 2)   mode=min
  value_value_mse: 0.202553   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6215, 2.5848]
  policy_loss : [2.5184, 2.4835]
  policy_policy_acc : [0.2827, 0.2871]
  policy_top10_acc : [0.7784, 0.7876]
  policy_top5_acc : [0.6200, 0.6310]
  val_loss : [2.9106, 5.4582]
  val_policy_loss : [2.8114, 5.3561]
  val_policy_policy_acc : [0.2913, 0.2904]
  val_policy_top10_acc : [0.7909, 0.7897]
  val_policy_top5_acc : [0.6330, 0.6354]
  val_value_loss : [0.1975, 0.2003]
  val_value_value_mse : [0.1975, 0.2002]
  value_loss : [0.2062, 0.2026]
  value_value_mse : [0.2062, 0.2026]

================================================================================

RUN #20
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:59:57.900032Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game163501_game165000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.578743   (epoch 2)   mode=min
  policy_loss: 2.476937   (epoch 2)   mode=min
  policy_policy_acc: 0.286340   (epoch 2)   mode=max
  policy_top10_acc: 0.790238   (epoch 2)   mode=max
  policy_top5_acc: 0.630442   (epoch 2)   mode=max
  val_loss: 3.663229   (epoch 2)   mode=min
  val_policy_loss: 3.563358   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285514   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788212   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628971   (epoch 1)   mode=max
  val_value_loss: 0.198084   (epoch 2)   mode=min
  val_value_value_mse: 0.198063   (epoch 2)   mode=min
  value_loss: 0.203616   (epoch 2)   mode=min
  value_value_mse: 0.203580   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6138, 2.5787]
  policy_loss : [2.5102, 2.4769]
  policy_policy_acc : [0.2796, 0.2863]
  policy_top10_acc : [0.7818, 0.7902]
  policy_top5_acc : [0.6217, 0.6304]
  val_loss : [6.1869, 3.6632]
  val_policy_loss : [6.0849, 3.5634]
  val_policy_policy_acc : [0.2847, 0.2855]
  val_policy_top10_acc : [0.7882, 0.7837]
  val_policy_top5_acc : [0.6290, 0.6258]
  val_value_loss : [0.1993, 0.1981]
  val_value_value_mse : [0.1993, 0.1981]
  value_loss : [0.2063, 0.2036]
  value_value_mse : [0.2063, 0.2036]

================================================================================

History file: model_versions/chess_elo_model_V25_history.npy
Total runs: 20
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:33:03.522053Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game165001_game166500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.580400   (epoch 2)   mode=min
  policy_loss: 2.477946   (epoch 2)   mode=min
  policy_policy_acc: 0.285785   (epoch 2)   mode=max
  policy_top10_acc: 0.789964   (epoch 2)   mode=max
  policy_top5_acc: 0.631132   (epoch 2)   mode=max
  val_loss: 2.610720   (epoch 2)   mode=min
  val_policy_loss: 2.510716   (epoch 2)   mode=min
  val_policy_policy_acc: 0.291309   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787213   (epoch 1)   mode=max
  val_policy_top5_acc: 0.632168   (epoch 1)   mode=max
  val_value_loss: 0.199558   (epoch 2)   mode=min
  val_value_value_mse: 0.199541   (epoch 2)   mode=min
  value_loss: 0.204905   (epoch 2)   mode=min
  value_value_mse: 0.204905   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6196, 2.5804]
  policy_loss : [2.5160, 2.4779]
  policy_policy_acc : [0.2788, 0.2858]
  policy_top10_acc : [0.7809, 0.7900]
  policy_top5_acc : [0.6215, 0.6311]
  val_loss : [2.8944, 2.6107]
  val_policy_loss : [2.7938, 2.5107]
  val_policy_policy_acc : [0.2913, 0.2834]
  val_policy_top10_acc : [0.7872, 0.7852]
  val_policy_top5_acc : [0.6322, 0.6271]
  val_value_loss : [0.2004, 0.1996]
  val_value_value_mse : [0.2004, 0.1995]
  value_loss : [0.2074, 0.2049]
  value_value_mse : [0.2074, 0.2049]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:35:06.309089Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game166501_game168000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595161   (epoch 2)   mode=min
  policy_loss: 2.495856   (epoch 2)   mode=min
  policy_policy_acc: 0.285690   (epoch 2)   mode=max
  policy_top10_acc: 0.786684   (epoch 2)   mode=max
  policy_top5_acc: 0.626620   (epoch 2)   mode=max
  val_loss: 3.317564   (epoch 1)   mode=min
  val_policy_loss: 3.219630   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287213   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627572   (epoch 2)   mode=max
  val_value_loss: 0.194602   (epoch 1)   mode=min
  val_value_value_mse: 0.194587   (epoch 1)   mode=min
  value_loss: 0.198835   (epoch 2)   mode=min
  value_value_mse: 0.198844   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6318, 2.5952]
  policy_loss : [2.5310, 2.4959]
  policy_policy_acc : [0.2793, 0.2857]
  policy_top10_acc : [0.7776, 0.7867]
  policy_top5_acc : [0.6175, 0.6266]
  val_loss : [3.3176, 5.3693]
  val_policy_loss : [3.2196, 5.2695]
  val_policy_policy_acc : [0.2869, 0.2872]
  val_policy_top10_acc : [0.7851, 0.7833]
  val_policy_top5_acc : [0.6255, 0.6276]
  val_value_loss : [0.1946, 0.1958]
  val_value_value_mse : [0.1946, 0.1958]
  value_loss : [0.2019, 0.1988]
  value_value_mse : [0.2019, 0.1988]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:37:18.983562Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game168001_game169500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.616781   (epoch 2)   mode=min
  policy_loss: 2.515627   (epoch 2)   mode=min
  policy_policy_acc: 0.279623   (epoch 2)   mode=max
  policy_top10_acc: 0.781771   (epoch 2)   mode=max
  policy_top5_acc: 0.620661   (epoch 2)   mode=max
  val_loss: 6.739080   (epoch 2)   mode=min
  val_policy_loss: 6.638170   (epoch 2)   mode=min
  val_policy_policy_acc: 0.289011   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789311   (epoch 1)   mode=max
  val_policy_top5_acc: 0.633966   (epoch 1)   mode=max
  val_value_loss: 0.195283   (epoch 1)   mode=min
  val_value_value_mse: 0.195269   (epoch 1)   mode=min
  value_loss: 0.202591   (epoch 2)   mode=min
  value_value_mse: 0.202586   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6522, 2.6168]
  policy_loss : [2.5503, 2.5156]
  policy_policy_acc : [0.2715, 0.2796]
  policy_top10_acc : [0.7726, 0.7818]
  policy_top5_acc : [0.6115, 0.6207]
  val_loss : [8.0730, 6.7391]
  val_policy_loss : [7.9718, 6.6382]
  val_policy_policy_acc : [0.2890, 0.2858]
  val_policy_top10_acc : [0.7893, 0.7888]
  val_policy_top5_acc : [0.6340, 0.6307]
  val_value_loss : [0.1953, 0.1964]
  val_value_value_mse : [0.1953, 0.1964]
  value_loss : [0.2045, 0.2026]
  value_value_mse : [0.2045, 0.2026]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:39:24.260006Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game169501_game171000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591751   (epoch 2)   mode=min
  policy_loss: 2.489442   (epoch 2)   mode=min
  policy_policy_acc: 0.283714   (epoch 2)   mode=max
  policy_top10_acc: 0.787575   (epoch 2)   mode=max
  policy_top5_acc: 0.626547   (epoch 2)   mode=max
  val_loss: 2.606118   (epoch 1)   mode=min
  val_policy_loss: 2.506423   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289311   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787612   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627972   (epoch 2)   mode=max
  val_value_loss: 0.198837   (epoch 1)   mode=min
  val_value_value_mse: 0.198815   (epoch 1)   mode=min
  value_loss: 0.204529   (epoch 2)   mode=min
  value_value_mse: 0.204537   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6311, 2.5918]
  policy_loss : [2.5274, 2.4894]
  policy_policy_acc : [0.2758, 0.2837]
  policy_top10_acc : [0.7785, 0.7876]
  policy_top5_acc : [0.6151, 0.6265]
  val_loss : [2.6061, 2.7002]
  val_policy_loss : [2.5064, 2.5998]
  val_policy_policy_acc : [0.2854, 0.2893]
  val_policy_top10_acc : [0.7831, 0.7876]
  val_policy_top5_acc : [0.6265, 0.6280]
  val_value_loss : [0.1988, 0.2002]
  val_value_value_mse : [0.1988, 0.2002]
  value_loss : [0.2074, 0.2045]
  value_value_mse : [0.2074, 0.2045]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:41:39.586324Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game171001_game172500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598414   (epoch 2)   mode=min
  policy_loss: 2.501108   (epoch 2)   mode=min
  policy_policy_acc: 0.281119   (epoch 2)   mode=max
  policy_top10_acc: 0.786128   (epoch 2)   mode=max
  policy_top5_acc: 0.623146   (epoch 2)   mode=max
  val_loss: 3.465308   (epoch 1)   mode=min
  val_policy_loss: 3.366054   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287313   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786613   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630569   (epoch 2)   mode=max
  val_value_loss: 0.194239   (epoch 2)   mode=min
  val_value_value_mse: 0.194218   (epoch 2)   mode=min
  value_loss: 0.194590   (epoch 2)   mode=min
  value_value_mse: 0.194598   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6320, 2.5984]
  policy_loss : [2.5334, 2.5011]
  policy_policy_acc : [0.2754, 0.2811]
  policy_top10_acc : [0.7775, 0.7861]
  policy_top5_acc : [0.6149, 0.6231]
  val_loss : [3.4653, 4.7360]
  val_policy_loss : [3.3661, 4.6374]
  val_policy_policy_acc : [0.2873, 0.2855]
  val_policy_top10_acc : [0.7814, 0.7866]
  val_policy_top5_acc : [0.6261, 0.6306]
  val_value_loss : [0.1971, 0.1942]
  val_value_value_mse : [0.1971, 0.1942]
  value_loss : [0.1972, 0.1946]
  value_value_mse : [0.1972, 0.1946]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:43:51.884728Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game172501_game174000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.600608   (epoch 2)   mode=min
  policy_loss: 2.503135   (epoch 2)   mode=min
  policy_policy_acc: 0.280491   (epoch 2)   mode=max
  policy_top10_acc: 0.786508   (epoch 2)   mode=max
  policy_top5_acc: 0.624990   (epoch 2)   mode=max
  val_loss: 14.609500   (epoch 2)   mode=min
  val_policy_loss: 14.504203   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288212   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788212   (epoch 2)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 1)   mode=max
  val_value_loss: 0.194576   (epoch 1)   mode=min
  val_value_value_mse: 0.194555   (epoch 1)   mode=min
  value_loss: 0.194931   (epoch 2)   mode=min
  value_value_mse: 0.194926   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6358, 2.6006]
  policy_loss : [2.5366, 2.5031]
  policy_policy_acc : [0.2747, 0.2805]
  policy_top10_acc : [0.7781, 0.7865]
  policy_top5_acc : [0.6159, 0.6250]
  val_loss : [17.3149, 14.6095]
  val_policy_loss : [17.2086, 14.5042]
  val_policy_policy_acc : [0.2882, 0.2853]
  val_policy_top10_acc : [0.7881, 0.7882]
  val_policy_top5_acc : [0.6289, 0.6252]
  val_value_loss : [0.1946, 0.1955]
  val_value_value_mse : [0.1946, 0.1955]
  value_loss : [0.1984, 0.1949]
  value_value_mse : [0.1984, 0.1949]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:46:01.995294Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game174001_game175500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603092   (epoch 2)   mode=min
  policy_loss: 2.510384   (epoch 2)   mode=min
  policy_policy_acc: 0.279164   (epoch 2)   mode=max
  policy_top10_acc: 0.784474   (epoch 2)   mode=max
  policy_top5_acc: 0.619766   (epoch 2)   mode=max
  val_loss: 18.473202   (epoch 1)   mode=min
  val_policy_loss: 18.366367   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285415   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 2)   mode=max
  val_policy_top5_acc: 0.629870   (epoch 2)   mode=max
  val_value_loss: 0.194185   (epoch 1)   mode=min
  val_value_value_mse: 0.194183   (epoch 1)   mode=min
  value_loss: 0.190226   (epoch 2)   mode=min
  value_value_mse: 0.190239   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6393, 2.6031]
  policy_loss : [2.5422, 2.5104]
  policy_policy_acc : [0.2735, 0.2792]
  policy_top10_acc : [0.7735, 0.7845]
  policy_top5_acc : [0.6110, 0.6198]
  val_loss : [18.4732, 23.6928]
  val_policy_loss : [18.3664, 23.5806]
  val_policy_policy_acc : [0.2823, 0.2854]
  val_policy_top10_acc : [0.7840, 0.7857]
  val_policy_top5_acc : [0.6280, 0.6299]
  val_value_loss : [0.1942, 0.1987]
  val_value_value_mse : [0.1942, 0.1987]
  value_loss : [0.1927, 0.1902]
  value_value_mse : [0.1927, 0.1902]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:48:15.205006Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game175501_game177000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.578328   (epoch 2)   mode=min
  policy_loss: 2.479851   (epoch 2)   mode=min
  policy_policy_acc: 0.284639   (epoch 2)   mode=max
  policy_top10_acc: 0.790963   (epoch 2)   mode=max
  policy_top5_acc: 0.630230   (epoch 2)   mode=max
  val_loss: 2.829641   (epoch 1)   mode=min
  val_policy_loss: 2.732148   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283816   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783916   (epoch 1)   mode=max
  val_policy_top5_acc: 0.632767   (epoch 2)   mode=max
  val_value_loss: 0.194244   (epoch 1)   mode=min
  val_value_value_mse: 0.194226   (epoch 1)   mode=min
  value_loss: 0.196946   (epoch 2)   mode=min
  value_value_mse: 0.196947   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6134, 2.5783]
  policy_loss : [2.5133, 2.4799]
  policy_policy_acc : [0.2781, 0.2846]
  policy_top10_acc : [0.7820, 0.7910]
  policy_top5_acc : [0.6197, 0.6302]
  val_loss : [2.8296, 3.2068]
  val_policy_loss : [2.7321, 3.1087]
  val_policy_policy_acc : [0.2838, 0.2824]
  val_policy_top10_acc : [0.7839, 0.7833]
  val_policy_top5_acc : [0.6286, 0.6328]
  val_value_loss : [0.1942, 0.1950]
  val_value_value_mse : [0.1942, 0.1950]
  value_loss : [0.2001, 0.1969]
  value_value_mse : [0.2001, 0.1969]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:50:24.408810Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game177001_game178500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584359   (epoch 2)   mode=min
  policy_loss: 2.483646   (epoch 2)   mode=min
  policy_policy_acc: 0.282673   (epoch 2)   mode=max
  policy_top10_acc: 0.791764   (epoch 2)   mode=max
  policy_top5_acc: 0.629696   (epoch 2)   mode=max
  val_loss: 3.176658   (epoch 2)   mode=min
  val_policy_loss: 3.078388   (epoch 2)   mode=min
  val_policy_policy_acc: 0.291209   (epoch 1)   mode=max
  val_policy_top10_acc: 0.790110   (epoch 1)   mode=max
  val_policy_top5_acc: 0.637263   (epoch 1)   mode=max
  val_value_loss: 0.195368   (epoch 2)   mode=min
  val_value_value_mse: 0.195358   (epoch 2)   mode=min
  value_loss: 0.201834   (epoch 2)   mode=min
  value_value_mse: 0.201837   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6188, 2.5844]
  policy_loss : [2.5164, 2.4836]
  policy_policy_acc : [0.2763, 0.2827]
  policy_top10_acc : [0.7834, 0.7918]
  policy_top5_acc : [0.6197, 0.6297]
  val_loss : [3.2173, 3.1767]
  val_policy_loss : [3.1180, 3.0784]
  val_policy_policy_acc : [0.2912, 0.2865]
  val_policy_top10_acc : [0.7901, 0.7850]
  val_policy_top5_acc : [0.6373, 0.6312]
  val_value_loss : [0.1972, 0.1954]
  val_value_value_mse : [0.1972, 0.1954]
  value_loss : [0.2047, 0.2018]
  value_value_mse : [0.2047, 0.2018]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:52:32.178862Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game178501_game180000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.581569   (epoch 2)   mode=min
  policy_loss: 2.483504   (epoch 2)   mode=min
  policy_policy_acc: 0.284720   (epoch 2)   mode=max
  policy_top10_acc: 0.789240   (epoch 2)   mode=max
  policy_top5_acc: 0.628370   (epoch 2)   mode=max
  val_loss: 2.594423   (epoch 1)   mode=min
  val_policy_loss: 2.496841   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285315   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788711   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631169   (epoch 1)   mode=max
  val_value_loss: 0.194850   (epoch 1)   mode=min
  val_value_value_mse: 0.194830   (epoch 1)   mode=min
  value_loss: 0.196361   (epoch 2)   mode=min
  value_value_mse: 0.196347   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6176, 2.5816]
  policy_loss : [2.5188, 2.4835]
  policy_policy_acc : [0.2776, 0.2847]
  policy_top10_acc : [0.7807, 0.7892]
  policy_top5_acc : [0.6202, 0.6284]
  val_loss : [2.5944, 2.6185]
  val_policy_loss : [2.4968, 2.5198]
  val_policy_policy_acc : [0.2853, 0.2851]
  val_policy_top10_acc : [0.7887, 0.7827]
  val_policy_top5_acc : [0.6312, 0.6309]
  val_value_loss : [0.1948, 0.1969]
  val_value_value_mse : [0.1948, 0.1968]
  value_loss : [0.1977, 0.1964]
  value_value_mse : [0.1977, 0.1963]

================================================================================

RUN #11
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:02:10.061334Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game165001_game166500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V24
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585114   (epoch 2)   mode=min
  policy_loss: 2.482119   (epoch 2)   mode=min
  policy_policy_acc: 0.285420   (epoch 2)   mode=max
  policy_top10_acc: 0.789541   (epoch 2)   mode=max
  policy_top5_acc: 0.629574   (epoch 2)   mode=max
  val_loss: 2.675937   (epoch 1)   mode=min
  val_policy_loss: 2.577500   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285714   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785714   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627173   (epoch 1)   mode=max
  val_value_loss: 0.196580   (epoch 1)   mode=min
  val_value_value_mse: 0.196562   (epoch 1)   mode=min
  value_loss: 0.205992   (epoch 2)   mode=min
  value_value_mse: 0.205992   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6231, 2.5851]
  policy_loss : [2.5192, 2.4821]
  policy_policy_acc : [0.2769, 0.2854]
  policy_top10_acc : [0.7782, 0.7895]
  policy_top5_acc : [0.6201, 0.6296]
  val_loss : [2.6759, 3.6448]
  val_policy_loss : [2.5775, 3.5443]
  val_policy_policy_acc : [0.2857, 0.2834]
  val_policy_top10_acc : [0.7804, 0.7857]
  val_policy_top5_acc : [0.6272, 0.6245]
  val_value_loss : [0.1966, 0.1996]
  val_value_value_mse : [0.1966, 0.1996]
  value_loss : [0.2078, 0.2060]
  value_value_mse : [0.2078, 0.2060]

================================================================================

RUN #12
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:04:07.771376Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game166501_game168000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592362   (epoch 2)   mode=min
  policy_loss: 2.493399   (epoch 2)   mode=min
  policy_policy_acc: 0.286746   (epoch 2)   mode=max
  policy_top10_acc: 0.786663   (epoch 2)   mode=max
  policy_top5_acc: 0.628481   (epoch 2)   mode=max
  val_loss: 4.379311   (epoch 1)   mode=min
  val_policy_loss: 4.280304   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289211   (epoch 1)   mode=max
  val_policy_top10_acc: 0.791009   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630170   (epoch 1)   mode=max
  val_value_loss: 0.195458   (epoch 1)   mode=min
  val_value_value_mse: 0.195437   (epoch 1)   mode=min
  value_loss: 0.198677   (epoch 2)   mode=min
  value_value_mse: 0.198696   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6349, 2.5924]
  policy_loss : [2.5342, 2.4934]
  policy_policy_acc : [0.2793, 0.2867]
  policy_top10_acc : [0.7774, 0.7867]
  policy_top5_acc : [0.6166, 0.6285]
  val_loss : [4.3793, 6.1406]
  val_policy_loss : [4.2803, 6.0403]
  val_policy_policy_acc : [0.2892, 0.2862]
  val_policy_top10_acc : [0.7910, 0.7859]
  val_policy_top5_acc : [0.6302, 0.6285]
  val_value_loss : [0.1955, 0.1959]
  val_value_value_mse : [0.1954, 0.1958]
  value_loss : [0.2014, 0.1987]
  value_value_mse : [0.2014, 0.1987]

================================================================================

RUN #13
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:06:11.559685Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game168001_game169500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.616694   (epoch 2)   mode=min
  policy_loss: 2.515828   (epoch 2)   mode=min
  policy_policy_acc: 0.278274   (epoch 2)   mode=max
  policy_top10_acc: 0.781087   (epoch 2)   mode=max
  policy_top5_acc: 0.620116   (epoch 2)   mode=max
  val_loss: 2.585085   (epoch 2)   mode=min
  val_policy_loss: 2.486488   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287313   (epoch 1)   mode=max
  val_policy_top10_acc: 0.791109   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631369   (epoch 1)   mode=max
  val_value_loss: 0.196738   (epoch 2)   mode=min
  val_value_value_mse: 0.196724   (epoch 2)   mode=min
  value_loss: 0.201936   (epoch 2)   mode=min
  value_value_mse: 0.201934   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6550, 2.6167]
  policy_loss : [2.5526, 2.5158]
  policy_policy_acc : [0.2727, 0.2783]
  policy_top10_acc : [0.7723, 0.7811]
  policy_top5_acc : [0.6097, 0.6201]
  val_loss : [7.8044, 2.5851]
  val_policy_loss : [7.7015, 2.4865]
  val_policy_policy_acc : [0.2873, 0.2846]
  val_policy_top10_acc : [0.7911, 0.7855]
  val_policy_top5_acc : [0.6314, 0.6286]
  val_value_loss : [0.1992, 0.1967]
  val_value_value_mse : [0.1992, 0.1967]
  value_loss : [0.2051, 0.2019]
  value_value_mse : [0.2050, 0.2019]

================================================================================

RUN #14
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:08:13.079966Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game169501_game171000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591456   (epoch 2)   mode=min
  policy_loss: 2.488858   (epoch 2)   mode=min
  policy_policy_acc: 0.282597   (epoch 2)   mode=max
  policy_top10_acc: 0.787857   (epoch 2)   mode=max
  policy_top5_acc: 0.627018   (epoch 2)   mode=max
  val_loss: 2.598543   (epoch 2)   mode=min
  val_policy_loss: 2.497833   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287213   (epoch 2)   mode=max
  val_policy_top10_acc: 0.781818   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630270   (epoch 2)   mode=max
  val_value_loss: 0.198254   (epoch 1)   mode=min
  val_value_value_mse: 0.198247   (epoch 1)   mode=min
  value_loss: 0.205245   (epoch 2)   mode=min
  value_value_mse: 0.205247   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6314, 2.5915]
  policy_loss : [2.5276, 2.4889]
  policy_policy_acc : [0.2758, 0.2826]
  policy_top10_acc : [0.7773, 0.7879]
  policy_top5_acc : [0.6155, 0.6270]
  val_loss : [2.7077, 2.5985]
  val_policy_loss : [2.6084, 2.4978]
  val_policy_policy_acc : [0.2819, 0.2872]
  val_policy_top10_acc : [0.7816, 0.7818]
  val_policy_top5_acc : [0.6272, 0.6303]
  val_value_loss : [0.1983, 0.2010]
  val_value_value_mse : [0.1982, 0.2010]
  value_loss : [0.2074, 0.2052]
  value_value_mse : [0.2074, 0.2052]

================================================================================

RUN #15
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:10:22.642704Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game171001_game172500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.599338   (epoch 2)   mode=min
  policy_loss: 2.501986   (epoch 2)   mode=min
  policy_policy_acc: 0.280265   (epoch 2)   mode=max
  policy_top10_acc: 0.785555   (epoch 2)   mode=max
  policy_top5_acc: 0.623680   (epoch 2)   mode=max
  val_loss: 6.774745   (epoch 1)   mode=min
  val_policy_loss: 6.674379   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285514   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783117   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623477   (epoch 1)   mode=max
  val_value_loss: 0.194309   (epoch 2)   mode=min
  val_value_value_mse: 0.194295   (epoch 2)   mode=min
  value_loss: 0.194744   (epoch 2)   mode=min
  value_value_mse: 0.194740   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6346, 2.5993]
  policy_loss : [2.5359, 2.5020]
  policy_policy_acc : [0.2733, 0.2803]
  policy_top10_acc : [0.7773, 0.7856]
  policy_top5_acc : [0.6150, 0.6237]
  val_loss : [6.7747, 7.3515]
  val_policy_loss : [6.6744, 7.2512]
  val_policy_policy_acc : [0.2855, 0.2807]
  val_policy_top10_acc : [0.7831, 0.7806]
  val_policy_top5_acc : [0.6235, 0.6230]
  val_value_loss : [0.1953, 0.1943]
  val_value_value_mse : [0.1952, 0.1943]
  value_loss : [0.1975, 0.1947]
  value_value_mse : [0.1975, 0.1947]

================================================================================

RUN #16
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:12:34.321567Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game172501_game174000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598444   (epoch 2)   mode=min
  policy_loss: 2.500483   (epoch 2)   mode=min
  policy_policy_acc: 0.282506   (epoch 2)   mode=max
  policy_top10_acc: 0.784975   (epoch 2)   mode=max
  policy_top5_acc: 0.625675   (epoch 2)   mode=max
  val_loss: 5.930133   (epoch 1)   mode=min
  val_policy_loss: 5.828606   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287013   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787612   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631868   (epoch 2)   mode=max
  val_value_loss: 0.195360   (epoch 2)   mode=min
  val_value_value_mse: 0.195345   (epoch 2)   mode=min
  value_loss: 0.195877   (epoch 2)   mode=min
  value_value_mse: 0.195876   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6350, 2.5984]
  policy_loss : [2.5356, 2.5005]
  policy_policy_acc : [0.2751, 0.2825]
  policy_top10_acc : [0.7783, 0.7850]
  policy_top5_acc : [0.6150, 0.6257]
  val_loss : [5.9301, 10.5649]
  val_policy_loss : [5.8286, 10.4622]
  val_policy_policy_acc : [0.2870, 0.2849]
  val_policy_top10_acc : [0.7876, 0.7833]
  val_policy_top5_acc : [0.6316, 0.6319]
  val_value_loss : [0.1987, 0.1954]
  val_value_value_mse : [0.1986, 0.1953]
  value_loss : [0.1988, 0.1959]
  value_value_mse : [0.1988, 0.1959]

================================================================================

RUN #17
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:14:43.895490Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game174001_game175500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.602236   (epoch 2)   mode=min
  policy_loss: 2.507342   (epoch 2)   mode=min
  policy_policy_acc: 0.279916   (epoch 2)   mode=max
  policy_top10_acc: 0.783393   (epoch 2)   mode=max
  policy_top5_acc: 0.621435   (epoch 2)   mode=max
  val_loss: 4.166947   (epoch 1)   mode=min
  val_policy_loss: 4.069111   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288112   (epoch 1)   mode=max
  val_policy_top10_acc: 0.790609   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627772   (epoch 1)   mode=max
  val_value_loss: 0.193313   (epoch 1)   mode=min
  val_value_value_mse: 0.193304   (epoch 1)   mode=min
  value_loss: 0.190189   (epoch 2)   mode=min
  value_value_mse: 0.190189   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6377, 2.6022]
  policy_loss : [2.5410, 2.5073]
  policy_policy_acc : [0.2722, 0.2799]
  policy_top10_acc : [0.7749, 0.7834]
  policy_top5_acc : [0.6105, 0.6214]
  val_loss : [4.1669, 5.8829]
  val_policy_loss : [4.0691, 5.7806]
  val_policy_policy_acc : [0.2881, 0.2828]
  val_policy_top10_acc : [0.7906, 0.7810]
  val_policy_top5_acc : [0.6278, 0.6236]
  val_value_loss : [0.1933, 0.2003]
  val_value_value_mse : [0.1933, 0.2003]
  value_loss : [0.1933, 0.1902]
  value_value_mse : [0.1932, 0.1902]

================================================================================

RUN #18
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:16:58.198334Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game175501_game177000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.577367   (epoch 2)   mode=min
  policy_loss: 2.478591   (epoch 2)   mode=min
  policy_policy_acc: 0.285585   (epoch 2)   mode=max
  policy_top10_acc: 0.791020   (epoch 2)   mode=max
  policy_top5_acc: 0.629020   (epoch 2)   mode=max
  val_loss: 2.580819   (epoch 1)   mode=min
  val_policy_loss: 2.483495   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289311   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789311   (epoch 2)   mode=max
  val_policy_top5_acc: 0.634466   (epoch 1)   mode=max
  val_value_loss: 0.194187   (epoch 1)   mode=min
  val_value_value_mse: 0.194169   (epoch 1)   mode=min
  value_loss: 0.197556   (epoch 2)   mode=min
  value_value_mse: 0.197556   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6097, 2.5774]
  policy_loss : [2.5095, 2.4786]
  policy_policy_acc : [0.2786, 0.2856]
  policy_top10_acc : [0.7833, 0.7910]
  policy_top5_acc : [0.6206, 0.6290]
  val_loss : [2.5808, 2.7757]
  val_policy_loss : [2.4835, 2.6779]
  val_policy_policy_acc : [0.2893, 0.2880]
  val_policy_top10_acc : [0.7847, 0.7893]
  val_policy_top5_acc : [0.6345, 0.6314]
  val_value_loss : [0.1942, 0.1948]
  val_value_value_mse : [0.1942, 0.1948]
  value_loss : [0.2003, 0.1976]
  value_value_mse : [0.2003, 0.1976]

================================================================================

RUN #19
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:19:09.318259Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game177001_game178500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584234   (epoch 2)   mode=min
  policy_loss: 2.483316   (epoch 2)   mode=min
  policy_policy_acc: 0.286319   (epoch 2)   mode=max
  policy_top10_acc: 0.791021   (epoch 2)   mode=max
  policy_top5_acc: 0.629377   (epoch 2)   mode=max
  val_loss: 3.570063   (epoch 1)   mode=min
  val_policy_loss: 3.470263   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286713   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788512   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632168   (epoch 2)   mode=max
  val_value_loss: 0.196127   (epoch 2)   mode=min
  val_value_value_mse: 0.196108   (epoch 2)   mode=min
  value_loss: 0.202128   (epoch 2)   mode=min
  value_value_mse: 0.202128   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6170, 2.5842]
  policy_loss : [2.5155, 2.4833]
  policy_policy_acc : [0.2772, 0.2863]
  policy_top10_acc : [0.7838, 0.7910]
  policy_top5_acc : [0.6203, 0.6294]
  val_loss : [3.5701, 3.9128]
  val_policy_loss : [3.4703, 3.8137]
  val_policy_policy_acc : [0.2867, 0.2840]
  val_policy_top10_acc : [0.7852, 0.7885]
  val_policy_top5_acc : [0.6285, 0.6322]
  val_value_loss : [0.1980, 0.1961]
  val_value_value_mse : [0.1980, 0.1961]
  value_loss : [0.2033, 0.2021]
  value_value_mse : [0.2033, 0.2021]

================================================================================

RUN #20
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:21:17.512814Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game178501_game180000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.582971   (epoch 2)   mode=min
  policy_loss: 2.485085   (epoch 2)   mode=min
  policy_policy_acc: 0.284878   (epoch 2)   mode=max
  policy_top10_acc: 0.788648   (epoch 2)   mode=max
  policy_top5_acc: 0.628548   (epoch 2)   mode=max
  val_loss: 2.606071   (epoch 2)   mode=min
  val_policy_loss: 2.509408   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284416   (epoch 1)   mode=max
  val_policy_top10_acc: 0.780020   (epoch 1)   mode=max
  val_policy_top5_acc: 0.629171   (epoch 2)   mode=max
  val_value_loss: 0.192903   (epoch 1)   mode=min
  val_value_value_mse: 0.192875   (epoch 1)   mode=min
  value_loss: 0.195712   (epoch 2)   mode=min
  value_value_mse: 0.195705   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6133, 2.5830]
  policy_loss : [2.5143, 2.4851]
  policy_policy_acc : [0.2778, 0.2849]
  policy_top10_acc : [0.7816, 0.7886]
  policy_top5_acc : [0.6185, 0.6285]
  val_loss : [2.6074, 2.6061]
  val_policy_loss : [2.5108, 2.5094]
  val_policy_policy_acc : [0.2844, 0.2839]
  val_policy_top10_acc : [0.7800, 0.7798]
  val_policy_top5_acc : [0.6271, 0.6292]
  val_value_loss : [0.1929, 0.1931]
  val_value_value_mse : [0.1929, 0.1930]
  value_loss : [0.1982, 0.1957]
  value_value_mse : [0.1982, 0.1957]

================================================================================

History file: model_versions/chess_elo_model_V26_history.npy
Total runs: 17
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:54:44.847832Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game180001_game181500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591312   (epoch 2)   mode=min
  policy_loss: 2.492051   (epoch 2)   mode=min
  policy_policy_acc: 0.283686   (epoch 2)   mode=max
  policy_top10_acc: 0.787697   (epoch 2)   mode=max
  policy_top5_acc: 0.628687   (epoch 2)   mode=max
  val_loss: 2.593538   (epoch 1)   mode=min
  val_policy_loss: 2.494870   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287113   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785514   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626773   (epoch 1)   mode=max
  val_value_loss: 0.196039   (epoch 2)   mode=min
  val_value_value_mse: 0.196017   (epoch 2)   mode=min
  value_loss: 0.198532   (epoch 2)   mode=min
  value_value_mse: 0.198534   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6321, 2.5913]
  policy_loss : [2.5312, 2.4921]
  policy_policy_acc : [0.2754, 0.2837]
  policy_top10_acc : [0.7800, 0.7877]
  policy_top5_acc : [0.6173, 0.6287]
  val_loss : [2.5935, 18.6004]
  val_policy_loss : [2.4949, 18.4926]
  val_policy_policy_acc : [0.2871, 0.2854]
  val_policy_top10_acc : [0.7837, 0.7855]
  val_policy_top5_acc : [0.6268, 0.6232]
  val_value_loss : [0.1969, 0.1960]
  val_value_value_mse : [0.1969, 0.1960]
  value_loss : [0.2018, 0.1985]
  value_value_mse : [0.2018, 0.1985]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:56:57.588855Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game181501_game183000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595367   (epoch 2)   mode=min
  policy_loss: 2.494686   (epoch 2)   mode=min
  policy_policy_acc: 0.282947   (epoch 2)   mode=max
  policy_top10_acc: 0.789454   (epoch 2)   mode=max
  policy_top5_acc: 0.626881   (epoch 2)   mode=max
  val_loss: 6.767928   (epoch 1)   mode=min
  val_policy_loss: 6.666595   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286913   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787812   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630270   (epoch 1)   mode=max
  val_value_loss: 0.197287   (epoch 1)   mode=min
  val_value_value_mse: 0.197281   (epoch 1)   mode=min
  value_loss: 0.201412   (epoch 2)   mode=min
  value_value_mse: 0.201416   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6331, 2.5954]
  policy_loss : [2.5307, 2.4947]
  policy_policy_acc : [0.2758, 0.2829]
  policy_top10_acc : [0.7800, 0.7895]
  policy_top5_acc : [0.6174, 0.6269]
  val_loss : [6.7679, 8.1381]
  val_policy_loss : [6.6666, 8.0346]
  val_policy_policy_acc : [0.2869, 0.2855]
  val_policy_top10_acc : [0.7851, 0.7878]
  val_policy_top5_acc : [0.6303, 0.6272]
  val_value_loss : [0.1973, 0.1998]
  val_value_value_mse : [0.1973, 0.1998]
  value_loss : [0.2045, 0.2014]
  value_value_mse : [0.2045, 0.2014]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T14:59:46.141437Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game183001_game184500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.588662   (epoch 2)   mode=min
  policy_loss: 2.488742   (epoch 2)   mode=min
  policy_policy_acc: 0.284721   (epoch 2)   mode=max
  policy_top10_acc: 0.787159   (epoch 2)   mode=max
  policy_top5_acc: 0.627469   (epoch 2)   mode=max
  val_loss: 2.589577   (epoch 1)   mode=min
  val_policy_loss: 2.491303   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285215   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787912   (epoch 1)   mode=max
  val_policy_top5_acc: 0.634865   (epoch 1)   mode=max
  val_value_loss: 0.196271   (epoch 1)   mode=min
  val_value_value_mse: 0.196254   (epoch 1)   mode=min
  value_loss: 0.199964   (epoch 2)   mode=min
  value_value_mse: 0.199966   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6222, 2.5887]
  policy_loss : [2.5215, 2.4887]
  policy_policy_acc : [0.2774, 0.2847]
  policy_top10_acc : [0.7779, 0.7872]
  policy_top5_acc : [0.6181, 0.6275]
  val_loss : [2.5896, 3.0197]
  val_policy_loss : [2.4913, 2.9204]
  val_policy_policy_acc : [0.2852, 0.2848]
  val_policy_top10_acc : [0.7879, 0.7870]
  val_policy_top5_acc : [0.6349, 0.6281]
  val_value_loss : [0.1963, 0.1976]
  val_value_value_mse : [0.1963, 0.1975]
  value_loss : [0.2018, 0.2000]
  value_value_mse : [0.2017, 0.2000]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:04:09.649791Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game184501_game186000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587400   (epoch 2)   mode=min
  policy_loss: 2.488023   (epoch 2)   mode=min
  policy_policy_acc: 0.285046   (epoch 2)   mode=max
  policy_top10_acc: 0.786030   (epoch 2)   mode=max
  policy_top5_acc: 0.626630   (epoch 2)   mode=max
  val_loss: 3.957767   (epoch 2)   mode=min
  val_policy_loss: 3.859297   (epoch 2)   mode=min
  val_policy_policy_acc: 0.284715   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787512   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625674   (epoch 2)   mode=max
  val_value_loss: 0.194052   (epoch 1)   mode=min
  val_value_value_mse: 0.194047   (epoch 1)   mode=min
  value_loss: 0.198868   (epoch 2)   mode=min
  value_value_mse: 0.198870   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6216, 2.5874]
  policy_loss : [2.5210, 2.4880]
  policy_policy_acc : [0.2779, 0.2850]
  policy_top10_acc : [0.7794, 0.7860]
  policy_top5_acc : [0.6175, 0.6266]
  val_loss : [4.4823, 3.9578]
  val_policy_loss : [4.3840, 3.8593]
  val_policy_policy_acc : [0.2816, 0.2847]
  val_policy_top10_acc : [0.7875, 0.7825]
  val_policy_top5_acc : [0.6238, 0.6257]
  val_value_loss : [0.1941, 0.1950]
  val_value_value_mse : [0.1940, 0.1949]
  value_loss : [0.2010, 0.1989]
  value_value_mse : [0.2010, 0.1989]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:10:51.123673Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game186001_game187500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.579466   (epoch 2)   mode=min
  policy_loss: 2.478822   (epoch 2)   mode=min
  policy_policy_acc: 0.285201   (epoch 2)   mode=max
  policy_top10_acc: 0.790660   (epoch 2)   mode=max
  policy_top5_acc: 0.630736   (epoch 2)   mode=max
  val_loss: 2.626426   (epoch 2)   mode=min
  val_policy_loss: 2.526607   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287413   (epoch 1)   mode=max
  val_policy_top10_acc: 0.788911   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632967   (epoch 1)   mode=max
  val_value_loss: 0.199109   (epoch 1)   mode=min
  val_value_value_mse: 0.199085   (epoch 1)   mode=min
  value_loss: 0.201286   (epoch 2)   mode=min
  value_value_mse: 0.201285   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6144, 2.5795]
  policy_loss : [2.5127, 2.4788]
  policy_policy_acc : [0.2782, 0.2852]
  policy_top10_acc : [0.7809, 0.7907]
  policy_top5_acc : [0.6221, 0.6307]
  val_loss : [2.6988, 2.6264]
  val_policy_loss : [2.5990, 2.5266]
  val_policy_policy_acc : [0.2874, 0.2850]
  val_policy_top10_acc : [0.7887, 0.7889]
  val_policy_top5_acc : [0.6330, 0.6275]
  val_value_loss : [0.1991, 0.1993]
  val_value_value_mse : [0.1991, 0.1993]
  value_loss : [0.2034, 0.2013]
  value_value_mse : [0.2034, 0.2013]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:15:49.843040Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game187501_game189000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585543   (epoch 2)   mode=min
  policy_loss: 2.489261   (epoch 2)   mode=min
  policy_policy_acc: 0.281525   (epoch 2)   mode=max
  policy_top10_acc: 0.788267   (epoch 2)   mode=max
  policy_top5_acc: 0.627088   (epoch 2)   mode=max
  val_loss: 2.797900   (epoch 1)   mode=min
  val_policy_loss: 2.700310   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289710   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788811   (epoch 2)   mode=max
  val_policy_top5_acc: 0.628472   (epoch 1)   mode=max
  val_value_loss: 0.194433   (epoch 1)   mode=min
  val_value_value_mse: 0.194431   (epoch 1)   mode=min
  value_loss: 0.192533   (epoch 2)   mode=min
  value_value_mse: 0.192535   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6207, 2.5855]
  policy_loss : [2.5232, 2.4893]
  policy_policy_acc : [0.2757, 0.2815]
  policy_top10_acc : [0.7806, 0.7883]
  policy_top5_acc : [0.6199, 0.6271]
  val_loss : [2.7979, 3.1822]
  val_policy_loss : [2.7003, 3.0822]
  val_policy_policy_acc : [0.2843, 0.2897]
  val_policy_top10_acc : [0.7866, 0.7888]
  val_policy_top5_acc : [0.6285, 0.6278]
  val_value_loss : [0.1944, 0.1989]
  val_value_value_mse : [0.1944, 0.1989]
  value_loss : [0.1949, 0.1925]
  value_value_mse : [0.1949, 0.1925]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T15:19:54.893807Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game189001_game190500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585340   (epoch 2)   mode=min
  policy_loss: 2.486949   (epoch 2)   mode=min
  policy_policy_acc: 0.282547   (epoch 2)   mode=max
  policy_top10_acc: 0.787957   (epoch 2)   mode=max
  policy_top5_acc: 0.627207   (epoch 2)   mode=max
  val_loss: 3.688999   (epoch 2)   mode=min
  val_policy_loss: 3.588873   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285914   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787113   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628372   (epoch 1)   mode=max
  val_value_loss: 0.195584   (epoch 1)   mode=min
  val_value_value_mse: 0.195568   (epoch 1)   mode=min
  value_loss: 0.196798   (epoch 2)   mode=min
  value_value_mse: 0.196798   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6196, 2.5853]
  policy_loss : [2.5199, 2.4869]
  policy_policy_acc : [0.2770, 0.2825]
  policy_top10_acc : [0.7802, 0.7880]
  policy_top5_acc : [0.6173, 0.6272]
  val_loss : [3.7561, 3.6890]
  val_policy_loss : [3.6574, 3.5889]
  val_policy_policy_acc : [0.2859, 0.2810]
  val_policy_top10_acc : [0.7871, 0.7815]
  val_policy_top5_acc : [0.6284, 0.6221]
  val_value_loss : [0.1956, 0.1984]
  val_value_value_mse : [0.1956, 0.1984]
  value_loss : [0.1995, 0.1968]
  value_value_mse : [0.1995, 0.1968]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:23:27.053625Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game180001_game181500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V25
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594052   (epoch 2)   mode=min
  policy_loss: 2.494762   (epoch 2)   mode=min
  policy_policy_acc: 0.284035   (epoch 2)   mode=max
  policy_top10_acc: 0.788126   (epoch 2)   mode=max
  policy_top5_acc: 0.626462   (epoch 2)   mode=max
  val_loss: 3.359040   (epoch 2)   mode=min
  val_policy_loss: 3.258863   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288312   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789411   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632567   (epoch 1)   mode=max
  val_value_loss: 0.193259   (epoch 1)   mode=min
  val_value_value_mse: 0.193246   (epoch 1)   mode=min
  value_loss: 0.198622   (epoch 2)   mode=min
  value_value_mse: 0.198623   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6316, 2.5941]
  policy_loss : [2.5308, 2.4948]
  policy_policy_acc : [0.2763, 0.2840]
  policy_top10_acc : [0.7790, 0.7881]
  policy_top5_acc : [0.6172, 0.6265]
  val_loss : [3.6419, 3.3590]
  val_policy_loss : [3.5445, 3.2589]
  val_policy_policy_acc : [0.2883, 0.2860]
  val_policy_top10_acc : [0.7869, 0.7894]
  val_policy_top5_acc : [0.6326, 0.6296]
  val_value_loss : [0.1933, 0.1990]
  val_value_value_mse : [0.1932, 0.1990]
  value_loss : [0.2016, 0.1986]
  value_value_mse : [0.2016, 0.1986]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:25:35.371957Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game181501_game183000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.597890   (epoch 2)   mode=min
  policy_loss: 2.496968   (epoch 2)   mode=min
  policy_policy_acc: 0.281569   (epoch 2)   mode=max
  policy_top10_acc: 0.788438   (epoch 2)   mode=max
  policy_top5_acc: 0.625332   (epoch 2)   mode=max
  val_loss: 2.587969   (epoch 2)   mode=min
  val_policy_loss: 2.487394   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287812   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785914   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630869   (epoch 1)   mode=max
  val_value_loss: 0.195777   (epoch 1)   mode=min
  val_value_value_mse: 0.195768   (epoch 1)   mode=min
  value_loss: 0.201729   (epoch 2)   mode=min
  value_value_mse: 0.201719   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6390, 2.5979]
  policy_loss : [2.5369, 2.4970]
  policy_policy_acc : [0.2740, 0.2816]
  policy_top10_acc : [0.7780, 0.7884]
  policy_top5_acc : [0.6163, 0.6253]
  val_loss : [2.7785, 2.5880]
  val_policy_loss : [2.6803, 2.4874]
  val_policy_policy_acc : [0.2878, 0.2857]
  val_policy_top10_acc : [0.7859, 0.7854]
  val_policy_top5_acc : [0.6309, 0.6290]
  val_value_loss : [0.1958, 0.2006]
  val_value_value_mse : [0.1958, 0.2006]
  value_loss : [0.2044, 0.2017]
  value_value_mse : [0.2044, 0.2017]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:27:45.589202Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game183001_game184500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591519   (epoch 2)   mode=min
  policy_loss: 2.491606   (epoch 2)   mode=min
  policy_policy_acc: 0.284848   (epoch 2)   mode=max
  policy_top10_acc: 0.786028   (epoch 2)   mode=max
  policy_top5_acc: 0.626309   (epoch 2)   mode=max
  val_loss: 2.578092   (epoch 2)   mode=min
  val_policy_loss: 2.480296   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288911   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789411   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631768   (epoch 2)   mode=max
  val_value_loss: 0.192952   (epoch 1)   mode=min
  val_value_value_mse: 0.192925   (epoch 1)   mode=min
  value_loss: 0.199792   (epoch 2)   mode=min
  value_value_mse: 0.199787   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6262, 2.5915]
  policy_loss : [2.5251, 2.4916]
  policy_policy_acc : [0.2770, 0.2848]
  policy_top10_acc : [0.7784, 0.7860]
  policy_top5_acc : [0.6179, 0.6263]
  val_loss : [2.6176, 2.5781]
  val_policy_loss : [2.5209, 2.4803]
  val_policy_policy_acc : [0.2845, 0.2889]
  val_policy_top10_acc : [0.7836, 0.7894]
  val_policy_top5_acc : [0.6243, 0.6318]
  val_value_loss : [0.1930, 0.1952]
  val_value_value_mse : [0.1929, 0.1952]
  value_loss : [0.2020, 0.1998]
  value_value_mse : [0.2020, 0.1998]

================================================================================

RUN #11
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:30:07.251437Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game184501_game186000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592842   (epoch 2)   mode=min
  policy_loss: 2.493218   (epoch 2)   mode=min
  policy_policy_acc: 0.283682   (epoch 2)   mode=max
  policy_top10_acc: 0.785980   (epoch 2)   mode=max
  policy_top5_acc: 0.625177   (epoch 2)   mode=max
  val_loss: 3.520001   (epoch 1)   mode=min
  val_policy_loss: 3.422362   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289610   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789411   (epoch 2)   mode=max
  val_policy_top5_acc: 0.633167   (epoch 2)   mode=max
  val_value_loss: 0.192002   (epoch 2)   mode=min
  val_value_value_mse: 0.191989   (epoch 2)   mode=min
  value_loss: 0.199346   (epoch 2)   mode=min
  value_value_mse: 0.199348   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6263, 2.5928]
  policy_loss : [2.5259, 2.4932]
  policy_policy_acc : [0.2765, 0.2837]
  policy_top10_acc : [0.7769, 0.7860]
  policy_top5_acc : [0.6181, 0.6252]
  val_loss : [3.5200, 4.3793]
  val_policy_loss : [3.4224, 4.2821]
  val_policy_policy_acc : [0.2827, 0.2896]
  val_policy_top10_acc : [0.7818, 0.7894]
  val_policy_top5_acc : [0.6238, 0.6332]
  val_value_loss : [0.1939, 0.1920]
  val_value_value_mse : [0.1938, 0.1920]
  value_loss : [0.2008, 0.1993]
  value_value_mse : [0.2008, 0.1993]

================================================================================

RUN #12
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:32:21.358544Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game186001_game187500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584800   (epoch 2)   mode=min
  policy_loss: 2.483905   (epoch 2)   mode=min
  policy_policy_acc: 0.286204   (epoch 2)   mode=max
  policy_top10_acc: 0.788665   (epoch 2)   mode=max
  policy_top5_acc: 0.628644   (epoch 2)   mode=max
  val_loss: 2.583938   (epoch 2)   mode=min
  val_policy_loss: 2.486281   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287712   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788711   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 1)   mode=max
  val_value_loss: 0.194436   (epoch 1)   mode=min
  val_value_value_mse: 0.194427   (epoch 1)   mode=min
  value_loss: 0.201823   (epoch 2)   mode=min
  value_value_mse: 0.201824   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6189, 2.5848]
  policy_loss : [2.5172, 2.4839]
  policy_policy_acc : [0.2767, 0.2862]
  policy_top10_acc : [0.7815, 0.7887]
  policy_top5_acc : [0.6200, 0.6286]
  val_loss : [2.6085, 2.5839]
  val_policy_loss : [2.5111, 2.4863]
  val_policy_policy_acc : [0.2874, 0.2877]
  val_policy_top10_acc : [0.7887, 0.7861]
  val_policy_top5_acc : [0.6289, 0.6256]
  val_value_loss : [0.1944, 0.1948]
  val_value_value_mse : [0.1944, 0.1948]
  value_loss : [0.2034, 0.2018]
  value_value_mse : [0.2034, 0.2018]

================================================================================

RUN #13
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:34:26.251185Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game187501_game189000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585597   (epoch 2)   mode=min
  policy_loss: 2.489031   (epoch 2)   mode=min
  policy_policy_acc: 0.284041   (epoch 2)   mode=max
  policy_top10_acc: 0.788579   (epoch 2)   mode=max
  policy_top5_acc: 0.627571   (epoch 2)   mode=max
  val_loss: 2.675511   (epoch 1)   mode=min
  val_policy_loss: 2.577663   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289211   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787812   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631568   (epoch 2)   mode=max
  val_value_loss: 0.195316   (epoch 1)   mode=min
  val_value_value_mse: 0.195303   (epoch 1)   mode=min
  value_loss: 0.193193   (epoch 2)   mode=min
  value_value_mse: 0.193192   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6219, 2.5856]
  policy_loss : [2.5239, 2.4890]
  policy_policy_acc : [0.2769, 0.2840]
  policy_top10_acc : [0.7806, 0.7886]
  policy_top5_acc : [0.6177, 0.6276]
  val_loss : [2.6755, 3.5420]
  val_policy_loss : [2.5777, 3.4425]
  val_policy_policy_acc : [0.2843, 0.2892]
  val_policy_top10_acc : [0.7869, 0.7878]
  val_policy_top5_acc : [0.6274, 0.6316]
  val_value_loss : [0.1953, 0.1973]
  val_value_value_mse : [0.1953, 0.1973]
  value_loss : [0.1958, 0.1932]
  value_value_mse : [0.1958, 0.1932]

================================================================================

RUN #14
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:36:50.631173Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game189001_game190500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.589247   (epoch 2)   mode=min
  policy_loss: 2.490951   (epoch 2)   mode=min
  policy_policy_acc: 0.283659   (epoch 2)   mode=max
  policy_top10_acc: 0.787611   (epoch 2)   mode=max
  policy_top5_acc: 0.626292   (epoch 2)   mode=max
  val_loss: 2.627572   (epoch 1)   mode=min
  val_policy_loss: 2.530557   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287413   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782917   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630170   (epoch 1)   mode=max
  val_value_loss: 0.193628   (epoch 1)   mode=min
  val_value_value_mse: 0.193613   (epoch 1)   mode=min
  value_loss: 0.196555   (epoch 2)   mode=min
  value_value_mse: 0.196554   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6229, 2.5892]
  policy_loss : [2.5230, 2.4910]
  policy_policy_acc : [0.2755, 0.2837]
  policy_top10_acc : [0.7795, 0.7876]
  policy_top5_acc : [0.6170, 0.6263]
  val_loss : [2.6276, 3.1958]
  val_policy_loss : [2.5306, 3.0971]
  val_policy_policy_acc : [0.2874, 0.2813]
  val_policy_top10_acc : [0.7829, 0.7827]
  val_policy_top5_acc : [0.6302, 0.6260]
  val_value_loss : [0.1936, 0.1963]
  val_value_value_mse : [0.1936, 0.1963]
  value_loss : [0.1997, 0.1966]
  value_value_mse : [0.1998, 0.1966]

================================================================================

RUN #15
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:39:13.143647Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game190501_game192000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.597923   (epoch 2)   mode=min
  policy_loss: 2.498205   (epoch 2)   mode=min
  policy_policy_acc: 0.281431   (epoch 2)   mode=max
  policy_top10_acc: 0.785780   (epoch 2)   mode=max
  policy_top5_acc: 0.623481   (epoch 2)   mode=max
  val_loss: 2.973532   (epoch 1)   mode=min
  val_policy_loss: 2.874358   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288611   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782817   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625475   (epoch 2)   mode=max
  val_value_loss: 0.195601   (epoch 2)   mode=min
  val_value_value_mse: 0.195589   (epoch 2)   mode=min
  value_loss: 0.200355   (epoch 2)   mode=min
  value_value_mse: 0.200308   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6287, 2.5979]
  policy_loss : [2.5270, 2.4982]
  policy_policy_acc : [0.2751, 0.2814]
  policy_top10_acc : [0.7777, 0.7858]
  policy_top5_acc : [0.6165, 0.6235]
  val_loss : [2.9735, 3.5963]
  val_policy_loss : [2.8744, 3.4976]
  val_policy_policy_acc : [0.2886, 0.2851]
  val_policy_top10_acc : [0.7828, 0.7808]
  val_policy_top5_acc : [0.6243, 0.6255]
  val_value_loss : [0.1975, 0.1956]
  val_value_value_mse : [0.1975, 0.1956]
  value_loss : [0.2033, 0.2004]
  value_value_mse : [0.2033, 0.2003]

================================================================================

RUN #16
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:41:20.856637Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game192001_game193500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601009   (epoch 2)   mode=min
  policy_loss: 2.501649   (epoch 2)   mode=min
  policy_policy_acc: 0.282963   (epoch 2)   mode=max
  policy_top10_acc: 0.784240   (epoch 2)   mode=max
  policy_top5_acc: 0.624025   (epoch 2)   mode=max
  val_loss: 5.479371   (epoch 1)   mode=min
  val_policy_loss: 5.379113   (epoch 1)   mode=min
  val_policy_policy_acc: 0.291908   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784116   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627972   (epoch 1)   mode=max
  val_value_loss: 0.194655   (epoch 2)   mode=min
  val_value_value_mse: 0.194647   (epoch 2)   mode=min
  value_loss: 0.198728   (epoch 2)   mode=min
  value_value_mse: 0.198730   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6374, 2.6010]
  policy_loss : [2.5368, 2.5016]
  policy_policy_acc : [0.2734, 0.2830]
  policy_top10_acc : [0.7768, 0.7842]
  policy_top5_acc : [0.6125, 0.6240]
  val_loss : [5.4794, 6.1452]
  val_policy_loss : [5.3791, 6.0456]
  val_policy_policy_acc : [0.2868, 0.2919]
  val_policy_top10_acc : [0.7840, 0.7841]
  val_policy_top5_acc : [0.6280, 0.6275]
  val_value_loss : [0.1967, 0.1947]
  val_value_value_mse : [0.1967, 0.1946]
  value_loss : [0.2010, 0.1987]
  value_value_mse : [0.2010, 0.1987]

================================================================================

RUN #17
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:43:28.940049Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game193501_game195000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V2.npz
  validation_indices: validation_selected_indices_V2.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594291   (epoch 2)   mode=min
  policy_loss: 2.495952   (epoch 2)   mode=min
  policy_policy_acc: 0.283807   (epoch 2)   mode=max
  policy_top10_acc: 0.786039   (epoch 2)   mode=max
  policy_top5_acc: 0.624012   (epoch 2)   mode=max
  val_loss: 2.598963   (epoch 2)   mode=min
  val_policy_loss: 2.502033   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287512   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785914   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632368   (epoch 1)   mode=max
  val_value_loss: 0.193496   (epoch 2)   mode=min
  val_value_value_mse: 0.193495   (epoch 2)   mode=min
  value_loss: 0.196691   (epoch 2)   mode=min
  value_value_mse: 0.196690   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6274, 2.5943]
  policy_loss : [2.5281, 2.4960]
  policy_policy_acc : [0.2772, 0.2838]
  policy_top10_acc : [0.7794, 0.7860]
  policy_top5_acc : [0.6164, 0.6240]
  val_loss : [2.9269, 2.5990]
  val_policy_loss : [2.8292, 2.5020]
  val_policy_policy_acc : [0.2875, 0.2844]
  val_policy_top10_acc : [0.7855, 0.7859]
  val_policy_top5_acc : [0.6324, 0.6224]
  val_value_loss : [0.1946, 0.1935]
  val_value_value_mse : [0.1946, 0.1935]
  value_loss : [0.1987, 0.1967]
  value_value_mse : [0.1987, 0.1967]

================================================================================

History file: model_versions/chess_elo_model_V27_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:45:33.137460Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V26
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.582023   (epoch 2)   mode=min
  policy_loss: 2.486005   (epoch 2)   mode=min
  policy_policy_acc: 0.285289   (epoch 2)   mode=max
  policy_top10_acc: 0.788412   (epoch 2)   mode=max
  policy_top5_acc: 0.628529   (epoch 2)   mode=max
  val_loss: 2.594416   (epoch 1)   mode=min
  val_policy_loss: 2.498168   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288412   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784416   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627972   (epoch 1)   mode=max
  val_value_loss: 0.192460   (epoch 1)   mode=min
  val_value_value_mse: 0.192456   (epoch 1)   mode=min
  value_loss: 0.192895   (epoch 2)   mode=min
  value_value_mse: 0.192900   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6161, 2.5820]
  policy_loss : [2.5182, 2.4860]
  policy_policy_acc : [0.2791, 0.2853]
  policy_top10_acc : [0.7801, 0.7884]
  policy_top5_acc : [0.6192, 0.6285]
  val_loss : [2.5944, 2.7803]
  val_policy_loss : [2.4982, 2.6815]
  val_policy_policy_acc : [0.2884, 0.2861]
  val_policy_top10_acc : [0.7844, 0.7802]
  val_policy_top5_acc : [0.6280, 0.6205]
  val_value_loss : [0.1925, 0.1976]
  val_value_value_mse : [0.1925, 0.1976]
  value_loss : [0.1961, 0.1929]
  value_value_mse : [0.1961, 0.1929]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:47:45.816972Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game151501_game153000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.586183   (epoch 2)   mode=min
  policy_loss: 2.486246   (epoch 2)   mode=min
  policy_policy_acc: 0.282922   (epoch 2)   mode=max
  policy_top10_acc: 0.790070   (epoch 2)   mode=max
  policy_top5_acc: 0.629546   (epoch 2)   mode=max
  val_loss: 2.593369   (epoch 2)   mode=min
  val_policy_loss: 2.496229   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285814   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783317   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627273   (epoch 2)   mode=max
  val_value_loss: 0.194143   (epoch 2)   mode=min
  val_value_value_mse: 0.194127   (epoch 2)   mode=min
  value_loss: 0.199802   (epoch 2)   mode=min
  value_value_mse: 0.199802   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6207, 2.5862]
  policy_loss : [2.5199, 2.4862]
  policy_policy_acc : [0.2766, 0.2829]
  policy_top10_acc : [0.7818, 0.7901]
  policy_top5_acc : [0.6193, 0.6295]
  val_loss : [2.5990, 2.5934]
  val_policy_loss : [2.5008, 2.4962]
  val_policy_policy_acc : [0.2821, 0.2858]
  val_policy_top10_acc : [0.7805, 0.7833]
  val_policy_top5_acc : [0.6228, 0.6273]
  val_value_loss : [0.1963, 0.1941]
  val_value_value_mse : [0.1963, 0.1941]
  value_loss : [0.2016, 0.1998]
  value_value_mse : [0.2016, 0.1998]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:49:57.785880Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game153001_game154500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.600937   (epoch 2)   mode=min
  policy_loss: 2.501706   (epoch 2)   mode=min
  policy_policy_acc: 0.277932   (epoch 2)   mode=max
  policy_top10_acc: 0.787113   (epoch 2)   mode=max
  policy_top5_acc: 0.624036   (epoch 2)   mode=max
  val_loss: 2.589654   (epoch 1)   mode=min
  val_policy_loss: 2.491620   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286314   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782218   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626673   (epoch 1)   mode=max
  val_value_loss: 0.194506   (epoch 2)   mode=min
  val_value_value_mse: 0.194509   (epoch 2)   mode=min
  value_loss: 0.198427   (epoch 2)   mode=min
  value_value_mse: 0.198426   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6325, 2.6009]
  policy_loss : [2.5320, 2.5017]
  policy_policy_acc : [0.2713, 0.2779]
  policy_top10_acc : [0.7794, 0.7871]
  policy_top5_acc : [0.6166, 0.6240]
  val_loss : [2.5897, 2.6031]
  val_policy_loss : [2.4916, 2.5059]
  val_policy_policy_acc : [0.2863, 0.2853]
  val_policy_top10_acc : [0.7822, 0.7794]
  val_policy_top5_acc : [0.6267, 0.6233]
  val_value_loss : [0.1960, 0.1945]
  val_value_value_mse : [0.1960, 0.1945]
  value_loss : [0.2010, 0.1984]
  value_value_mse : [0.2010, 0.1984]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:52:07.252409Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game154501_game156000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.589763   (epoch 2)   mode=min
  policy_loss: 2.491568   (epoch 2)   mode=min
  policy_policy_acc: 0.282144   (epoch 2)   mode=max
  policy_top10_acc: 0.787942   (epoch 2)   mode=max
  policy_top5_acc: 0.624760   (epoch 2)   mode=max
  val_loss: 2.598068   (epoch 1)   mode=min
  val_policy_loss: 2.502023   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784615   (epoch 1)   mode=max
  val_policy_top5_acc: 0.624675   (epoch 2)   mode=max
  val_value_loss: 0.192048   (epoch 1)   mode=min
  val_value_value_mse: 0.192042   (epoch 1)   mode=min
  value_loss: 0.196394   (epoch 2)   mode=min
  value_value_mse: 0.196394   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6237, 2.5898]
  policy_loss : [2.5248, 2.4916]
  policy_policy_acc : [0.2745, 0.2821]
  policy_top10_acc : [0.7803, 0.7879]
  policy_top5_acc : [0.6180, 0.6248]
  val_loss : [2.5981, 2.6256]
  val_policy_loss : [2.5020, 2.5246]
  val_policy_policy_acc : [0.2851, 0.2836]
  val_policy_top10_acc : [0.7846, 0.7832]
  val_policy_top5_acc : [0.6240, 0.6247]
  val_value_loss : [0.1920, 0.2019]
  val_value_value_mse : [0.1920, 0.2019]
  value_loss : [0.1977, 0.1964]
  value_value_mse : [0.1977, 0.1964]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:54:13.186395Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game156001_game157500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.612792   (epoch 2)   mode=min
  policy_loss: 2.512143   (epoch 2)   mode=min
  policy_policy_acc: 0.276389   (epoch 2)   mode=max
  policy_top10_acc: 0.782139   (epoch 2)   mode=max
  policy_top5_acc: 0.620131   (epoch 2)   mode=max
  val_loss: 2.594690   (epoch 1)   mode=min
  val_policy_loss: 2.497266   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289011   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785015   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622478   (epoch 1)   mode=max
  val_value_loss: 0.194817   (epoch 1)   mode=min
  val_value_value_mse: 0.194816   (epoch 1)   mode=min
  value_loss: 0.201299   (epoch 2)   mode=min
  value_value_mse: 0.201311   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6432, 2.6128]
  policy_loss : [2.5415, 2.5121]
  policy_policy_acc : [0.2702, 0.2764]
  policy_top10_acc : [0.7762, 0.7821]
  policy_top5_acc : [0.6121, 0.6201]
  val_loss : [2.5947, 2.6133]
  val_policy_loss : [2.4973, 2.5156]
  val_policy_policy_acc : [0.2881, 0.2890]
  val_policy_top10_acc : [0.7850, 0.7788]
  val_policy_top5_acc : [0.6225, 0.6193]
  val_value_loss : [0.1948, 0.1956]
  val_value_value_mse : [0.1948, 0.1956]
  value_loss : [0.2034, 0.2013]
  value_value_mse : [0.2034, 0.2013]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:56:16.838571Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game157501_game159000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.606026   (epoch 2)   mode=min
  policy_loss: 2.504526   (epoch 2)   mode=min
  policy_policy_acc: 0.281165   (epoch 2)   mode=max
  policy_top10_acc: 0.785818   (epoch 2)   mode=max
  policy_top5_acc: 0.623746   (epoch 2)   mode=max
  val_loss: 2.603653   (epoch 1)   mode=min
  val_policy_loss: 2.505826   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287812   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786613   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630969   (epoch 1)   mode=max
  val_value_loss: 0.195680   (epoch 1)   mode=min
  val_value_value_mse: 0.195668   (epoch 1)   mode=min
  value_loss: 0.202805   (epoch 2)   mode=min
  value_value_mse: 0.202816   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6412, 2.6060]
  policy_loss : [2.5384, 2.5045]
  policy_policy_acc : [0.2756, 0.2812]
  policy_top10_acc : [0.7758, 0.7858]
  policy_top5_acc : [0.6132, 0.6237]
  val_loss : [2.6037, 3.1820]
  val_policy_loss : [2.5058, 3.0820]
  val_policy_policy_acc : [0.2878, 0.2862]
  val_policy_top10_acc : [0.7866, 0.7833]
  val_policy_top5_acc : [0.6310, 0.6232]
  val_value_loss : [0.1957, 0.1994]
  val_value_value_mse : [0.1957, 0.1993]
  value_loss : [0.2055, 0.2028]
  value_value_mse : [0.2055, 0.2028]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T16:58:22.843252Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game159001_game160500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595284   (epoch 2)   mode=min
  policy_loss: 2.494150   (epoch 2)   mode=min
  policy_policy_acc: 0.283443   (epoch 2)   mode=max
  policy_top10_acc: 0.785291   (epoch 2)   mode=max
  policy_top5_acc: 0.625527   (epoch 2)   mode=max
  val_loss: 2.752908   (epoch 1)   mode=min
  val_policy_loss: 2.656350   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286314   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626673   (epoch 1)   mode=max
  val_value_loss: 0.192980   (epoch 1)   mode=min
  val_value_value_mse: 0.192969   (epoch 1)   mode=min
  value_loss: 0.202253   (epoch 2)   mode=min
  value_value_mse: 0.202251   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6309, 2.5953]
  policy_loss : [2.5288, 2.4942]
  policy_policy_acc : [0.2773, 0.2834]
  policy_top10_acc : [0.7794, 0.7853]
  policy_top5_acc : [0.6165, 0.6255]
  val_loss : [2.7529, 3.0380]
  val_policy_loss : [2.6564, 2.9375]
  val_policy_policy_acc : [0.2863, 0.2861]
  val_policy_top10_acc : [0.7853, 0.7827]
  val_policy_top5_acc : [0.6267, 0.6242]
  val_value_loss : [0.1930, 0.2005]
  val_value_value_mse : [0.1930, 0.2005]
  value_loss : [0.2042, 0.2023]
  value_value_mse : [0.2042, 0.2023]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:00:28.266166Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game160501_game162000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587243   (epoch 2)   mode=min
  policy_loss: 2.489920   (epoch 2)   mode=min
  policy_policy_acc: 0.283419   (epoch 2)   mode=max
  policy_top10_acc: 0.787867   (epoch 2)   mode=max
  policy_top5_acc: 0.628128   (epoch 2)   mode=max
  val_loss: 2.769185   (epoch 1)   mode=min
  val_policy_loss: 2.672429   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289211   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 2)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 2)   mode=max
  val_value_loss: 0.192681   (epoch 2)   mode=min
  val_value_value_mse: 0.192674   (epoch 2)   mode=min
  value_loss: 0.194755   (epoch 2)   mode=min
  value_value_mse: 0.194756   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6237, 2.5872]
  policy_loss : [2.5251, 2.4899]
  policy_policy_acc : [0.2747, 0.2834]
  policy_top10_acc : [0.7794, 0.7879]
  policy_top5_acc : [0.6184, 0.6281]
  val_loss : [2.7692, 2.8215]
  val_policy_loss : [2.6724, 2.7251]
  val_policy_policy_acc : [0.2858, 0.2892]
  val_policy_top10_acc : [0.7802, 0.7832]
  val_policy_top5_acc : [0.6239, 0.6289]
  val_value_loss : [0.1934, 0.1927]
  val_value_value_mse : [0.1933, 0.1927]
  value_loss : [0.1971, 0.1948]
  value_value_mse : [0.1971, 0.1948]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:02:34.318465Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game162001_game163500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587141   (epoch 2)   mode=min
  policy_loss: 2.485703   (epoch 2)   mode=min
  policy_policy_acc: 0.286524   (epoch 2)   mode=max
  policy_top10_acc: 0.787686   (epoch 2)   mode=max
  policy_top5_acc: 0.628323   (epoch 2)   mode=max
  val_loss: 2.605028   (epoch 1)   mode=min
  val_policy_loss: 2.503461   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287612   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625075   (epoch 1)   mode=max
  val_value_loss: 0.194531   (epoch 2)   mode=min
  val_value_value_mse: 0.194524   (epoch 2)   mode=min
  value_loss: 0.202738   (epoch 2)   mode=min
  value_value_mse: 0.202736   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6207, 2.5871]
  policy_loss : [2.5190, 2.4857]
  policy_policy_acc : [0.2798, 0.2865]
  policy_top10_acc : [0.7807, 0.7877]
  policy_top5_acc : [0.6200, 0.6283]
  val_loss : [2.6050, 3.0671]
  val_policy_loss : [2.5035, 2.9696]
  val_policy_policy_acc : [0.2876, 0.2807]
  val_policy_top10_acc : [0.7851, 0.7834]
  val_policy_top5_acc : [0.6251, 0.6233]
  val_value_loss : [0.2031, 0.1945]
  val_value_value_mse : [0.2031, 0.1945]
  value_loss : [0.2033, 0.2027]
  value_value_mse : [0.2033, 0.2027]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:04:39.478748Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game163501_game165000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.569305   (epoch 2)   mode=min
  policy_loss: 2.467742   (epoch 2)   mode=min
  policy_policy_acc: 0.290178   (epoch 2)   mode=max
  policy_top10_acc: 0.792651   (epoch 2)   mode=max
  policy_top5_acc: 0.633254   (epoch 2)   mode=max
  val_loss: 2.594119   (epoch 1)   mode=min
  val_policy_loss: 2.494534   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284915   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627473   (epoch 1)   mode=max
  val_value_loss: 0.197577   (epoch 2)   mode=min
  val_value_value_mse: 0.197569   (epoch 2)   mode=min
  value_loss: 0.203025   (epoch 2)   mode=min
  value_value_mse: 0.203009   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6044, 2.5693]
  policy_loss : [2.5018, 2.4677]
  policy_policy_acc : [0.2801, 0.2902]
  policy_top10_acc : [0.7832, 0.7927]
  policy_top5_acc : [0.6235, 0.6333]
  val_loss : [2.5941, 2.6143]
  val_policy_loss : [2.4945, 2.5154]
  val_policy_policy_acc : [0.2849, 0.2846]
  val_policy_top10_acc : [0.7832, 0.7816]
  val_policy_top5_acc : [0.6275, 0.6243]
  val_value_loss : [0.1991, 0.1976]
  val_value_value_mse : [0.1991, 0.1976]
  value_loss : [0.2049, 0.2030]
  value_value_mse : [0.2049, 0.2030]

================================================================================

History file: model_versions/chess_elo_model_V28_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:06:48.965645Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game165001_game166500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V27
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.589511   (epoch 2)   mode=min
  policy_loss: 2.486738   (epoch 2)   mode=min
  policy_policy_acc: 0.285775   (epoch 2)   mode=max
  policy_top10_acc: 0.787855   (epoch 2)   mode=max
  policy_top5_acc: 0.630215   (epoch 2)   mode=max
  val_loss: 2.595544   (epoch 1)   mode=min
  val_policy_loss: 2.497938   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289111   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626673   (epoch 1)   mode=max
  val_value_loss: 0.193843   (epoch 2)   mode=min
  val_value_value_mse: 0.193821   (epoch 2)   mode=min
  value_loss: 0.205554   (epoch 2)   mode=min
  value_value_mse: 0.205554   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6181, 2.5895]
  policy_loss : [2.5147, 2.4867]
  policy_policy_acc : [0.2792, 0.2858]
  policy_top10_acc : [0.7806, 0.7879]
  policy_top5_acc : [0.6218, 0.6302]
  val_loss : [2.5955, 2.6061]
  val_policy_loss : [2.4979, 2.5091]
  val_policy_policy_acc : [0.2891, 0.2828]
  val_policy_top10_acc : [0.7832, 0.7753]
  val_policy_top5_acc : [0.6267, 0.6211]
  val_value_loss : [0.1953, 0.1938]
  val_value_value_mse : [0.1953, 0.1938]
  value_loss : [0.2068, 0.2056]
  value_value_mse : [0.2068, 0.2056]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:08:48.803120Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game166501_game168000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594961   (epoch 2)   mode=min
  policy_loss: 2.496335   (epoch 2)   mode=min
  policy_policy_acc: 0.286182   (epoch 2)   mode=max
  policy_top10_acc: 0.785554   (epoch 2)   mode=max
  policy_top5_acc: 0.627655   (epoch 2)   mode=max
  val_loss: 2.695385   (epoch 1)   mode=min
  val_policy_loss: 2.598859   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286314   (epoch 2)   mode=max
  val_policy_top10_acc: 0.781818   (epoch 2)   mode=max
  val_policy_top5_acc: 0.622378   (epoch 2)   mode=max
  val_value_loss: 0.192950   (epoch 1)   mode=min
  val_value_value_mse: 0.192920   (epoch 1)   mode=min
  value_loss: 0.197620   (epoch 2)   mode=min
  value_value_mse: 0.197619   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6325, 2.5950]
  policy_loss : [2.5324, 2.4963]
  policy_policy_acc : [0.2792, 0.2862]
  policy_top10_acc : [0.7774, 0.7856]
  policy_top5_acc : [0.6176, 0.6277]
  val_loss : [2.6954, 2.8361]
  val_policy_loss : [2.5989, 2.7395]
  val_policy_policy_acc : [0.2839, 0.2863]
  val_policy_top10_acc : [0.7744, 0.7818]
  val_policy_top5_acc : [0.6187, 0.6224]
  val_value_loss : [0.1930, 0.1931]
  val_value_value_mse : [0.1929, 0.1930]
  value_loss : [0.2000, 0.1976]
  value_value_mse : [0.2000, 0.1976]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:10:55.519464Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game168001_game169500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.607208   (epoch 2)   mode=min
  policy_loss: 2.506742   (epoch 2)   mode=min
  policy_policy_acc: 0.280397   (epoch 2)   mode=max
  policy_top10_acc: 0.783041   (epoch 2)   mode=max
  policy_top5_acc: 0.621266   (epoch 2)   mode=max
  val_loss: 2.597097   (epoch 2)   mode=min
  val_policy_loss: 2.501315   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286813   (epoch 2)   mode=max
  val_policy_top10_acc: 0.779221   (epoch 2)   mode=max
  val_policy_top5_acc: 0.622777   (epoch 2)   mode=max
  val_value_loss: 0.191559   (epoch 2)   mode=min
  val_value_value_mse: 0.191560   (epoch 2)   mode=min
  value_loss: 0.200977   (epoch 2)   mode=min
  value_value_mse: 0.200985   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6399, 2.6072]
  policy_loss : [2.5374, 2.5067]
  policy_policy_acc : [0.2749, 0.2804]
  policy_top10_acc : [0.7763, 0.7830]
  policy_top5_acc : [0.6129, 0.6213]
  val_loss : [2.6010, 2.5971]
  val_policy_loss : [2.5051, 2.5013]
  val_policy_policy_acc : [0.2799, 0.2868]
  val_policy_top10_acc : [0.7745, 0.7792]
  val_policy_top5_acc : [0.6180, 0.6228]
  val_value_loss : [0.1920, 0.1916]
  val_value_value_mse : [0.1920, 0.1916]
  value_loss : [0.2047, 0.2010]
  value_value_mse : [0.2047, 0.2010]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:12:57.865124Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game169501_game171000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.589262   (epoch 2)   mode=min
  policy_loss: 2.487688   (epoch 2)   mode=min
  policy_policy_acc: 0.283652   (epoch 2)   mode=max
  policy_top10_acc: 0.786948   (epoch 2)   mode=max
  policy_top5_acc: 0.625315   (epoch 2)   mode=max
  val_loss: 2.585888   (epoch 1)   mode=min
  val_policy_loss: 2.489707   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287712   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785015   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628372   (epoch 1)   mode=max
  val_value_loss: 0.192516   (epoch 1)   mode=min
  val_value_value_mse: 0.192513   (epoch 1)   mode=min
  value_loss: 0.203398   (epoch 2)   mode=min
  value_value_mse: 0.203402   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6209, 2.5893]
  policy_loss : [2.5180, 2.4877]
  policy_policy_acc : [0.2773, 0.2837]
  policy_top10_acc : [0.7802, 0.7869]
  policy_top5_acc : [0.6179, 0.6253]
  val_loss : [2.5859, 2.6078]
  val_policy_loss : [2.4897, 2.5099]
  val_policy_policy_acc : [0.2877, 0.2823]
  val_policy_top10_acc : [0.7850, 0.7755]
  val_policy_top5_acc : [0.6284, 0.6180]
  val_value_loss : [0.1925, 0.1959]
  val_value_value_mse : [0.1925, 0.1959]
  value_loss : [0.2058, 0.2034]
  value_value_mse : [0.2058, 0.2034]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:15:04.668335Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game171001_game172500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598373   (epoch 2)   mode=min
  policy_loss: 2.501391   (epoch 2)   mode=min
  policy_policy_acc: 0.280935   (epoch 2)   mode=max
  policy_top10_acc: 0.784914   (epoch 2)   mode=max
  policy_top5_acc: 0.623961   (epoch 2)   mode=max
  val_loss: 2.592390   (epoch 2)   mode=min
  val_policy_loss: 2.495839   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288312   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783516   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626274   (epoch 2)   mode=max
  val_value_loss: 0.193167   (epoch 2)   mode=min
  val_value_value_mse: 0.193166   (epoch 2)   mode=min
  value_loss: 0.193931   (epoch 2)   mode=min
  value_value_mse: 0.193940   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6296, 2.5984]
  policy_loss : [2.5313, 2.5014]
  policy_policy_acc : [0.2757, 0.2809]
  policy_top10_acc : [0.7773, 0.7849]
  policy_top5_acc : [0.6147, 0.6240]
  val_loss : [2.6407, 2.5924]
  val_policy_loss : [2.5428, 2.4958]
  val_policy_policy_acc : [0.2834, 0.2883]
  val_policy_top10_acc : [0.7822, 0.7835]
  val_policy_top5_acc : [0.6210, 0.6263]
  val_value_loss : [0.1957, 0.1932]
  val_value_value_mse : [0.1957, 0.1932]
  value_loss : [0.1965, 0.1939]
  value_value_mse : [0.1965, 0.1939]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:17:16.324977Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game172501_game174000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.601150   (epoch 2)   mode=min
  policy_loss: 2.503668   (epoch 2)   mode=min
  policy_policy_acc: 0.282458   (epoch 2)   mode=max
  policy_top10_acc: 0.783740   (epoch 2)   mode=max
  policy_top5_acc: 0.625154   (epoch 2)   mode=max
  val_loss: 2.576933   (epoch 1)   mode=min
  val_policy_loss: 2.479593   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287912   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784815   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627373   (epoch 1)   mode=max
  val_value_loss: 0.194641   (epoch 1)   mode=min
  val_value_value_mse: 0.194633   (epoch 1)   mode=min
  value_loss: 0.194929   (epoch 2)   mode=min
  value_value_mse: 0.194933   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6338, 2.6012]
  policy_loss : [2.5348, 2.5037]
  policy_policy_acc : [0.2748, 0.2825]
  policy_top10_acc : [0.7765, 0.7837]
  policy_top5_acc : [0.6149, 0.6252]
  val_loss : [2.5769, 2.6034]
  val_policy_loss : [2.4796, 2.5060]
  val_policy_policy_acc : [0.2868, 0.2879]
  val_policy_top10_acc : [0.7848, 0.7781]
  val_policy_top5_acc : [0.6274, 0.6239]
  val_value_loss : [0.1946, 0.1948]
  val_value_value_mse : [0.1946, 0.1947]
  value_loss : [0.1980, 0.1949]
  value_value_mse : [0.1980, 0.1949]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:19:27.983344Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game174001_game175500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603059   (epoch 2)   mode=min
  policy_loss: 2.508114   (epoch 2)   mode=min
  policy_policy_acc: 0.278904   (epoch 2)   mode=max
  policy_top10_acc: 0.782448   (epoch 2)   mode=max
  policy_top5_acc: 0.619998   (epoch 2)   mode=max
  val_loss: 2.598093   (epoch 1)   mode=min
  val_policy_loss: 2.501540   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286414   (epoch 2)   mode=max
  val_policy_top10_acc: 0.781918   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624476   (epoch 2)   mode=max
  val_value_loss: 0.193044   (epoch 1)   mode=min
  val_value_value_mse: 0.193051   (epoch 1)   mode=min
  value_loss: 0.189023   (epoch 2)   mode=min
  value_value_mse: 0.188999   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6378, 2.6031]
  policy_loss : [2.5413, 2.5081]
  policy_policy_acc : [0.2727, 0.2789]
  policy_top10_acc : [0.7750, 0.7824]
  policy_top5_acc : [0.6107, 0.6200]
  val_loss : [2.5981, 2.7490]
  val_policy_loss : [2.5015, 2.6519]
  val_policy_policy_acc : [0.2836, 0.2864]
  val_policy_top10_acc : [0.7796, 0.7819]
  val_policy_top5_acc : [0.6199, 0.6245]
  val_value_loss : [0.1930, 0.1938]
  val_value_value_mse : [0.1931, 0.1938]
  value_loss : [0.1925, 0.1890]
  value_value_mse : [0.1925, 0.1890]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:21:41.136305Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game175501_game177000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.581547   (epoch 2)   mode=min
  policy_loss: 2.483081   (epoch 2)   mode=min
  policy_policy_acc: 0.283392   (epoch 2)   mode=max
  policy_top10_acc: 0.789328   (epoch 2)   mode=max
  policy_top5_acc: 0.628207   (epoch 2)   mode=max
  val_loss: 2.590986   (epoch 1)   mode=min
  val_policy_loss: 2.494275   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782418   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624875   (epoch 1)   mode=max
  val_value_loss: 0.192749   (epoch 2)   mode=min
  val_value_value_mse: 0.192736   (epoch 2)   mode=min
  value_loss: 0.196942   (epoch 2)   mode=min
  value_value_mse: 0.196942   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6115, 2.5815]
  policy_loss : [2.5118, 2.4831]
  policy_policy_acc : [0.2782, 0.2834]
  policy_top10_acc : [0.7826, 0.7893]
  policy_top5_acc : [0.6200, 0.6282]
  val_loss : [2.5910, 2.6192]
  val_policy_loss : [2.4943, 2.5227]
  val_policy_policy_acc : [0.2851, 0.2829]
  val_policy_top10_acc : [0.7819, 0.7824]
  val_policy_top5_acc : [0.6249, 0.6199]
  val_value_loss : [0.1934, 0.1927]
  val_value_value_mse : [0.1934, 0.1927]
  value_loss : [0.1993, 0.1969]
  value_value_mse : [0.1993, 0.1969]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:23:50.678708Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game177001_game178500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587075   (epoch 2)   mode=min
  policy_loss: 2.486411   (epoch 2)   mode=min
  policy_policy_acc: 0.282056   (epoch 2)   mode=max
  policy_top10_acc: 0.791233   (epoch 2)   mode=max
  policy_top5_acc: 0.628847   (epoch 2)   mode=max
  val_loss: 2.585313   (epoch 1)   mode=min
  val_policy_loss: 2.488138   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289411   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785115   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625974   (epoch 1)   mode=max
  val_value_loss: 0.194368   (epoch 1)   mode=min
  val_value_value_mse: 0.194370   (epoch 1)   mode=min
  value_loss: 0.201692   (epoch 2)   mode=min
  value_value_mse: 0.201687   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6210, 2.5871]
  policy_loss : [2.5191, 2.4864]
  policy_policy_acc : [0.2749, 0.2821]
  policy_top10_acc : [0.7825, 0.7912]
  policy_top5_acc : [0.6196, 0.6288]
  val_loss : [2.5853, 2.6118]
  val_policy_loss : [2.4881, 2.5141]
  val_policy_policy_acc : [0.2894, 0.2828]
  val_policy_top10_acc : [0.7851, 0.7765]
  val_policy_top5_acc : [0.6260, 0.6197]
  val_value_loss : [0.1944, 0.1954]
  val_value_value_mse : [0.1944, 0.1954]
  value_loss : [0.2041, 0.2017]
  value_value_mse : [0.2041, 0.2017]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:26:00.074980Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game178501_game180000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.573704   (epoch 2)   mode=min
  policy_loss: 2.476134   (epoch 2)   mode=min
  policy_policy_acc: 0.284424   (epoch 2)   mode=max
  policy_top10_acc: 0.791116   (epoch 2)   mode=max
  policy_top5_acc: 0.630147   (epoch 2)   mode=max
  val_loss: 2.594971   (epoch 1)   mode=min
  val_policy_loss: 2.500003   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285914   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625075   (epoch 1)   mode=max
  val_value_loss: 0.189845   (epoch 1)   mode=min
  val_value_value_mse: 0.189842   (epoch 1)   mode=min
  value_loss: 0.194914   (epoch 2)   mode=min
  value_value_mse: 0.194909   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6097, 2.5737]
  policy_loss : [2.5117, 2.4761]
  policy_policy_acc : [0.2781, 0.2844]
  policy_top10_acc : [0.7836, 0.7911]
  policy_top5_acc : [0.6203, 0.6301]
  val_loss : [2.5950, 2.5972]
  val_policy_loss : [2.5000, 2.5013]
  val_policy_policy_acc : [0.2859, 0.2815]
  val_policy_top10_acc : [0.7853, 0.7825]
  val_policy_top5_acc : [0.6251, 0.6223]
  val_value_loss : [0.1898, 0.1915]
  val_value_value_mse : [0.1898, 0.1915]
  value_loss : [0.1962, 0.1949]
  value_value_mse : [0.1962, 0.1949]

================================================================================

History file: model_versions/chess_elo_model_V29_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:28:06.817786Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game180001_game181500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V28
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584018   (epoch 2)   mode=min
  policy_loss: 2.485323   (epoch 2)   mode=min
  policy_policy_acc: 0.283227   (epoch 2)   mode=max
  policy_top10_acc: 0.790441   (epoch 2)   mode=max
  policy_top5_acc: 0.629465   (epoch 2)   mode=max
  val_loss: 2.585389   (epoch 1)   mode=min
  val_policy_loss: 2.489641   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283816   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785514   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625075   (epoch 1)   mode=max
  val_value_loss: 0.190576   (epoch 2)   mode=min
  val_value_value_mse: 0.190560   (epoch 2)   mode=min
  value_loss: 0.197374   (epoch 2)   mode=min
  value_value_mse: 0.197375   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6219, 2.5840]
  policy_loss : [2.5216, 2.4853]
  policy_policy_acc : [0.2794, 0.2832]
  policy_top10_acc : [0.7820, 0.7904]
  policy_top5_acc : [0.6192, 0.6295]
  val_loss : [2.5854, 2.6344]
  val_policy_loss : [2.4896, 2.5391]
  val_policy_policy_acc : [0.2831, 0.2838]
  val_policy_top10_acc : [0.7855, 0.7854]
  val_policy_top5_acc : [0.6251, 0.6215]
  val_value_loss : [0.1915, 0.1906]
  val_value_value_mse : [0.1915, 0.1906]
  value_loss : [0.2005, 0.1974]
  value_value_mse : [0.2005, 0.1974]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:30:10.943161Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game181501_game183000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.589414   (epoch 2)   mode=min
  policy_loss: 2.489070   (epoch 2)   mode=min
  policy_policy_acc: 0.283219   (epoch 2)   mode=max
  policy_top10_acc: 0.788549   (epoch 2)   mode=max
  policy_top5_acc: 0.627665   (epoch 2)   mode=max
  val_loss: 2.601946   (epoch 2)   mode=min
  val_policy_loss: 2.503519   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287413   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783716   (epoch 1)   mode=max
  val_policy_top5_acc: 0.627373   (epoch 2)   mode=max
  val_value_loss: 0.194088   (epoch 1)   mode=min
  val_value_value_mse: 0.194082   (epoch 1)   mode=min
  value_loss: 0.200476   (epoch 2)   mode=min
  value_value_mse: 0.200463   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6226, 2.5894]
  policy_loss : [2.5211, 2.4891]
  policy_policy_acc : [0.2763, 0.2832]
  policy_top10_acc : [0.7824, 0.7885]
  policy_top5_acc : [0.6204, 0.6277]
  val_loss : [2.7645, 2.6019]
  val_policy_loss : [2.6673, 2.5035]
  val_policy_policy_acc : [0.2874, 0.2825]
  val_policy_top10_acc : [0.7837, 0.7828]
  val_policy_top5_acc : [0.6268, 0.6274]
  val_value_loss : [0.1941, 0.1970]
  val_value_value_mse : [0.1941, 0.1969]
  value_loss : [0.2028, 0.2005]
  value_value_mse : [0.2028, 0.2005]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:32:25.210195Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game183001_game184500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.584693   (epoch 2)   mode=min
  policy_loss: 2.485238   (epoch 2)   mode=min
  policy_policy_acc: 0.284780   (epoch 2)   mode=max
  policy_top10_acc: 0.787831   (epoch 2)   mode=max
  policy_top5_acc: 0.628317   (epoch 2)   mode=max
  val_loss: 2.590760   (epoch 1)   mode=min
  val_policy_loss: 2.493619   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287612   (epoch 1)   mode=max
  val_policy_top10_acc: 0.783117   (epoch 2)   mode=max
  val_policy_top5_acc: 0.629371   (epoch 1)   mode=max
  val_value_loss: 0.194309   (epoch 1)   mode=min
  val_value_value_mse: 0.194302   (epoch 1)   mode=min
  value_loss: 0.199092   (epoch 2)   mode=min
  value_value_mse: 0.199092   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6260, 2.5847]
  policy_loss : [2.5258, 2.4852]
  policy_policy_acc : [0.2762, 0.2848]
  policy_top10_acc : [0.7784, 0.7878]
  policy_top5_acc : [0.6166, 0.6283]
  val_loss : [2.5908, 3.5470]
  val_policy_loss : [2.4936, 3.4491]
  val_policy_policy_acc : [0.2876, 0.2847]
  val_policy_top10_acc : [0.7811, 0.7831]
  val_policy_top5_acc : [0.6294, 0.6277]
  val_value_loss : [0.1943, 0.1947]
  val_value_value_mse : [0.1943, 0.1946]
  value_loss : [0.2006, 0.1991]
  value_value_mse : [0.2006, 0.1991]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:34:36.548152Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game184501_game186000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591024   (epoch 2)   mode=min
  policy_loss: 2.491929   (epoch 2)   mode=min
  policy_policy_acc: 0.285639   (epoch 2)   mode=max
  policy_top10_acc: 0.786089   (epoch 2)   mode=max
  policy_top5_acc: 0.627094   (epoch 2)   mode=max
  val_loss: 2.708976   (epoch 1)   mode=min
  val_policy_loss: 2.612583   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288212   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784116   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626673   (epoch 1)   mode=max
  val_value_loss: 0.191633   (epoch 2)   mode=min
  val_value_value_mse: 0.191614   (epoch 2)   mode=min
  value_loss: 0.198382   (epoch 2)   mode=min
  value_value_mse: 0.198384   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6213, 2.5910]
  policy_loss : [2.5213, 2.4919]
  policy_policy_acc : [0.2772, 0.2856]
  policy_top10_acc : [0.7792, 0.7861]
  policy_top5_acc : [0.6179, 0.6271]
  val_loss : [2.7090, 2.7382]
  val_policy_loss : [2.6126, 2.6423]
  val_policy_policy_acc : [0.2882, 0.2859]
  val_policy_top10_acc : [0.7841, 0.7781]
  val_policy_top5_acc : [0.6267, 0.6229]
  val_value_loss : [0.1926, 0.1916]
  val_value_value_mse : [0.1926, 0.1916]
  value_loss : [0.1999, 0.1984]
  value_value_mse : [0.1999, 0.1984]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:36:45.692014Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game186001_game187500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.575025   (epoch 2)   mode=min
  policy_loss: 2.474821   (epoch 2)   mode=min
  policy_policy_acc: 0.286632   (epoch 2)   mode=max
  policy_top10_acc: 0.789881   (epoch 2)   mode=max
  policy_top5_acc: 0.631358   (epoch 2)   mode=max
  val_loss: 2.674592   (epoch 2)   mode=min
  val_policy_loss: 2.575114   (epoch 2)   mode=min
  val_policy_policy_acc: 0.286014   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783217   (epoch 1)   mode=max
  val_policy_top5_acc: 0.625774   (epoch 1)   mode=max
  val_value_loss: 0.190691   (epoch 1)   mode=min
  val_value_value_mse: 0.190673   (epoch 1)   mode=min
  value_loss: 0.200418   (epoch 2)   mode=min
  value_value_mse: 0.200419   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6116, 2.5750]
  policy_loss : [2.5105, 2.4748]
  policy_policy_acc : [0.2779, 0.2866]
  policy_top10_acc : [0.7840, 0.7899]
  policy_top5_acc : [0.6210, 0.6314]
  val_loss : [2.8221, 2.6746]
  val_policy_loss : [2.7266, 2.5751]
  val_policy_policy_acc : [0.2851, 0.2860]
  val_policy_top10_acc : [0.7832, 0.7773]
  val_policy_top5_acc : [0.6258, 0.6239]
  val_value_loss : [0.1907, 0.1989]
  val_value_value_mse : [0.1907, 0.1988]
  value_loss : [0.2022, 0.2004]
  value_value_mse : [0.2022, 0.2004]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:38:52.876813Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game187501_game189000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.586235   (epoch 2)   mode=min
  policy_loss: 2.490161   (epoch 2)   mode=min
  policy_policy_acc: 0.281938   (epoch 2)   mode=max
  policy_top10_acc: 0.788549   (epoch 2)   mode=max
  policy_top5_acc: 0.626766   (epoch 2)   mode=max
  val_loss: 2.594771   (epoch 1)   mode=min
  val_policy_loss: 2.496895   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286414   (epoch 2)   mode=max
  val_policy_top10_acc: 0.783417   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622378   (epoch 2)   mode=max
  val_value_loss: 0.193937   (epoch 2)   mode=min
  val_value_value_mse: 0.193926   (epoch 2)   mode=min
  value_loss: 0.192065   (epoch 2)   mode=min
  value_value_mse: 0.192063   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6250, 2.5862]
  policy_loss : [2.5276, 2.4902]
  policy_policy_acc : [0.2758, 0.2819]
  policy_top10_acc : [0.7815, 0.7885]
  policy_top5_acc : [0.6175, 0.6268]
  val_loss : [2.5948, 2.7059]
  val_policy_loss : [2.4969, 2.6089]
  val_policy_policy_acc : [0.2851, 0.2864]
  val_policy_top10_acc : [0.7834, 0.7782]
  val_policy_top5_acc : [0.6212, 0.6224]
  val_value_loss : [0.1958, 0.1939]
  val_value_value_mse : [0.1958, 0.1939]
  value_loss : [0.1948, 0.1921]
  value_value_mse : [0.1948, 0.1921]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:41:07.491045Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game189001_game190500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587871   (epoch 2)   mode=min
  policy_loss: 2.489377   (epoch 2)   mode=min
  policy_policy_acc: 0.281790   (epoch 2)   mode=max
  policy_top10_acc: 0.787433   (epoch 2)   mode=max
  policy_top5_acc: 0.627768   (epoch 2)   mode=max
  val_loss: 2.583261   (epoch 1)   mode=min
  val_policy_loss: 2.487790   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289910   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786913   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628971   (epoch 1)   mode=max
  val_value_loss: 0.191078   (epoch 1)   mode=min
  val_value_value_mse: 0.191078   (epoch 1)   mode=min
  value_loss: 0.197066   (epoch 2)   mode=min
  value_value_mse: 0.197067   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6210, 2.5879]
  policy_loss : [2.5213, 2.4894]
  policy_policy_acc : [0.2754, 0.2818]
  policy_top10_acc : [0.7805, 0.7874]
  policy_top5_acc : [0.6184, 0.6278]
  val_loss : [2.5833, 2.6610]
  val_policy_loss : [2.4878, 2.5635]
  val_policy_policy_acc : [0.2899, 0.2839]
  val_policy_top10_acc : [0.7869, 0.7855]
  val_policy_top5_acc : [0.6290, 0.6199]
  val_value_loss : [0.1911, 0.1951]
  val_value_value_mse : [0.1911, 0.1951]
  value_loss : [0.1994, 0.1971]
  value_value_mse : [0.1994, 0.1971]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:43:18.906382Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game190501_game192000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.597583   (epoch 2)   mode=min
  policy_loss: 2.498402   (epoch 2)   mode=min
  policy_policy_acc: 0.280995   (epoch 2)   mode=max
  policy_top10_acc: 0.783560   (epoch 2)   mode=max
  policy_top5_acc: 0.623055   (epoch 2)   mode=max
  val_loss: 2.585974   (epoch 1)   mode=min
  val_policy_loss: 2.489928   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287512   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785315   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628072   (epoch 2)   mode=max
  val_value_loss: 0.192125   (epoch 1)   mode=min
  val_value_value_mse: 0.192132   (epoch 1)   mode=min
  value_loss: 0.199702   (epoch 2)   mode=min
  value_value_mse: 0.199687   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6269, 2.5976]
  policy_loss : [2.5260, 2.4984]
  policy_policy_acc : [0.2751, 0.2810]
  policy_top10_acc : [0.7763, 0.7836]
  policy_top5_acc : [0.6166, 0.6231]
  val_loss : [2.5860, 2.5924]
  val_policy_loss : [2.4899, 2.4957]
  val_policy_policy_acc : [0.2873, 0.2875]
  val_policy_top10_acc : [0.7853, 0.7845]
  val_policy_top5_acc : [0.6261, 0.6281]
  val_value_loss : [0.1921, 0.1935]
  val_value_value_mse : [0.1921, 0.1935]
  value_loss : [0.2020, 0.1997]
  value_value_mse : [0.2020, 0.1997]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:45:25.794890Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game192001_game193500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.600318   (epoch 2)   mode=min
  policy_loss: 2.501484   (epoch 2)   mode=min
  policy_policy_acc: 0.283409   (epoch 2)   mode=max
  policy_top10_acc: 0.784497   (epoch 2)   mode=max
  policy_top5_acc: 0.624461   (epoch 2)   mode=max
  val_loss: 2.593178   (epoch 2)   mode=min
  val_policy_loss: 2.497667   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287512   (epoch 1)   mode=max
  val_policy_top10_acc: 0.782318   (epoch 2)   mode=max
  val_policy_top5_acc: 0.623876   (epoch 2)   mode=max
  val_value_loss: 0.191273   (epoch 2)   mode=min
  val_value_value_mse: 0.191264   (epoch 2)   mode=min
  value_loss: 0.197665   (epoch 2)   mode=min
  value_value_mse: 0.197665   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6331, 2.6003]
  policy_loss : [2.5331, 2.5015]
  policy_policy_acc : [0.2750, 0.2834]
  policy_top10_acc : [0.7763, 0.7845]
  policy_top5_acc : [0.6136, 0.6245]
  val_loss : [2.5988, 2.5932]
  val_policy_loss : [2.5007, 2.4977]
  val_policy_policy_acc : [0.2875, 0.2868]
  val_policy_top10_acc : [0.7811, 0.7823]
  val_policy_top5_acc : [0.6234, 0.6239]
  val_value_loss : [0.1962, 0.1913]
  val_value_value_mse : [0.1962, 0.1913]
  value_loss : [0.2000, 0.1977]
  value_value_mse : [0.2000, 0.1977]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:47:36.547430Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game193501_game195000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V3.npz
  validation_indices: validation_selected_indices_V3.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.587586   (epoch 2)   mode=min
  policy_loss: 2.489659   (epoch 2)   mode=min
  policy_policy_acc: 0.284999   (epoch 2)   mode=max
  policy_top10_acc: 0.788617   (epoch 2)   mode=max
  policy_top5_acc: 0.627030   (epoch 2)   mode=max
  val_loss: 2.587267   (epoch 1)   mode=min
  val_policy_loss: 2.491407   (epoch 1)   mode=min
  val_policy_policy_acc: 0.286114   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784815   (epoch 1)   mode=max
  val_policy_top5_acc: 0.622577   (epoch 1)   mode=max
  val_value_loss: 0.191786   (epoch 1)   mode=min
  val_value_value_mse: 0.191787   (epoch 1)   mode=min
  value_loss: 0.195850   (epoch 2)   mode=min
  value_value_mse: 0.195848   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6219, 2.5876]
  policy_loss : [2.5231, 2.4897]
  policy_policy_acc : [0.2772, 0.2850]
  policy_top10_acc : [0.7814, 0.7886]
  policy_top5_acc : [0.6160, 0.6270]
  val_loss : [2.5873, 2.5954]
  val_policy_loss : [2.4914, 2.4980]
  val_policy_policy_acc : [0.2858, 0.2861]
  val_policy_top10_acc : [0.7848, 0.7833]
  val_policy_top5_acc : [0.6226, 0.6202]
  val_value_loss : [0.1918, 0.1947]
  val_value_value_mse : [0.1918, 0.1947]
  value_loss : [0.1977, 0.1958]
  value_value_mse : [0.1977, 0.1958]

================================================================================

History file: model_versions/chess_elo_model_V30_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:49:42.158746Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game150001_game151500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V29
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.578630   (epoch 2)   mode=min
  policy_loss: 2.482376   (epoch 2)   mode=min
  policy_policy_acc: 0.286959   (epoch 2)   mode=max
  policy_top10_acc: 0.788864   (epoch 2)   mode=max
  policy_top5_acc: 0.630188   (epoch 2)   mode=max
  val_loss: 3.152396   (epoch 2)   mode=min
  val_policy_loss: 3.052156   (epoch 2)   mode=min
  val_policy_policy_acc: 0.289710   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787213   (epoch 1)   mode=max
  val_policy_top5_acc: 0.629071   (epoch 2)   mode=max
  val_value_loss: 0.199394   (epoch 2)   mode=min
  val_value_value_mse: 0.199394   (epoch 2)   mode=min
  value_loss: 0.192339   (epoch 2)   mode=min
  value_value_mse: 0.192346   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6149, 2.5786]
  policy_loss : [2.5173, 2.4824]
  policy_policy_acc : [0.2784, 0.2870]
  policy_top10_acc : [0.7810, 0.7889]
  policy_top5_acc : [0.6191, 0.6302]
  val_loss : [4.2060, 3.1524]
  val_policy_loss : [4.1039, 3.0522]
  val_policy_policy_acc : [0.2897, 0.2897]
  val_policy_top10_acc : [0.7872, 0.7805]
  val_policy_top5_acc : [0.6266, 0.6291]
  val_value_loss : [0.2019, 0.1994]
  val_value_value_mse : [0.2019, 0.1994]
  value_loss : [0.1948, 0.1923]
  value_value_mse : [0.1948, 0.1923]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:51:51.604929Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game151501_game153000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.580936   (epoch 2)   mode=min
  policy_loss: 2.481309   (epoch 2)   mode=min
  policy_policy_acc: 0.284441   (epoch 2)   mode=max
  policy_top10_acc: 0.792203   (epoch 2)   mode=max
  policy_top5_acc: 0.629770   (epoch 2)   mode=max
  val_loss: 5.600078   (epoch 1)   mode=min
  val_policy_loss: 5.499251   (epoch 1)   mode=min
  val_policy_policy_acc: 0.291808   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789710   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632767   (epoch 2)   mode=max
  val_value_loss: 0.197757   (epoch 1)   mode=min
  val_value_value_mse: 0.197762   (epoch 1)   mode=min
  value_loss: 0.199253   (epoch 2)   mode=min
  value_value_mse: 0.199253   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6153, 2.5809]
  policy_loss : [2.5146, 2.4813]
  policy_policy_acc : [0.2779, 0.2844]
  policy_top10_acc : [0.7829, 0.7922]
  policy_top5_acc : [0.6224, 0.6298]
  val_loss : [5.6001, 14.2797]
  val_policy_loss : [5.4993, 14.1696]
  val_policy_policy_acc : [0.2898, 0.2918]
  val_policy_top10_acc : [0.7877, 0.7897]
  val_policy_top5_acc : [0.6246, 0.6328]
  val_value_loss : [0.1978, 0.2058]
  val_value_value_mse : [0.1978, 0.2058]
  value_loss : [0.2014, 0.1993]
  value_value_mse : [0.2014, 0.1993]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:53:59.536976Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game153001_game154500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603039   (epoch 2)   mode=min
  policy_loss: 2.503944   (epoch 2)   mode=min
  policy_policy_acc: 0.278881   (epoch 2)   mode=max
  policy_top10_acc: 0.787694   (epoch 2)   mode=max
  policy_top5_acc: 0.623871   (epoch 2)   mode=max
  val_loss: 3.364197   (epoch 1)   mode=min
  val_policy_loss: 3.261889   (epoch 1)   mode=min
  val_policy_policy_acc: 0.292308   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789411   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630370   (epoch 2)   mode=max
  val_value_loss: 0.203369   (epoch 1)   mode=min
  val_value_value_mse: 0.203366   (epoch 1)   mode=min
  value_loss: 0.198016   (epoch 2)   mode=min
  value_value_mse: 0.198000   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6321, 2.6030]
  policy_loss : [2.5320, 2.5039]
  policy_policy_acc : [0.2717, 0.2789]
  policy_top10_acc : [0.7790, 0.7877]
  policy_top5_acc : [0.6153, 0.6239]
  val_loss : [3.3642, 4.0207]
  val_policy_loss : [3.2619, 3.9173]
  val_policy_policy_acc : [0.2923, 0.2897]
  val_policy_top10_acc : [0.7882, 0.7894]
  val_policy_top5_acc : [0.6298, 0.6304]
  val_value_loss : [0.2034, 0.2046]
  val_value_value_mse : [0.2034, 0.2046]
  value_loss : [0.2001, 0.1980]
  value_value_mse : [0.2001, 0.1980]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:56:11.520737Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game154501_game156000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.588116   (epoch 2)   mode=min
  policy_loss: 2.490257   (epoch 2)   mode=min
  policy_policy_acc: 0.281865   (epoch 2)   mode=max
  policy_top10_acc: 0.787750   (epoch 2)   mode=max
  policy_top5_acc: 0.627433   (epoch 2)   mode=max
  val_loss: 5.047712   (epoch 1)   mode=min
  val_policy_loss: 4.947245   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290809   (epoch 1)   mode=max
  val_policy_top10_acc: 0.785914   (epoch 1)   mode=max
  val_policy_top5_acc: 0.631868   (epoch 1)   mode=max
  val_value_loss: 0.197553   (epoch 1)   mode=min
  val_value_value_mse: 0.197554   (epoch 1)   mode=min
  value_loss: 0.195718   (epoch 2)   mode=min
  value_value_mse: 0.195718   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6185, 2.5881]
  policy_loss : [2.5198, 2.4903]
  policy_policy_acc : [0.2767, 0.2819]
  policy_top10_acc : [0.7811, 0.7878]
  policy_top5_acc : [0.6181, 0.6274]
  val_loss : [5.0477, 5.8001]
  val_policy_loss : [4.9472, 5.6958]
  val_policy_policy_acc : [0.2908, 0.2901]
  val_policy_top10_acc : [0.7859, 0.7841]
  val_policy_top5_acc : [0.6319, 0.6301]
  val_value_loss : [0.1976, 0.2043]
  val_value_value_mse : [0.1976, 0.2043]
  value_loss : [0.1973, 0.1957]
  value_value_mse : [0.1973, 0.1957]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T17:58:19.933733Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game156001_game157500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.611386   (epoch 2)   mode=min
  policy_loss: 2.511338   (epoch 2)   mode=min
  policy_policy_acc: 0.275171   (epoch 2)   mode=max
  policy_top10_acc: 0.781999   (epoch 2)   mode=max
  policy_top5_acc: 0.619262   (epoch 2)   mode=max
  val_loss: 3.994104   (epoch 2)   mode=min
  val_policy_loss: 3.893492   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288012   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789510   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628571   (epoch 2)   mode=max
  val_value_loss: 0.199080   (epoch 2)   mode=min
  val_value_value_mse: 0.199084   (epoch 2)   mode=min
  value_loss: 0.200117   (epoch 2)   mode=min
  value_value_mse: 0.200102   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6437, 2.6114]
  policy_loss : [2.5423, 2.5113]
  policy_policy_acc : [0.2699, 0.2752]
  policy_top10_acc : [0.7751, 0.7820]
  policy_top5_acc : [0.6106, 0.6193]
  val_loss : [4.1944, 3.9941]
  val_policy_loss : [4.0929, 3.8935]
  val_policy_policy_acc : [0.2880, 0.2846]
  val_policy_top10_acc : [0.7895, 0.7856]
  val_policy_top5_acc : [0.6240, 0.6286]
  val_value_loss : [0.2007, 0.1991]
  val_value_value_mse : [0.2007, 0.1991]
  value_loss : [0.2027, 0.2001]
  value_value_mse : [0.2027, 0.2001]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:00:23.397952Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game157501_game159000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.609959   (epoch 2)   mode=min
  policy_loss: 2.508139   (epoch 2)   mode=min
  policy_policy_acc: 0.280442   (epoch 2)   mode=max
  policy_top10_acc: 0.783897   (epoch 2)   mode=max
  policy_top5_acc: 0.622342   (epoch 2)   mode=max
  val_loss: 3.806094   (epoch 1)   mode=min
  val_policy_loss: 3.705755   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287213   (epoch 1)   mode=max
  val_policy_top10_acc: 0.794106   (epoch 2)   mode=max
  val_policy_top5_acc: 0.628372   (epoch 2)   mode=max
  val_value_loss: 0.198759   (epoch 1)   mode=min
  val_value_value_mse: 0.198741   (epoch 1)   mode=min
  value_loss: 0.203490   (epoch 2)   mode=min
  value_value_mse: 0.203484   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6446, 2.6100]
  policy_loss : [2.5416, 2.5081]
  policy_policy_acc : [0.2730, 0.2804]
  policy_top10_acc : [0.7769, 0.7839]
  policy_top5_acc : [0.6131, 0.6223]
  val_loss : [3.8061, 3.9805]
  val_policy_loss : [3.7058, 3.8793]
  val_policy_policy_acc : [0.2872, 0.2846]
  val_policy_top10_acc : [0.7855, 0.7941]
  val_policy_top5_acc : [0.6237, 0.6284]
  val_value_loss : [0.1988, 0.2003]
  val_value_value_mse : [0.1987, 0.2003]
  value_loss : [0.2059, 0.2035]
  value_value_mse : [0.2059, 0.2035]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:02:32.014755Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game159001_game160500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.597934   (epoch 2)   mode=min
  policy_loss: 2.497110   (epoch 2)   mode=min
  policy_policy_acc: 0.283861   (epoch 2)   mode=max
  policy_top10_acc: 0.786088   (epoch 2)   mode=max
  policy_top5_acc: 0.625148   (epoch 2)   mode=max
  val_loss: 4.252674   (epoch 2)   mode=min
  val_policy_loss: 4.149546   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287612   (epoch 2)   mode=max
  val_policy_top10_acc: 0.790410   (epoch 2)   mode=max
  val_policy_top5_acc: 0.626773   (epoch 1)   mode=max
  val_value_loss: 0.203101   (epoch 1)   mode=min
  val_value_value_mse: 0.203095   (epoch 1)   mode=min
  value_loss: 0.201679   (epoch 2)   mode=min
  value_value_mse: 0.201679   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6327, 2.5979]
  policy_loss : [2.5306, 2.4971]
  policy_policy_acc : [0.2763, 0.2839]
  policy_top10_acc : [0.7778, 0.7861]
  policy_top5_acc : [0.6165, 0.6251]
  val_loss : [4.2691, 4.2527]
  val_policy_loss : [4.1664, 4.1495]
  val_policy_policy_acc : [0.2857, 0.2876]
  val_policy_top10_acc : [0.7870, 0.7904]
  val_policy_top5_acc : [0.6268, 0.6265]
  val_value_loss : [0.2031, 0.2038]
  val_value_value_mse : [0.2031, 0.2038]
  value_loss : [0.2043, 0.2017]
  value_value_mse : [0.2043, 0.2017]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:04:38.687132Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game160501_game162000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.594543   (epoch 2)   mode=min
  policy_loss: 2.497229   (epoch 2)   mode=min
  policy_policy_acc: 0.280596   (epoch 2)   mode=max
  policy_top10_acc: 0.787451   (epoch 2)   mode=max
  policy_top5_acc: 0.624700   (epoch 2)   mode=max
  val_loss: 3.116187   (epoch 2)   mode=min
  val_policy_loss: 3.014960   (epoch 2)   mode=min
  val_policy_policy_acc: 0.289211   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784615   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630270   (epoch 2)   mode=max
  val_value_loss: 0.199044   (epoch 1)   mode=min
  val_value_value_mse: 0.199050   (epoch 1)   mode=min
  value_loss: 0.194613   (epoch 2)   mode=min
  value_value_mse: 0.194617   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6252, 2.5945]
  policy_loss : [2.5268, 2.4972]
  policy_policy_acc : [0.2750, 0.2806]
  policy_top10_acc : [0.7797, 0.7875]
  policy_top5_acc : [0.6178, 0.6247]
  val_loss : [3.2172, 3.1162]
  val_policy_loss : [3.1171, 3.0150]
  val_policy_policy_acc : [0.2848, 0.2892]
  val_policy_top10_acc : [0.7824, 0.7846]
  val_policy_top5_acc : [0.6211, 0.6303]
  val_value_loss : [0.1990, 0.2014]
  val_value_value_mse : [0.1990, 0.2014]
  value_loss : [0.1970, 0.1946]
  value_value_mse : [0.1970, 0.1946]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:06:44.711855Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game162001_game163500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595464   (epoch 2)   mode=min
  policy_loss: 2.493966   (epoch 2)   mode=min
  policy_policy_acc: 0.284159   (epoch 2)   mode=max
  policy_top10_acc: 0.787147   (epoch 2)   mode=max
  policy_top5_acc: 0.628603   (epoch 2)   mode=max
  val_loss: 2.667484   (epoch 1)   mode=min
  val_policy_loss: 2.560616   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289111   (epoch 2)   mode=max
  val_policy_top10_acc: 0.790310   (epoch 2)   mode=max
  val_policy_top5_acc: 0.629970   (epoch 2)   mode=max
  val_value_loss: 0.203106   (epoch 2)   mode=min
  val_value_value_mse: 0.203111   (epoch 2)   mode=min
  value_loss: 0.203039   (epoch 2)   mode=min
  value_value_mse: 0.203044   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6279, 2.5955]
  policy_loss : [2.5256, 2.4940]
  policy_policy_acc : [0.2800, 0.2842]
  policy_top10_acc : [0.7776, 0.7871]
  policy_top5_acc : [0.6172, 0.6286]
  val_loss : [2.6675, 4.9334]
  val_policy_loss : [2.5606, 4.8303]
  val_policy_policy_acc : [0.2842, 0.2891]
  val_policy_top10_acc : [0.7888, 0.7903]
  val_policy_top5_acc : [0.6251, 0.6300]
  val_value_loss : [0.2134, 0.2031]
  val_value_value_mse : [0.2134, 0.2031]
  value_loss : [0.2047, 0.2030]
  value_value_mse : [0.2047, 0.2030]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:08:51.941649Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game163501_game165000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.575931   (epoch 2)   mode=min
  policy_loss: 2.474828   (epoch 2)   mode=min
  policy_policy_acc: 0.289151   (epoch 2)   mode=max
  policy_top10_acc: 0.789790   (epoch 2)   mode=max
  policy_top5_acc: 0.630322   (epoch 2)   mode=max
  val_loss: 3.288216   (epoch 1)   mode=min
  val_policy_loss: 3.188167   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288212   (epoch 2)   mode=max
  val_policy_top10_acc: 0.791908   (epoch 1)   mode=max
  val_policy_top5_acc: 0.636663   (epoch 2)   mode=max
  val_value_loss: 0.198967   (epoch 1)   mode=min
  val_value_value_mse: 0.198966   (epoch 1)   mode=min
  value_loss: 0.202655   (epoch 2)   mode=min
  value_value_mse: 0.202682   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6143, 2.5759]
  policy_loss : [2.5111, 2.4748]
  policy_policy_acc : [0.2793, 0.2892]
  policy_top10_acc : [0.7803, 0.7898]
  policy_top5_acc : [0.6216, 0.6303]
  val_loss : [3.2882, 3.6138]
  val_policy_loss : [3.1882, 3.5129]
  val_policy_policy_acc : [0.2845, 0.2882]
  val_policy_top10_acc : [0.7919, 0.7896]
  val_policy_top5_acc : [0.6310, 0.6367]
  val_value_loss : [0.1990, 0.2002]
  val_value_value_mse : [0.1990, 0.2002]
  value_loss : [0.2059, 0.2027]
  value_value_mse : [0.2059, 0.2027]

================================================================================

History file: model_versions/chess_elo_model_V31_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:11:03.532633Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game165001_game166500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V30
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.591455   (epoch 2)   mode=min
  policy_loss: 2.489044   (epoch 2)   mode=min
  policy_policy_acc: 0.283596   (epoch 2)   mode=max
  policy_top10_acc: 0.786731   (epoch 2)   mode=max
  policy_top5_acc: 0.627484   (epoch 2)   mode=max
  val_loss: 2.602060   (epoch 1)   mode=min
  val_policy_loss: 2.498507   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289211   (epoch 2)   mode=max
  val_policy_top10_acc: 0.782917   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 1)   mode=max
  val_value_loss: 0.198594   (epoch 2)   mode=min
  val_value_value_mse: 0.198610   (epoch 2)   mode=min
  value_loss: 0.204821   (epoch 2)   mode=min
  value_value_mse: 0.204821   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6233, 2.5915]
  policy_loss : [2.5192, 2.4890]
  policy_policy_acc : [0.2778, 0.2836]
  policy_top10_acc : [0.7790, 0.7867]
  policy_top5_acc : [0.6208, 0.6275]
  val_loss : [2.6021, 2.6429]
  val_policy_loss : [2.4985, 2.5434]
  val_policy_policy_acc : [0.2888, 0.2892]
  val_policy_top10_acc : [0.7829, 0.7825]
  val_policy_top5_acc : [0.6289, 0.6281]
  val_value_loss : [0.2068, 0.1986]
  val_value_value_mse : [0.2068, 0.1986]
  value_loss : [0.2083, 0.2048]
  value_value_mse : [0.2083, 0.2048]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:13:05.111832Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game166501_game168000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.598377   (epoch 2)   mode=min
  policy_loss: 2.499722   (epoch 2)   mode=min
  policy_policy_acc: 0.285471   (epoch 2)   mode=max
  policy_top10_acc: 0.783850   (epoch 2)   mode=max
  policy_top5_acc: 0.625481   (epoch 2)   mode=max
  val_loss: 3.138175   (epoch 2)   mode=min
  val_policy_loss: 3.038234   (epoch 2)   mode=min
  val_policy_policy_acc: 0.290709   (epoch 2)   mode=max
  val_policy_top10_acc: 0.790509   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630769   (epoch 2)   mode=max
  val_value_loss: 0.198406   (epoch 1)   mode=min
  val_value_value_mse: 0.198401   (epoch 1)   mode=min
  value_loss: 0.197488   (epoch 2)   mode=min
  value_value_mse: 0.197492   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6325, 2.5984]
  policy_loss : [2.5327, 2.4997]
  policy_policy_acc : [0.2783, 0.2855]
  policy_top10_acc : [0.7781, 0.7839]
  policy_top5_acc : [0.6151, 0.6255]
  val_loss : [3.2440, 3.1382]
  val_policy_loss : [3.1442, 3.0382]
  val_policy_policy_acc : [0.2842, 0.2907]
  val_policy_top10_acc : [0.7905, 0.7898]
  val_policy_top5_acc : [0.6279, 0.6308]
  val_value_loss : [0.1984, 0.1988]
  val_value_value_mse : [0.1984, 0.1988]
  value_loss : [0.1997, 0.1975]
  value_value_mse : [0.1997, 0.1975]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:15:15.227315Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game168001_game169500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.607320   (epoch 2)   mode=min
  policy_loss: 2.507085   (epoch 2)   mode=min
  policy_policy_acc: 0.280030   (epoch 2)   mode=max
  policy_top10_acc: 0.782743   (epoch 2)   mode=max
  policy_top5_acc: 0.620999   (epoch 2)   mode=max
  val_loss: 16.736406   (epoch 2)   mode=min
  val_policy_loss: 16.624973   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288312   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789011   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627373   (epoch 2)   mode=max
  val_value_loss: 0.198678   (epoch 1)   mode=min
  val_value_value_mse: 0.198689   (epoch 1)   mode=min
  value_loss: 0.200526   (epoch 2)   mode=min
  value_value_mse: 0.200527   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6422, 2.6073]
  policy_loss : [2.5408, 2.5071]
  policy_policy_acc : [0.2735, 0.2800]
  policy_top10_acc : [0.7757, 0.7827]
  policy_top5_acc : [0.6133, 0.6210]
  val_loss : [17.7124, 16.7364]
  val_policy_loss : [17.6038, 16.6250]
  val_policy_policy_acc : [0.2838, 0.2883]
  val_policy_top10_acc : [0.7828, 0.7890]
  val_policy_top5_acc : [0.6261, 0.6274]
  val_value_loss : [0.1987, 0.2055]
  val_value_value_mse : [0.1987, 0.2055]
  value_loss : [0.2033, 0.2005]
  value_value_mse : [0.2033, 0.2005]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:17:15.179710Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game169501_game171000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.590048   (epoch 2)   mode=min
  policy_loss: 2.488415   (epoch 2)   mode=min
  policy_policy_acc: 0.285281   (epoch 2)   mode=max
  policy_top10_acc: 0.787961   (epoch 2)   mode=max
  policy_top5_acc: 0.626547   (epoch 2)   mode=max
  val_loss: 3.944439   (epoch 1)   mode=min
  val_policy_loss: 3.845147   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288412   (epoch 1)   mode=max
  val_policy_top10_acc: 0.787512   (epoch 1)   mode=max
  val_policy_top5_acc: 0.628871   (epoch 1)   mode=max
  val_value_loss: 0.196722   (epoch 1)   mode=min
  val_value_value_mse: 0.196720   (epoch 1)   mode=min
  value_loss: 0.203425   (epoch 2)   mode=min
  value_value_mse: 0.203418   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6254, 2.5900]
  policy_loss : [2.5218, 2.4884]
  policy_policy_acc : [0.2779, 0.2853]
  policy_top10_acc : [0.7806, 0.7880]
  policy_top5_acc : [0.6179, 0.6265]
  val_loss : [3.9444, 4.3554]
  val_policy_loss : [3.8451, 4.2539]
  val_policy_policy_acc : [0.2884, 0.2819]
  val_policy_top10_acc : [0.7875, 0.7858]
  val_policy_top5_acc : [0.6289, 0.6285]
  val_value_loss : [0.1967, 0.2005]
  val_value_value_mse : [0.1967, 0.2005]
  value_loss : [0.2070, 0.2034]
  value_value_mse : [0.2070, 0.2034]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:19:26.027574Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game171001_game172500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592269   (epoch 2)   mode=min
  policy_loss: 2.495655   (epoch 2)   mode=min
  policy_policy_acc: 0.282371   (epoch 2)   mode=max
  policy_top10_acc: 0.786011   (epoch 2)   mode=max
  policy_top5_acc: 0.625243   (epoch 2)   mode=max
  val_loss: 2.714550   (epoch 1)   mode=min
  val_policy_loss: 2.614706   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289111   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787712   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631968   (epoch 2)   mode=max
  val_value_loss: 0.199140   (epoch 1)   mode=min
  val_value_value_mse: 0.199129   (epoch 1)   mode=min
  value_loss: 0.193065   (epoch 2)   mode=min
  value_value_mse: 0.193070   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6280, 2.5923]
  policy_loss : [2.5299, 2.4957]
  policy_policy_acc : [0.2757, 0.2824]
  policy_top10_acc : [0.7778, 0.7860]
  policy_top5_acc : [0.6146, 0.6252]
  val_loss : [2.7146, 17.1593]
  val_policy_loss : [2.6147, 17.0488]
  val_policy_policy_acc : [0.2852, 0.2891]
  val_policy_top10_acc : [0.7850, 0.7877]
  val_policy_top5_acc : [0.6290, 0.6320]
  val_value_loss : [0.1991, 0.2032]
  val_value_value_mse : [0.1991, 0.2032]
  value_loss : [0.1962, 0.1931]
  value_value_mse : [0.1962, 0.1931]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:21:35.295177Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game172501_game174000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.599932   (epoch 2)   mode=min
  policy_loss: 2.502161   (epoch 2)   mode=min
  policy_policy_acc: 0.281243   (epoch 2)   mode=max
  policy_top10_acc: 0.785958   (epoch 2)   mode=max
  policy_top5_acc: 0.624865   (epoch 2)   mode=max
  val_loss: 2.680823   (epoch 2)   mode=min
  val_policy_loss: 2.579325   (epoch 2)   mode=min
  val_policy_policy_acc: 0.291209   (epoch 1)   mode=max
  val_policy_top10_acc: 0.792408   (epoch 1)   mode=max
  val_policy_top5_acc: 0.630470   (epoch 1)   mode=max
  val_value_loss: 0.201387   (epoch 1)   mode=min
  val_value_value_mse: 0.201381   (epoch 1)   mode=min
  value_loss: 0.195603   (epoch 2)   mode=min
  value_value_mse: 0.195604   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6304, 2.5999]
  policy_loss : [2.5316, 2.5022]
  policy_policy_acc : [0.2754, 0.2812]
  policy_top10_acc : [0.7790, 0.7860]
  policy_top5_acc : [0.6169, 0.6249]
  val_loss : [2.7064, 2.6808]
  val_policy_loss : [2.6054, 2.5793]
  val_policy_policy_acc : [0.2912, 0.2872]
  val_policy_top10_acc : [0.7924, 0.7866]
  val_policy_top5_acc : [0.6305, 0.6266]
  val_value_loss : [0.2014, 0.2023]
  val_value_value_mse : [0.2014, 0.2023]
  value_loss : [0.1978, 0.1956]
  value_value_mse : [0.1978, 0.1956]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:23:45.037761Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game174001_game175500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.610085   (epoch 2)   mode=min
  policy_loss: 2.515579   (epoch 2)   mode=min
  policy_policy_acc: 0.277293   (epoch 2)   mode=max
  policy_top10_acc: 0.779989   (epoch 2)   mode=max
  policy_top5_acc: 0.619593   (epoch 2)   mode=max
  val_loss: 11.808876   (epoch 1)   mode=min
  val_policy_loss: 11.703651   (epoch 1)   mode=min
  val_policy_policy_acc: 0.287812   (epoch 2)   mode=max
  val_policy_top10_acc: 0.788711   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632068   (epoch 2)   mode=max
  val_value_loss: 0.199128   (epoch 1)   mode=min
  val_value_value_mse: 0.199148   (epoch 1)   mode=min
  value_loss: 0.189634   (epoch 2)   mode=min
  value_value_mse: 0.189691   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6387, 2.6101]
  policy_loss : [2.5439, 2.5156]
  policy_policy_acc : [0.2730, 0.2773]
  policy_top10_acc : [0.7736, 0.7800]
  policy_top5_acc : [0.6113, 0.6196]
  val_loss : [11.8089, 12.7754]
  val_policy_loss : [11.7037, 12.6686]
  val_policy_policy_acc : [0.2831, 0.2878]
  val_policy_top10_acc : [0.7882, 0.7887]
  val_policy_top5_acc : [0.6277, 0.6321]
  val_value_loss : [0.1991, 0.2010]
  val_value_value_mse : [0.1991, 0.2010]
  value_loss : [0.1926, 0.1896]
  value_value_mse : [0.1926, 0.1897]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:26:10.780736Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game175501_game177000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.582099   (epoch 2)   mode=min
  policy_loss: 2.483454   (epoch 2)   mode=min
  policy_policy_acc: 0.285585   (epoch 2)   mode=max
  policy_top10_acc: 0.788846   (epoch 2)   mode=max
  policy_top5_acc: 0.628708   (epoch 2)   mode=max
  val_loss: 2.765218   (epoch 2)   mode=min
  val_policy_loss: 2.665761   (epoch 1)   mode=min
  val_policy_policy_acc: 0.288511   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789610   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631768   (epoch 1)   mode=max
  val_value_loss: 0.196827   (epoch 2)   mode=min
  val_value_value_mse: 0.196829   (epoch 2)   mode=min
  value_loss: 0.197265   (epoch 2)   mode=min
  value_value_mse: 0.197266   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6158, 2.5821]
  policy_loss : [2.5160, 2.4835]
  policy_policy_acc : [0.2766, 0.2856]
  policy_top10_acc : [0.7822, 0.7888]
  policy_top5_acc : [0.6188, 0.6287]
  val_loss : [2.7658, 2.7652]
  val_policy_loss : [2.6658, 2.6666]
  val_policy_policy_acc : [0.2841, 0.2885]
  val_policy_top10_acc : [0.7879, 0.7896]
  val_policy_top5_acc : [0.6318, 0.6314]
  val_value_loss : [0.1996, 0.1968]
  val_value_value_mse : [0.1996, 0.1968]
  value_loss : [0.1995, 0.1973]
  value_value_mse : [0.1995, 0.1973]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:28:28.087144Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game177001_game178500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.588984   (epoch 2)   mode=min
  policy_loss: 2.488073   (epoch 2)   mode=min
  policy_policy_acc: 0.281757   (epoch 2)   mode=max
  policy_top10_acc: 0.789950   (epoch 2)   mode=max
  policy_top5_acc: 0.627487   (epoch 2)   mode=max
  val_loss: 2.672650   (epoch 2)   mode=min
  val_policy_loss: 2.571779   (epoch 2)   mode=min
  val_policy_policy_acc: 0.287912   (epoch 2)   mode=max
  val_policy_top10_acc: 0.785814   (epoch 1)   mode=max
  val_policy_top5_acc: 0.629471   (epoch 1)   mode=max
  val_value_loss: 0.201191   (epoch 2)   mode=min
  val_value_value_mse: 0.201202   (epoch 2)   mode=min
  value_loss: 0.201262   (epoch 2)   mode=min
  value_value_mse: 0.201243   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6199, 2.5890]
  policy_loss : [2.5182, 2.4881]
  policy_policy_acc : [0.2748, 0.2818]
  policy_top10_acc : [0.7828, 0.7900]
  policy_top5_acc : [0.6208, 0.6275]
  val_loss : [4.6951, 2.6727]
  val_policy_loss : [4.5926, 2.5718]
  val_policy_policy_acc : [0.2844, 0.2879]
  val_policy_top10_acc : [0.7858, 0.7842]
  val_policy_top5_acc : [0.6295, 0.6268]
  val_value_loss : [0.2020, 0.2012]
  val_value_value_mse : [0.2020, 0.2012]
  value_loss : [0.2039, 0.2013]
  value_value_mse : [0.2039, 0.2012]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:30:33.861411Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game178501_game180000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.585364   (epoch 2)   mode=min
  policy_loss: 2.487770   (epoch 2)   mode=min
  policy_policy_acc: 0.283052   (epoch 2)   mode=max
  policy_top10_acc: 0.788569   (epoch 2)   mode=max
  policy_top5_acc: 0.627186   (epoch 2)   mode=max
  val_loss: 2.630457   (epoch 1)   mode=min
  val_policy_loss: 2.532448   (epoch 1)   mode=min
  val_policy_policy_acc: 0.293107   (epoch 1)   mode=max
  val_policy_top10_acc: 0.790809   (epoch 2)   mode=max
  val_policy_top5_acc: 0.630370   (epoch 2)   mode=max
  val_value_loss: 0.195508   (epoch 1)   mode=min
  val_value_value_mse: 0.195514   (epoch 1)   mode=min
  value_loss: 0.195150   (epoch 2)   mode=min
  value_value_mse: 0.195162   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6169, 2.5854]
  policy_loss : [2.5181, 2.4878]
  policy_policy_acc : [0.2754, 0.2831]
  policy_top10_acc : [0.7804, 0.7886]
  policy_top5_acc : [0.6176, 0.6272]
  val_loss : [2.6305, 2.7655]
  val_policy_loss : [2.5324, 2.6636]
  val_policy_policy_acc : [0.2931, 0.2881]
  val_policy_top10_acc : [0.7892, 0.7908]
  val_policy_top5_acc : [0.6294, 0.6304]
  val_value_loss : [0.1955, 0.2033]
  val_value_value_mse : [0.1955, 0.2033]
  value_loss : [0.1978, 0.1951]
  value_value_mse : [0.1978, 0.1952]

================================================================================

History file: model_versions/chess_elo_model_V32_history.npy
Total runs: 10
================================================================================

RUN #1
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:32:39.716236Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game180001_game181500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V31
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.590480   (epoch 2)   mode=min
  policy_loss: 2.491401   (epoch 2)   mode=min
  policy_policy_acc: 0.283915   (epoch 2)   mode=max
  policy_top10_acc: 0.788425   (epoch 2)   mode=max
  policy_top5_acc: 0.626941   (epoch 2)   mode=max
  val_loss: 7.373467   (epoch 2)   mode=min
  val_policy_loss: 7.271222   (epoch 2)   mode=min
  val_policy_policy_acc: 0.288012   (epoch 2)   mode=max
  val_policy_top10_acc: 0.793507   (epoch 2)   mode=max
  val_policy_top5_acc: 0.628472   (epoch 2)   mode=max
  val_value_loss: 0.198178   (epoch 2)   mode=min
  val_value_value_mse: 0.198180   (epoch 2)   mode=min
  value_loss: 0.198206   (epoch 2)   mode=min
  value_value_mse: 0.198207   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6246, 2.5905]
  policy_loss : [2.5241, 2.4914]
  policy_policy_acc : [0.2766, 0.2839]
  policy_top10_acc : [0.7811, 0.7884]
  policy_top5_acc : [0.6178, 0.6269]
  val_loss : [7.8196, 7.3735]
  val_policy_loss : [7.7165, 7.2712]
  val_policy_policy_acc : [0.2824, 0.2880]
  val_policy_top10_acc : [0.7892, 0.7935]
  val_policy_top5_acc : [0.6263, 0.6285]
  val_value_loss : [0.1994, 0.1982]
  val_value_value_mse : [0.1994, 0.1982]
  value_loss : [0.2010, 0.1982]
  value_value_mse : [0.2010, 0.1982]

================================================================================

RUN #2
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:34:41.511717Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game181501_game183000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.593044   (epoch 2)   mode=min
  policy_loss: 2.492627   (epoch 2)   mode=min
  policy_policy_acc: 0.282354   (epoch 2)   mode=max
  policy_top10_acc: 0.788117   (epoch 2)   mode=max
  policy_top5_acc: 0.626669   (epoch 2)   mode=max
  val_loss: 8.250741   (epoch 1)   mode=min
  val_policy_loss: 8.147927   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290110   (epoch 2)   mode=max
  val_policy_top10_acc: 0.791209   (epoch 2)   mode=max
  val_policy_top5_acc: 0.631868   (epoch 1)   mode=max
  val_value_loss: 0.198461   (epoch 1)   mode=min
  val_value_value_mse: 0.198458   (epoch 1)   mode=min
  value_loss: 0.200978   (epoch 2)   mode=min
  value_value_mse: 0.200957   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6253, 2.5930]
  policy_loss : [2.5237, 2.4926]
  policy_policy_acc : [0.2766, 0.2824]
  policy_top10_acc : [0.7821, 0.7881]
  policy_top5_acc : [0.6203, 0.6267]
  val_loss : [8.2507, 9.0030]
  val_policy_loss : [8.1479, 8.8982]
  val_policy_policy_acc : [0.2870, 0.2901]
  val_policy_top10_acc : [0.7871, 0.7912]
  val_policy_top5_acc : [0.6319, 0.6308]
  val_value_loss : [0.1985, 0.2014]
  val_value_value_mse : [0.1985, 0.2014]
  value_loss : [0.2033, 0.2010]
  value_value_mse : [0.2033, 0.2010]

================================================================================

RUN #3
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:36:48.950784Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game183001_game184500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.593987   (epoch 2)   mode=min
  policy_loss: 2.494377   (epoch 2)   mode=min
  policy_policy_acc: 0.281611   (epoch 2)   mode=max
  policy_top10_acc: 0.785365   (epoch 2)   mode=max
  policy_top5_acc: 0.624310   (epoch 2)   mode=max
  val_loss: 2.952492   (epoch 2)   mode=min
  val_policy_loss: 2.853515   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285115   (epoch 1)   mode=max
  val_policy_top10_acc: 0.786913   (epoch 2)   mode=max
  val_policy_top5_acc: 0.627772   (epoch 2)   mode=max
  val_value_loss: 0.197058   (epoch 2)   mode=min
  val_value_value_mse: 0.197047   (epoch 2)   mode=min
  value_loss: 0.199294   (epoch 2)   mode=min
  value_value_mse: 0.199300   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6261, 2.5940]
  policy_loss : [2.5255, 2.4944]
  policy_policy_acc : [0.2769, 0.2816]
  policy_top10_acc : [0.7780, 0.7854]
  policy_top5_acc : [0.6174, 0.6243]
  val_loss : [3.3491, 2.9525]
  val_policy_loss : [3.2471, 2.8535]
  val_policy_policy_acc : [0.2851, 0.2830]
  val_policy_top10_acc : [0.7859, 0.7869]
  val_policy_top5_acc : [0.6250, 0.6278]
  val_value_loss : [0.2026, 0.1971]
  val_value_value_mse : [0.2026, 0.1970]
  value_loss : [0.2012, 0.1993]
  value_value_mse : [0.2012, 0.1993]

================================================================================

RUN #4
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:38:52.526625Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game184501_game186000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595113   (epoch 2)   mode=min
  policy_loss: 2.495903   (epoch 2)   mode=min
  policy_policy_acc: 0.282101   (epoch 2)   mode=max
  policy_top10_acc: 0.785150   (epoch 2)   mode=max
  policy_top5_acc: 0.626106   (epoch 2)   mode=max
  val_loss: 2.645579   (epoch 2)   mode=min
  val_policy_loss: 2.545815   (epoch 2)   mode=min
  val_policy_policy_acc: 0.285914   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789810   (epoch 1)   mode=max
  val_policy_top5_acc: 0.632068   (epoch 1)   mode=max
  val_value_loss: 0.196417   (epoch 1)   mode=min
  val_value_value_mse: 0.196421   (epoch 1)   mode=min
  value_loss: 0.198510   (epoch 2)   mode=min
  value_value_mse: 0.198517   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6270, 2.5951]
  policy_loss : [2.5265, 2.4959]
  policy_policy_acc : [0.2757, 0.2821]
  policy_top10_acc : [0.7781, 0.7852]
  policy_top5_acc : [0.6156, 0.6261]
  val_loss : [3.2134, 2.6456]
  val_policy_loss : [3.1145, 2.5458]
  val_policy_policy_acc : [0.2821, 0.2859]
  val_policy_top10_acc : [0.7898, 0.7841]
  val_policy_top5_acc : [0.6321, 0.6266]
  val_value_loss : [0.1964, 0.1990]
  val_value_value_mse : [0.1964, 0.1990]
  value_loss : [0.2010, 0.1985]
  value_value_mse : [0.2010, 0.1985]

================================================================================

RUN #5
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:41:03.367780Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game186001_game187500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.578843   (epoch 2)   mode=min
  policy_loss: 2.478226   (epoch 2)   mode=min
  policy_policy_acc: 0.285211   (epoch 2)   mode=max
  policy_top10_acc: 0.788938   (epoch 2)   mode=max
  policy_top5_acc: 0.628809   (epoch 2)   mode=max
  val_loss: 2.606502   (epoch 1)   mode=min
  val_policy_loss: 2.505036   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290509   (epoch 2)   mode=max
  val_policy_top10_acc: 0.792308   (epoch 2)   mode=max
  val_policy_top5_acc: 0.633467   (epoch 2)   mode=max
  val_value_loss: 0.202501   (epoch 1)   mode=min
  val_value_value_mse: 0.202485   (epoch 1)   mode=min
  value_loss: 0.201247   (epoch 2)   mode=min
  value_value_mse: 0.201248   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6143, 2.5788]
  policy_loss : [2.5133, 2.4782]
  policy_policy_acc : [0.2793, 0.2852]
  policy_top10_acc : [0.7828, 0.7889]
  policy_top5_acc : [0.6227, 0.6288]
  val_loss : [2.6065, 2.7236]
  val_policy_loss : [2.5050, 2.6216]
  val_policy_policy_acc : [0.2850, 0.2905]
  val_policy_top10_acc : [0.7896, 0.7923]
  val_policy_top5_acc : [0.6317, 0.6335]
  val_value_loss : [0.2025, 0.2033]
  val_value_value_mse : [0.2025, 0.2033]
  value_loss : [0.2019, 0.2012]
  value_value_mse : [0.2019, 0.2012]

================================================================================

RUN #6
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:43:10.413841Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game187501_game189000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.588533   (epoch 2)   mode=min
  policy_loss: 2.492241   (epoch 2)   mode=min
  policy_policy_acc: 0.281717   (epoch 2)   mode=max
  policy_top10_acc: 0.787321   (epoch 2)   mode=max
  policy_top5_acc: 0.625186   (epoch 2)   mode=max
  val_loss: 2.638527   (epoch 1)   mode=min
  val_policy_loss: 2.536880   (epoch 1)   mode=min
  val_policy_policy_acc: 0.283117   (epoch 2)   mode=max
  val_policy_top10_acc: 0.787013   (epoch 1)   mode=max
  val_policy_top5_acc: 0.623576   (epoch 2)   mode=max
  val_value_loss: 0.199366   (epoch 2)   mode=min
  val_value_value_mse: 0.199352   (epoch 2)   mode=min
  value_loss: 0.192624   (epoch 2)   mode=min
  value_value_mse: 0.192628   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6265, 2.5885]
  policy_loss : [2.5290, 2.4922]
  policy_policy_acc : [0.2758, 0.2817]
  policy_top10_acc : [0.7791, 0.7873]
  policy_top5_acc : [0.6157, 0.6252]
  val_loss : [2.6385, 2.6530]
  val_policy_loss : [2.5369, 2.5532]
  val_policy_policy_acc : [0.2807, 0.2831]
  val_policy_top10_acc : [0.7870, 0.7845]
  val_policy_top5_acc : [0.6217, 0.6236]
  val_value_loss : [0.2029, 0.1994]
  val_value_value_mse : [0.2029, 0.1994]
  value_loss : [0.1951, 0.1926]
  value_value_mse : [0.1951, 0.1926]

================================================================================

RUN #7
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:45:25.080958Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game189001_game190500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592341   (epoch 2)   mode=min
  policy_loss: 2.493246   (epoch 2)   mode=min
  policy_policy_acc: 0.280604   (epoch 2)   mode=max
  policy_top10_acc: 0.787536   (epoch 2)   mode=max
  policy_top5_acc: 0.625787   (epoch 2)   mode=max
  val_loss: 2.997826   (epoch 1)   mode=min
  val_policy_loss: 2.898249   (epoch 1)   mode=min
  val_policy_policy_acc: 0.289610   (epoch 1)   mode=max
  val_policy_top10_acc: 0.789710   (epoch 2)   mode=max
  val_policy_top5_acc: 0.632368   (epoch 2)   mode=max
  val_value_loss: 0.198302   (epoch 1)   mode=min
  val_value_value_mse: 0.198306   (epoch 1)   mode=min
  value_loss: 0.198178   (epoch 2)   mode=min
  value_value_mse: 0.198172   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6240, 2.5923]
  policy_loss : [2.5242, 2.4932]
  policy_policy_acc : [0.2754, 0.2806]
  policy_top10_acc : [0.7789, 0.7875]
  policy_top5_acc : [0.6167, 0.6258]
  val_loss : [2.9978, 3.3854]
  val_policy_loss : [2.8982, 3.2845]
  val_policy_policy_acc : [0.2896, 0.2875]
  val_policy_top10_acc : [0.7886, 0.7897]
  val_policy_top5_acc : [0.6298, 0.6324]
  val_value_loss : [0.1983, 0.2004]
  val_value_value_mse : [0.1983, 0.2004]
  value_loss : [0.1997, 0.1982]
  value_value_mse : [0.1997, 0.1982]

================================================================================

RUN #8
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:47:32.633088Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game190501_game192000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.603531   (epoch 2)   mode=min
  policy_loss: 2.503123   (epoch 2)   mode=min
  policy_policy_acc: 0.281121   (epoch 2)   mode=max
  policy_top10_acc: 0.782920   (epoch 2)   mode=max
  policy_top5_acc: 0.622366   (epoch 2)   mode=max
  val_loss: 3.354501   (epoch 1)   mode=min
  val_policy_loss: 3.254012   (epoch 1)   mode=min
  val_policy_policy_acc: 0.284615   (epoch 2)   mode=max
  val_policy_top10_acc: 0.784316   (epoch 2)   mode=max
  val_policy_top5_acc: 0.624176   (epoch 2)   mode=max
  val_value_loss: 0.198552   (epoch 2)   mode=min
  val_value_value_mse: 0.198554   (epoch 2)   mode=min
  value_loss: 0.200564   (epoch 2)   mode=min
  value_value_mse: 0.200539   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6341, 2.6035]
  policy_loss : [2.5339, 2.5031]
  policy_policy_acc : [0.2744, 0.2811]
  policy_top10_acc : [0.7756, 0.7829]
  policy_top5_acc : [0.6135, 0.6224]
  val_loss : [3.3545, 13.4321]
  val_policy_loss : [3.2540, 13.3261]
  val_policy_policy_acc : [0.2813, 0.2846]
  val_policy_top10_acc : [0.7791, 0.7843]
  val_policy_top5_acc : [0.6224, 0.6242]
  val_value_loss : [0.1996, 0.1986]
  val_value_value_mse : [0.1996, 0.1986]
  value_loss : [0.2027, 0.2006]
  value_value_mse : [0.2026, 0.2005]

================================================================================

RUN #9
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:49:35.633268Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game192001_game193500.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.595643   (epoch 2)   mode=min
  policy_loss: 2.496419   (epoch 2)   mode=min
  policy_policy_acc: 0.283834   (epoch 2)   mode=max
  policy_top10_acc: 0.785447   (epoch 2)   mode=max
  policy_top5_acc: 0.625252   (epoch 2)   mode=max
  val_loss: 4.680795   (epoch 1)   mode=min
  val_policy_loss: 4.577959   (epoch 1)   mode=min
  val_policy_policy_acc: 0.290709   (epoch 2)   mode=max
  val_policy_top10_acc: 0.789411   (epoch 2)   mode=max
  val_policy_top5_acc: 0.635165   (epoch 2)   mode=max
  val_value_loss: 0.198401   (epoch 2)   mode=min
  val_value_value_mse: 0.198412   (epoch 2)   mode=min
  value_loss: 0.198483   (epoch 2)   mode=min
  value_value_mse: 0.198482   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6247, 2.5956]
  policy_loss : [2.5246, 2.4964]
  policy_policy_acc : [0.2769, 0.2838]
  policy_top10_acc : [0.7776, 0.7854]
  policy_top5_acc : [0.6170, 0.6253]
  val_loss : [4.6808, 7.0019]
  val_policy_loss : [4.5780, 6.8999]
  val_policy_policy_acc : [0.2848, 0.2907]
  val_policy_top10_acc : [0.7867, 0.7894]
  val_policy_top5_acc : [0.6289, 0.6352]
  val_value_loss : [0.2028, 0.1984]
  val_value_value_mse : [0.2028, 0.1984]
  value_loss : [0.2003, 0.1985]
  value_value_mse : [0.2003, 0.1985]

================================================================================

RUN #10
------------------------------------------------------------
Timestamp (UTC): 2025-10-18T18:51:44.337192Z
Args (salvati):
  alpha: 1.0
  batch_size: 32
  compile: True
  dataset: all_positions_jul2014_npz/positions_jul2014_game193501_game195000.npz
  epochs: 2
  lr: 0.001
  model: model_versions/chess_elo_model_V32
  monitor: val_policy_loss
  policy_weight: 1.0
  train_per: 0.9
  validation: validation_10k_positions_from_130_files_V4.npz
  validation_indices: validation_selected_indices_V4.json
  value_weight: 0.5

Best per metrica (valore, epoca, mode):
  learning_rate: 0.001000   (epoch 1)   mode=max
  loss: 2.592693   (epoch 2)   mode=min
  policy_loss: 2.494486   (epoch 2)   mode=min
  policy_policy_acc: 0.284862   (epoch 2)   mode=max
  policy_top10_acc: 0.786966   (epoch 2)   mode=max
  policy_top5_acc: 0.626083   (epoch 2)   mode=max
  val_loss: 2.839129   (epoch 1)   mode=min
  val_policy_loss: 2.739861   (epoch 1)   mode=min
  val_policy_policy_acc: 0.285415   (epoch 1)   mode=max
  val_policy_top10_acc: 0.784715   (epoch 1)   mode=max
  val_policy_top5_acc: 0.626873   (epoch 1)   mode=max
  val_value_loss: 0.197844   (epoch 1)   mode=min
  val_value_value_mse: 0.197845   (epoch 1)   mode=min
  value_loss: 0.196428   (epoch 2)   mode=min
  value_value_mse: 0.196429   (epoch 2)   mode=min

Full history (per metrica -> lista di valori per epoca):
  learning_rate : [0.0010, 0.0010]
  loss : [2.6297, 2.5927]
  policy_loss : [2.5302, 2.4945]
  policy_policy_acc : [0.2744, 0.2849]
  policy_top10_acc : [0.7788, 0.7870]
  policy_top5_acc : [0.6172, 0.6261]
  val_loss : [2.8391, 2.9875]
  val_policy_loss : [2.7399, 2.8865]
  val_policy_policy_acc : [0.2854, 0.2831]
  val_policy_top10_acc : [0.7847, 0.7816]
  val_policy_top5_acc : [0.6269, 0.6252]
  val_value_loss : [0.1978, 0.2009]
  val_value_value_mse : [0.1978, 0.2010]
  value_loss : [0.1990, 0.1964]
  value_value_mse : [0.1990, 0.1964]

================================================================================

